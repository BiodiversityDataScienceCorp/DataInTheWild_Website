[
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Course Structure",
    "section": "",
    "text": "This page serves as an inventory of the material accessible under each subsection of this site.\n\n\n\n\n\n\n\n\n\n\n\nModule\nTitle\nLecture / Code-along\nData sets\nDiscussions\nAssignments\n\n\n\n\n1.1\nIntroduction to RStudio\n\n\n\n\n\n\n1.2\nIntroduction to Coding\n\n\n\n\n\n\n1.3\n2-dimensional Data and the tidyverse\n\n\n\n\n\n\n2.1\nIntroduction to Descriptive Statistics and Data Visualization\n\n\n\n\n\n\n2.2\nWriting Functions\n\n\n\n\n\n\n2.3\nPlotting with ggplot2\n\n\n\n\n\n\n2.4\nA Visualization Primer\n\n\n\n\n\n\n2.5\nSick Fish\n\n\n\n\n\n\n2.6\nExploring geom Functions\n\n\n\n\n\n\n2.7\nWrap-Up\n\n\n\n\n\n\n3.1\nLeopard Seals\n\n\n\n\n\n\n3.2\nT-Tests\n\n\n\n\n\n\n3.3\nComparing (Multiple) Means\n\n\n\n\n\n\n3.4\nCombining Data (Joins and Binds)\n\n\n\n\n\n\n3.5\nK-Nearest Neighbor\n\n\n\n\n\n\n4.1\nRoads and Regressions\n\n\n\n\n\n\n4.2\nMultiple Regression\n\n\n\n\n\n\n4.3\nUsing Functions to Automate Tasks"
  },
  {
    "objectID": "structure.html#the-full-course-cheese-board",
    "href": "structure.html#the-full-course-cheese-board",
    "title": "Course Structure",
    "section": "",
    "text": "This page serves as an inventory of the material accessible under each subsection of this site.\n\n\n\n\n\n\n\n\n\n\n\nModule\nTitle\nLecture / Code-along\nData sets\nDiscussions\nAssignments\n\n\n\n\n1.1\nIntroduction to RStudio\n\n\n\n\n\n\n1.2\nIntroduction to Coding\n\n\n\n\n\n\n1.3\n2-dimensional Data and the tidyverse\n\n\n\n\n\n\n2.1\nIntroduction to Descriptive Statistics and Data Visualization\n\n\n\n\n\n\n2.2\nWriting Functions\n\n\n\n\n\n\n2.3\nPlotting with ggplot2\n\n\n\n\n\n\n2.4\nA Visualization Primer\n\n\n\n\n\n\n2.5\nSick Fish\n\n\n\n\n\n\n2.6\nExploring geom Functions\n\n\n\n\n\n\n2.7\nWrap-Up\n\n\n\n\n\n\n3.1\nLeopard Seals\n\n\n\n\n\n\n3.2\nT-Tests\n\n\n\n\n\n\n3.3\nComparing (Multiple) Means\n\n\n\n\n\n\n3.4\nCombining Data (Joins and Binds)\n\n\n\n\n\n\n3.5\nK-Nearest Neighbor\n\n\n\n\n\n\n4.1\nRoads and Regressions\n\n\n\n\n\n\n4.2\nMultiple Regression\n\n\n\n\n\n\n4.3\nUsing Functions to Automate Tasks"
  },
  {
    "objectID": "modules/module_4/module4_2.html",
    "href": "modules/module_4/module4_2.html",
    "title": "4.2: Multiple Regression",
    "section": "",
    "text": "Sometimes (often!), we have multiple independent variables that we think might be influencing our dependent variable. In order to take into account more than one variable, we can perform what is called a “multiple regression.”\nMultiple regressions can become complicated beasts, but we are going to focus on adding in just one additional variable and how it can impact our interpretations.\nLet’s use some penguins nesting site data to demonstrate."
  },
  {
    "objectID": "modules/module_4/module4_2.html#set-up",
    "href": "modules/module_4/module4_2.html#set-up",
    "title": "4.2: Multiple Regression",
    "section": "Set-Up",
    "text": "Set-Up\nAs usual, we want to load our packages and our data before we begin.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\nsite_data &lt;- read_csv(\"data/site_changes.csv\") %&gt;% \n  mutate(year = as.factor(year)) # don't worry about this line too much\n\n# We just want the year column treated as a category, not a number\n\nWe want to understand what factors influence penguin nesting — we wouldn’t want to build our road through important penguin nesting territories!\nLet’s quickly take a look at the data we have.\n\n# View first few rows\nhead(site_data)\n\n# A tibble: 6 × 6\n  site_id year  tussocks dist_to_water stone_size num_nests\n    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1       1 1971      4.95          26.6       45.0        25\n2       2 1971      3.60          28.2       45.1        18\n3       3 1971      3.71          24.1       46.3        27\n4       4 1971      5.14          29.6       43.3        16\n5       5 1971      6.71          27.9       46.6        15\n6       6 1971      5.16          26.7       47.1        29\n\n# View last few rows\ntail(site_data)\n\n# A tibble: 6 × 6\n  site_id year  tussocks dist_to_water stone_size num_nests\n    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1      95 2011      7.46          25.8       42.1        19\n2      96 2011      6.29          21.9       43.7        31\n3      97 2011      7.10          21.7       45.5        25\n4      98 2011      6.61          21.9       42.3        18\n5      99 2011      6.74          21.7       41.7        29\n6     100 2011      7.50          18.6       40.1        31\n\n\nOkay, it looks like we have data for 100 different sites collected in two different years. We have the number of penguin nests, which is the variable that we are interested in. We also have a few other variables that might be influencing the number of nests at each site: the number of tussocks of grass, the stone size, the distance to water, and the year that data were collected.\nLet’s start with looking at how the number of grass tussocks affects nest numbers."
  },
  {
    "objectID": "modules/module_4/module4_2.html#regression-model-1-independent-variable",
    "href": "modules/module_4/module4_2.html#regression-model-1-independent-variable",
    "title": "4.2: Multiple Regression",
    "section": "Regression Model (1 Independent Variable)",
    "text": "Regression Model (1 Independent Variable)\nLet’s start by plotting our tussocks data and run a model.\n\n# Scatterplot\nggplot(site_data, aes(x = tussocks, y = num_nests)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Tussocks\", y = \"Number of Nests\") +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis model is showing us a positive relationship between the number of tussocks and number of nests. Let’s run our linear regression model.\n\n# Linear model of number of nests v. number of tussocks\ntussocks_model &lt;- lm(num_nests ~ tussocks, data = site_data)\n\n# View results\nsummary(tussocks_model)\n\n\nCall:\nlm(formula = num_nests ~ tussocks, data = site_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0717  -4.5553   0.2871   4.4302  14.1867 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.0711     2.0555   6.359 1.37e-09 ***\ntussocks      1.3299     0.3322   4.004 8.81e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.384 on 198 degrees of freedom\nMultiple R-squared:  0.0749,    Adjusted R-squared:  0.07023 \nF-statistic: 16.03 on 1 and 198 DF,  p-value: 8.81e-05\n\n\nOur model is telling us that the relationship between the number of nests and number of tussocks is highly significant."
  },
  {
    "objectID": "modules/module_4/module4_2.html#running-a-multiple-regression",
    "href": "modules/module_4/module4_2.html#running-a-multiple-regression",
    "title": "4.2: Multiple Regression",
    "section": "Running a Multiple Regression",
    "text": "Running a Multiple Regression\nWhat if we incorporate the year the data were collected into our model? Adding an additional independent variable to our model can change how we interpret the results.\nFirst, let’s plot a multiple scatterplot. Do we see the same patterns?\n\n# Scatterplot + year\nggplot(site_data, aes(x = tussocks, y = num_nests, color = year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Tussocks\", y = \"Number of Nests\") +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHuh, that looks a little bit different…we no longer seem to have much of a positive relationship. Instead, both lines are looking pretty flat! Let’s see what happens when we add year into our regression model.\nBy adding the year variable into the model, we are also asking if the number of nests changes by year. Does it change our significance?\nThe way we add an independent variable is with a + in the equation.\n\n# Linear model of number of nests v. number of tussocks and the year\ntussocks_model &lt;- lm(num_nests ~ tussocks + year, data = site_data)\n\n# View results\nsummary(tussocks_model)\n\n\nCall:\nlm(formula = num_nests ~ tussocks + year, data = site_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.644  -4.580   0.424   4.504  12.259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.4867     2.5055   7.777 4.05e-13 ***\ntussocks     -0.1868     0.4844  -0.386      0.7    \nyear2011      5.4818     1.3167   4.163 4.69e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.136 on 197 degrees of freedom\nMultiple R-squared:  0.1497,    Adjusted R-squared:  0.1411 \nF-statistic: 17.34 on 2 and 197 DF,  p-value: 1.154e-07\n\n\nWe have a bunch of estimates now! Don’t worry, I won’t be asking you to put them into an equation :) We are only focusing on significance values.\n\ntussocks: the p-value for the tussocks row (p = 0.7) shows whether the number of tussocks is a significant driver of nest numbers (it isn’t!)\nyear2011: this p-value is significant (p = 4.69e-05) — year does seem to impact how many nests there are\n\n(you don’t need to know this, but if you are curious…): the reason this says year2011 and not just year is because year is acting as a category, and we can get different coefficient estimates for year. The first option in the categorical variable (1971, in this case) is incorporated into the current intercept estimate, so year2011 indicates the difference from the original.\n\n\n\nInteractions\nSometimes, we might expect that there will be in interactive effect between the two independent variables we are including in our model. This means that the combinations of independent variables impact the dependent variable in different ways.\nWe can typically spot interactive effects in a plot when the slopes of the regression lines differ from each other a lot.\nLet’s talk through the visual demonstration below to hopefully get a better understanding of how interaction effects work. These plots are showing different possible relationships between two independent variables: how many cups of coffee someone has had and whether they drank them in the morning or the afternoon (categorical variable), and how those might impact the dependent variable, how long it takes to complete a task.\n\nThe top two panels show examples of no interactive effect. It looks like there is an effect of time of day; in each, drinking coffee in the morning makes tasks take longer. However, the slopes between time of day are the same. In the first panel (top left), there also looks to be an impact from the number of cups of coffee.\nThe bottom four panels demonstrate possible variations of interactive effects. Not only does time of day and/or number of cups of coffee impact how long it takes to complete a task, but those independent variable affect how long it takes to complete a task in different ways.\n\n\nIn our example above, an interactive effect between tussocks and year would mean that we think the number of tussocks impacts the number of nests in different ways between the two categories (year). Since the slopes between the years don’t look very different, we aren’t really expecting there to be a significant interaction, but let’s check anyway.\nLet’s run a model that includes the interaction term in our model. To do this, we will use an asterisk (*) instead of a plus sign.\n\n# Linear model with interaction\ntussocks_model &lt;- lm(data = site_data, num_nests ~ tussocks * year)\n\n# View results\nsummary(tussocks_model)\n\n\nCall:\nlm(formula = num_nests ~ tussocks * year, data = site_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5186  -4.5456   0.4779   4.4342  12.1779 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       18.23648    3.88911   4.689 5.13e-06 ***\ntussocks           0.06251    0.76571   0.082    0.935    \nyear2011           7.91428    5.92732   1.335    0.183    \ntussocks:year2011 -0.41675    0.99005  -0.421    0.674    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 196 degrees of freedom\nMultiple R-squared:  0.1505,    Adjusted R-squared:  0.1375 \nF-statistic: 11.57 on 3 and 196 DF,  p-value: 5.12e-07\n\n\nWe can see that we still have estimates for both tussocks and year alone. However, we now also have something new: our interaction term, tussocks:year2011.\nIt turns out that adding in the year variable has changed the significance in our model, and the interaction term is also not significant.\n\nThe positive relationship we originally observed is no longer present when we account for the year.\nThere is no significant interaction, meaning that penguins respond to the number of tussocks the same way in each year.\nInterestingly (and maybe frustratingly), the significant effect of year that we saw before is also gone.\n\nThis is a confusing bit of statistics, and I won’t go into depth here about why this happens. Similar things sometimes happen when we have a significant overall ANOVA but no significant pairwise comparisons.\nWhat is happening behind the scenes has to do with how the information for each variable is being partitioned. Adding new variables (including an interaction) decreases the likelihood of any individual variable being significant.\nI will never ask you to explain this! Only to correctly interpret the significance values.\n\n\nLet’s explore another example to get a better understanding."
  },
  {
    "objectID": "modules/module_4/module4_2.html#practicing-with-stone-size",
    "href": "modules/module_4/module4_2.html#practicing-with-stone-size",
    "title": "4.2: Multiple Regression",
    "section": "Practicing with Stone Size",
    "text": "Practicing with Stone Size\nLet’s investigate how stone size influences nest numbers. Gentoo penguins build their nests out of stones (they actually court their penguin partner with stones, too)!\n\nStart by plotting the relationship between stone size and the number of nests.\n\n# Scatterplot\nggplot(site_data, aes(x = stone_size, y = num_nests)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWhat does the regression model?\n\n# Linear model of number of nests v. stone size\nstone_size_mod = lm(num_nests ~ stone_size, data = site_data)\n\n# View results\nsummary(stone_size_mod)\n\n\nCall:\nlm(formula = num_nests ~ stone_size, data = site_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.0069  -4.9641  -0.3939   4.0581  15.5562 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.1572     5.9598  -0.865    0.388    \nstone_size    0.5940     0.1344   4.418 1.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.333 on 198 degrees of freedom\nMultiple R-squared:  0.08974,   Adjusted R-squared:  0.08515 \nF-statistic: 19.52 on 1 and 198 DF,  p-value: 1.637e-05\n\n\nHow to we interpret this model?\nNext, let’s add year in the mix. First, plot the relationship between the number of nests and stone size by year.\n\n# Scatterplot + year\nggplot(site_data, aes(x = stone_size, y = num_nests, color = year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis plot looks pretty different from the one with tussocks. First, let’s see what happens when we add the year variable without the interactive term. To do that, we use a plus sign (+), not an asterisk (*).\n\n# Linear model of number of nests v. stone size and the year\nstone_size_mod = lm(num_nests ~ stone_size + year, data = site_data)\n\n# View results\nsummary(stone_size_mod)\n\n\nCall:\nlm(formula = num_nests ~ stone_size + year, data = site_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.3183  -4.3173  -0.4471   4.7328  13.0118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -9.5585     5.4622  -1.750   0.0817 .  \nstone_size    0.6335     0.1224   5.175  5.6e-07 ***\nyear2011      5.3092     0.8155   6.510  6.1e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.76 on 197 degrees of freedom\nMultiple R-squared:  0.2509,    Adjusted R-squared:  0.2433 \nF-statistic: 32.99 on 2 and 197 DF,  p-value: 4.387e-13\n\n\nHow do we interpret this multiple regression model?\nFrom the ggplot above, we can see that the slopes for the two different years are quite different: one is pretty flat while the other is definitely positive. Different slopes often indicate that an interaction might be at work. Let’s find out. Run another multiple regression model, this time adding in the interactive effect (*).\n\n# Linear model with interaction\nstone_size_mod = lm(num_nests ~ stone_size * year, data = site_data)\n\n# View results\nsummary(stone_size_mod)\n\n\nCall:\nlm(formula = num_nests ~ stone_size * year, data = site_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3384  -4.1293   0.1189   3.7493  11.7629 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -37.5446     7.7555  -4.841 2.61e-06 ***\nstone_size            1.2643     0.1744   7.251 9.35e-12 ***\nyear2011             55.3424    10.3558   5.344 2.51e-07 ***\nstone_size:year2011  -1.1314     0.2335  -4.845 2.57e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.457 on 196 degrees of freedom\nMultiple R-squared:  0.331, Adjusted R-squared:  0.3208 \nF-statistic: 32.33 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nWow, a bunch of significant variables! What does that mean, exactly?\nNot only are stone size and year significant, but so is the interaction between stone size and year. What does this mean? It means that penguins in different years seems to respond different to stone size. In 1971, the number of nests increases with stone size, but in 2011, the number of nests doesn’t seem impacted by stone size. Therefore, there is an interaction between stone size and year.\n\nResources\nStill feeling a bit confused about multiple regression and interactions? I recommend checking out this website. You can skip all the equation stuff at the beginning. Below that, though, I think they do a nice job of laying out what interactions are.\nAlso, if you are interested in how we select which model is the best model, this article is a decent place to start."
  },
  {
    "objectID": "modules/module_3/module3_5.html",
    "href": "modules/module_3/module3_5.html",
    "title": "3.5: K-Nearest Neighbor",
    "section": "",
    "text": "Now, we’re going to work through an example of machine learning, which is used in a variety of ways in data science, across industries and disciplines.\nMachine learning is a sub-field of artificial intelligence (AI). Machine learning uses a data-driven approach; a model is typically trained on data with known outcomes, then the model uses that previous knowledge to predict the outcomes of new data.\nRemember, we have 50 unlabeled collars, and we want to be able to predict who made them so we can increase the safety of our fishers. We need to throw out any collars that we think were made by Budget Collars LLC, and only incorporate new collars from our unlabeled batch that we think were made by Collarium Inc.\nGiven the data we have from the collars we know were created by Budget Collars LLC and Collarium Inc, our goal is to train a model on collars with known makers to then apply to our unknown collars."
  },
  {
    "objectID": "modules/module_3/module3_5.html#machine-learning",
    "href": "modules/module_3/module3_5.html#machine-learning",
    "title": "3.5: K-Nearest Neighbor",
    "section": "",
    "text": "Now, we’re going to work through an example of machine learning, which is used in a variety of ways in data science, across industries and disciplines.\nMachine learning is a sub-field of artificial intelligence (AI). Machine learning uses a data-driven approach; a model is typically trained on data with known outcomes, then the model uses that previous knowledge to predict the outcomes of new data.\nRemember, we have 50 unlabeled collars, and we want to be able to predict who made them so we can increase the safety of our fishers. We need to throw out any collars that we think were made by Budget Collars LLC, and only incorporate new collars from our unlabeled batch that we think were made by Collarium Inc.\nGiven the data we have from the collars we know were created by Budget Collars LLC and Collarium Inc, our goal is to train a model on collars with known makers to then apply to our unknown collars."
  },
  {
    "objectID": "modules/module_3/module3_5.html#knn-in-r",
    "href": "modules/module_3/module3_5.html#knn-in-r",
    "title": "3.5: K-Nearest Neighbor",
    "section": "KNN in R",
    "text": "KNN in R\nK-nearest neighbors (KNN) is a classification algorithm (e.g., it attempts to predict labels or “classes”). It uses the classes of the nearest neighbors to a new data point (the number of neighbors = k) to predict the class of the new data point.\n\nYou don’t need to know the inner-workings of the model or even the code below; all you need to know is the overall concept of how K-nearest neighbors works and what the results mean.\n\nHow does KNN work?\nLet’s start with watch a video that does a nice job explaining the basics of KNN.\nWhat are your takeaways?\n\n\nVisualizing\nCreating plots that visualize KNN is pretty complicated, so we won’t be doing that as a class. However, you can find a nice interactive data visualization here.\nPlay around with the data visualization. Can you figure out what it is telling you?"
  },
  {
    "objectID": "modules/module_3/module3_5.html#classifying-collars",
    "href": "modules/module_3/module3_5.html#classifying-collars",
    "title": "3.5: K-Nearest Neighbor",
    "section": "Classifying Collars",
    "text": "Classifying Collars\nNow that we have an idea of how KNN works, let’s get started with our dataset.\nHere’s our overall trajectory:\n\nLoad our collars data.\nSplit the data into collars where we know the maker and collars where we don’t.\nSplit the collars with known makers into 2 data frames: one for training our model and one for testing the model out on data where we already know the answers.\nApply the best model to our collars with unknown makers to get predictions for which maker made the collar.\n\nFirst, we need our data!\n\nData Cleaning and Prepping\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\nfull_collars &lt;- read_csv(\"data/all_collar_data.csv\")\n\nNext, we need to scale our data.\nWhat does this mean? It means we are transforms the data so that they are all in the same scale. Because KNN uses distances between points to determine neighbors, we need to have our data measured on the same scale.\nBefore scaling, battery life (~100) and signal distance (~4000) were working in very different ranges, with signal distance being much bigger. Once we scale them, they will both have means around 0 and similar standard deviations.\n\n# Standardize and remove unnecessary data\n\n# Start with the full data set\nknn_data &lt;- full_collars %&gt;%\n  # Mutate the battery and signal columns to be scaled\n  mutate(battery_scaled = scale(battery_life),\n         signal_scaled = scale(signal_distance)) %&gt;%\n  # Remove columns we aren't using anymore\n  dplyr::select(-battery_life, -signal_distance, -fail:-weight) %&gt;% \n  # Make the maker column a factor (categorical)\n  mutate(maker = as.factor(maker))\n\nLet’s summarize and plot the scaled data to see what it looks like. Let’s focus on battery life.\n\n# Mean and standard deviation of battery life\nknn_data %&gt;% \n  summarise(mean_bat = mean(battery_scaled),\n            sd_bat = sd(battery_scaled))\n\n# A tibble: 1 × 2\n   mean_bat sd_bat\n      &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.31e-16      1\n\n# Histogram\nggplot(knn_data, aes(battery_scaled, fill = maker)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nRight now, we have 3 collar categories because we haven’t separated the unknown collars. Let’s do that.\n\n\nKnown and Unknown Labels\nBefore we continue, we need to make separate data frames to work with, one with collars with known makers and one with collars with unknown labels\n\n# Filter collars with unknown maker\ncollars_to_label &lt;- knn_data %&gt;%\n  filter(is.na(maker))\n\n# We need to save the collar ids before removing them\nunknown_collar_ids &lt;- collars_to_label %&gt;% \n  dplyr::select(collar_id)\n\n# Remove maker and collar_id columns\ncollars_to_label &lt;- collars_to_label %&gt;% \n  dplyr::select(-maker, -collar_id)\n\n# Back to original dataset: filter collars with a known maker, and remove collar_id\nlabeled_collars &lt;- knn_data %&gt;%\n  filter(!is.na(maker)) %&gt;% \n  select(-collar_id)\n\n\n\nTraining and Testing Data\nFirst, we need to train and test our model. We will do that with the data with known collars makers.\nNote: This is where the code starts getting a bit complicated. It’s ok if you aren’t entirely clear on what everything is doing.\n\n# Randomly choose 30 out of the 100 collars to be our test data\n\n# We will train the model with the other 70 collars, then test how well that model does using the 30 collars we held back\n\n# Because we KNOW the actual makers of those 30 collars, we will know if our model made any mistakes\n\n# Since we are randomly choosing values, we will use set.seed() to make sure everyone in class is getting the same random values\nset.seed(411)\n\n# Randomly select 30 row positions (70%) to pull out\nrows_to_pull &lt;- sample(1:100, 30)\n\n# View random selections\nrows_to_pull\n\n [1]  12   8  79  54  31  80  49  22  35  72  73  43  50  57  38  18  60  99  67\n[20]   7  23  32   9  16  15   2  82  71 100  45\n\n# Remove test data from labeled collars\ntraining &lt;- labeled_collars[-rows_to_pull, ] \n\n# Pull out the 30 rows for testing\ntesting &lt;- labeled_collars[rows_to_pull, ] \n\n\n\nTraining and Testing KNN\nNow that we have training and testing data, we can perform the first phase of KNN.\nWe want to know how many nearest neighbors we should use and how well our model works on the testing data. We will use the caret package to run these analyses.\n\n# Loading package to do KNN\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.2.3\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Set seed again\nset.seed(411)\n\n# Training the model and finding the best k value\nknn_training_model &lt;- train(maker ~ ., data = training, method = \"knn\")\n\n# View results and find that the best k value is 5\nknn_training_model\n\nk-Nearest Neighbors \n\n70 samples\n 2 predictor\n 2 classes: 'Budget Collars LLC', 'Collarium Inc.' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 70, 70, 70, 70, 70, 70, ... \nResampling results across tuning parameters:\n\n  k  Accuracy   Kappa    \n  5  0.9782931  0.9552329\n  7  0.9771543  0.9525164\n  9  0.9790591  0.9566543\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 9.\n\n\nOk, we have determined that our best k value is 5. Now we can use our training model on the testing data to see if it makes any mistakes. We use the predict function from caret to do this.\n\n# Making predictions with the test data\nknn_predict_test &lt;- predict(knn_training_model, testing)\n\n# This will show us that there are no mistakes!\nconfusionMatrix(knn_predict_test, testing$maker) \n\nConfusion Matrix and Statistics\n\n                    Reference\nPrediction           Budget Collars LLC Collarium Inc.\n  Budget Collars LLC                 14              0\n  Collarium Inc.                      0             16\n                                            \n               Accuracy : 1                 \n                 95% CI : (0.8843, 1)       \n    No Information Rate : 0.5333            \n    P-Value [Acc &gt; NIR] : 6.456e-09         \n                                            \n                  Kappa : 1                 \n                                            \n Mcnemar's Test P-Value : NA                \n                                            \n            Sensitivity : 1.0000            \n            Specificity : 1.0000            \n         Pos Pred Value : 1.0000            \n         Neg Pred Value : 1.0000            \n             Prevalence : 0.4667            \n         Detection Rate : 0.4667            \n   Detection Prevalence : 0.4667            \n      Balanced Accuracy : 1.0000            \n                                            \n       'Positive' Class : Budget Collars LLC\n                                            \n\n\nGreat, our model didn’t make any mistakes! It is probably ready to apply to our collars of unknown origin."
  },
  {
    "objectID": "modules/module_3/module3_5.html#predicting-the-unknown-collar-makers",
    "href": "modules/module_3/module3_5.html#predicting-the-unknown-collar-makers",
    "title": "3.5: K-Nearest Neighbor",
    "section": "Predicting the Unknown Collar Makers",
    "text": "Predicting the Unknown Collar Makers\nWe will apply the same model but now to the new data.\n\n# Making predictions for the collars without makers\nknn_predict &lt;- predict(knn_training_model, collars_to_label)\n\n# View results\nknn_predict\n\n [1] Budget Collars LLC Budget Collars LLC Budget Collars LLC Budget Collars LLC\n [5] Collarium Inc.     Collarium Inc.     Collarium Inc.     Collarium Inc.    \n [9] Collarium Inc.     Collarium Inc.     Budget Collars LLC Budget Collars LLC\n[13] Collarium Inc.     Budget Collars LLC Budget Collars LLC Collarium Inc.    \n[17] Budget Collars LLC Budget Collars LLC Collarium Inc.     Collarium Inc.    \n[21] Collarium Inc.     Collarium Inc.     Budget Collars LLC Budget Collars LLC\n[25] Collarium Inc.     Collarium Inc.     Budget Collars LLC Collarium Inc.    \n[29] Budget Collars LLC Budget Collars LLC Collarium Inc.     Collarium Inc.    \n[33] Budget Collars LLC Budget Collars LLC Budget Collars LLC Collarium Inc.    \n[37] Collarium Inc.     Collarium Inc.     Collarium Inc.     Collarium Inc.    \n[41] Budget Collars LLC Collarium Inc.     Budget Collars LLC Budget Collars LLC\n[45] Budget Collars LLC Collarium Inc.     Budget Collars LLC Collarium Inc.    \n[49] Collarium Inc.     Budget Collars LLC\nLevels: Budget Collars LLC Collarium Inc.\n\n# As a table\ntable(knn_predict)\n\nknn_predict\nBudget Collars LLC     Collarium Inc. \n                24                 26 \n\n\nWe have predictions! Our last step is to recombine our data so that the collars now have their new labels.\n\n# Let's bind the columns together\nnew_labels &lt;- bind_cols(unknown_collar_ids, knn_predict)\n\nNew names:\n• `` -&gt; `...2`\n\n# View collars now with known makers!\nnew_labels\n\n# A tibble: 50 × 2\n   collar_id ...2              \n       &lt;dbl&gt; &lt;fct&gt;             \n 1       129 Budget Collars LLC\n 2       138 Budget Collars LLC\n 3       134 Budget Collars LLC\n 4       140 Budget Collars LLC\n 5       104 Collarium Inc.    \n 6       118 Collarium Inc.    \n 7       105 Collarium Inc.    \n 8       127 Collarium Inc.    \n 9       111 Collarium Inc.    \n10       125 Collarium Inc.    \n# ℹ 40 more rows\n\n\nTypically, you would want to replace the columns in the entire dataframe, but we will end here for today :) Our goal of labeling our unlabeled collars is complete! We will deploy any that are now labeled as Collarium LLC."
  },
  {
    "objectID": "modules/module_3/module3_3.html",
    "href": "modules/module_3/module3_3.html",
    "title": "3.3: Comparing (Multiple) Means",
    "section": "",
    "text": "So far in this module, we have discussed comparing the means of two groups in multiple ways: numerically, visually, and statistically. Specifically, we learned about running t-tests to statistically compare the means of two groups.\nWhat if we have more than 2 groups, though? Often, we have 3, 4, 12…any number of groups that we might want to compare against each other. How do we do that?"
  },
  {
    "objectID": "modules/module_3/module3_3.html#anova",
    "href": "modules/module_3/module3_3.html#anova",
    "title": "3.3: Comparing (Multiple) Means",
    "section": "ANOVA",
    "text": "ANOVA\nWhat is an ANOVA? It stands for ANalysis Of VAriance.\nWhy is it called that? Well, ANOVAs compare the amount of variability in the data between and within groups. In plainer terms, it is looking at how different the data in each group are from each other (within) and also how different the groups are from the other groups (between).\nThe group of statistical tests called ANOVAs are extensions of the t-test, using the same concepts but allowing us to compare the means of more than two groups at a time. There are lots of different types of ANOVAs, but we are going to stick with the most simple version.\n\nWhen to use an ANOVA\nTo run an ANOVA, the following things need to be true:\n\nThe variable that we use to create our groups (independent variable) is categorical.\nThe variable that we want to compare between groups (dependent or response variable) is numerical.\n\nThese two aspects above might sound familiar, because they also need to be true for t-tests!\n\nWhen we only have 2 groups to compare, we can use a t-test.\nWhen we have more than 2 groups to compare, we use an ANOVA.\n\n\n\nHypothesis Testing\nLike t-tests, ANOVAs are another example of statistical hypothesis testing. We always want to formulate a null hypothesis and an alternative hypothesis. The generic version of these for any ANOVA are:\nNull Hypothesis (\\(H_{0}\\)): The means of the populations we sampled from are all equal.\nAlternative Hypothesis (\\(H_{A}\\)): The means of the populations we sampled from differ from each other."
  },
  {
    "objectID": "modules/module_3/module3_3.html#lets-run-an-example",
    "href": "modules/module_3/module3_3.html#lets-run-an-example",
    "title": "3.3: Comparing (Multiple) Means",
    "section": "Let’s Run an Example",
    "text": "Let’s Run an Example\nLast week, we ran t-tests on the battery life and signal distance variables in order to determine if there was a statistically significant difference between two groups (the collar makers).\nWhat if our large mammal team had deployed collars from three different companies rather than just two? How would we compare the means numerically, visually, and statistically? Let’s find out.\nFirst up, in small groups, work through the set-up, numeric, and visual sections. We’ll spend 15-20 minutes on this.\n\nSet-Up\nFirst, as always, we need to load the tidyverse and the read in our collars data. This time, use the file more_collars.csv.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\ncollars &lt;- read_csv(\"data/more_collars.csv\")\n\n\n\nConceptual\nWhat questions are we trying to answer about these collars?\n\nWhich variable is the independent variable? What about the dependent variable?\n\nLet’s write out our hypotheses.\nFirst, for battery life:\n\nNull hypothesis: There is no difference in the average battery life between the three collar makers.\nAlternative hypothesis: There is a difference in the average battery life the three collar makers.\n\nNext, for signal distance:\n\nNull hypothesis: There is no difference in the average signal distance between the three collar makers.\nAlternative hypothesis: There is a difference in the average signal distance between the three collar makers.\n\n\n\nNumeric\nLet’s calculate some summary values for our data to get some preliminary information about our collar companies.\nCalculate the means and the standard deviations (sd()) for the battery life and the signal distance for each collar maker.\n\nsummary_stats &lt;- collars %&gt;% \n  group_by(maker) %&gt;%                              # Group by maker\n  summarise(mean_battery = mean(battery_life),     # Mean (battery)\n            sd_battery = sd(battery_life),         # SD (battery)\n            mean_signal = mean(signal_distance),   # Mean (signal)\n            sd_signal = sd(signal_distance))       # SD (signal)\n\nsummary_stats\n\n# A tibble: 3 × 5\n  maker                     mean_battery sd_battery mean_signal sd_signal\n  &lt;chr&gt;                            &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Budget Collars LLC                86.8       9.64       4302.      31.4\n2 Collarium Inc.                   121.       11.8        4195.      35.8\n3 Darn Tuff Collars Company        118.       10.7        4172.      41.4\n\n\nLet’s examine the data. Do you think the collar companies are all different from each other? Are there pairs of companies that look more similar?\n\n\nVisual\nAs always, we love (&lt;3) visualizing our data, especially as a form of data exploration and comparing means.\nFirst, explore the distribution of battery life by maker using a multiple density plot (no need to add a line for the mean, but you can if you really want to!).\n\n# Battery life - density\nggplot(collars, aes(battery_life, fill = maker)) +\n  geom_density(alpha = 0.5)\n\n\n\n\nLet’s do a different type of visualization for signal distance. What type of data visualization have we covered that does a good job of showing both central tendency (median, in this case) and variation (middle 50% and 95% confidence intervals) in one continuous variable?\n\n# Signal distance - boxplot\nggplot(collars, aes(maker, signal_distance)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, alpha = 0.5)\n\n\n\n\nBased on the numbers and your visualizations, what is your prediction? Are the 3 groups different enough, whatever that means?\n\n\nStatistic\nNow that we have explored the data numerically and visually, let’s interrogate the data statistically.\nTo run an ANOVA, we use the aov() function. Some helpful tips about the aov() function:\n\nUnlike the t.test() function, the initial output for the aov() function isn’t particularly helpful. We will want to save it to an object and then look at it using the summary() function.\nLike t.test(), the syntax for aov() is dependent ~ independent.\n\nFirst, let’s run an ANOVA on the battery life variable.\n\n# ANOVA on battery life\nbattery_model &lt;- aov(data = collars, battery_life ~ maker)\n\n# Summary of model\nsummary(battery_model)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nmaker         2  36919   18460   161.2 &lt;2e-16 ***\nResiduals   147  16828     114                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor us, our main focus here in the ANOVA summary is on the maker row. We can ignore the Residuals.\nWe want to pay attention to the following:\n\nF-value: this is our test statistic. It determines our p-value (you don’t need to know how).\nPr(&gt;F): this is our p-value, which has roughly the same meaning as with a t-test.\n\nWe want to know if this value is larger or smaller than our cut-off value. We will stick with 0.05.\nThe notation here, Pr(&gt;F), is saying “What is the probability of us getting an F-value larger than the one we have by change alone?” If that probability is less that 0.05 (or 5%), then we reject the null hypothesis that the means are the same.\n\n***: the asterisks are reiterating what the p-value tells us, indicating just how small the p-value is.\n\nWhat is our conclusion? Are these means different?\n\nPractice\nHow about for signal life? What is your interpretation of these results?\n\n# ANOVA for signal distance\nsignal_model &lt;- aov(data = collars, signal_distance ~ maker)\n\n# Summary of model\nsummary(signal_model)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nmaker         2 499547  249773   189.1 &lt;2e-16 ***\nResiduals   147 194137    1321                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nPairwise Comparisons\nANOVAs are incredibly useful to tell you if there is a difference in the means of any of the groups. However, they do not tell you which means differ from another.\nTo figure that out, you need to use a class of tests called “post hoc tests,” meaning “after the event.” Post hoc tests take into account the fact that we are running multiple pairwise comparisons, which unfortunately increases our chance of error. To account for this, we need to adjust our p-values based on the number of pairwise comparisons we run.\nThe most common post hoc test for these pairwise comparisons is TukeyHSD(), but there are others depending on the specifics of your data set. You don’t need to worry about understanding the nuances of Tukey’s test, but you should know how to run the code and interpret it.\n\n# Pairwise comparisons for battery life\n# We take the results from the ANOVA and put it in TukeyHSD\nTukeyHSD(battery_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = battery_life ~ maker, data = collars)\n\n$maker\n                                                  diff       lwr       upr\nCollarium Inc.-Budget Collars LLC            34.667433 29.591701 39.743164\nDarn Tuff Collars Company-Budget Collars LLC 30.765879 25.771504 35.760254\nDarn Tuff Collars Company-Collarium Inc.     -3.901553 -9.048355  1.245248\n                                                 p adj\nCollarium Inc.-Budget Collars LLC            0.0000000\nDarn Tuff Collars Company-Budget Collars LLC 0.0000000\nDarn Tuff Collars Company-Collarium Inc.     0.1748636\n\n\nWhat do these different columns represent?\n\ndiff: the difference in the means for that pair\nlwr: the lower end of the 95% confidence interval\nupr: the upper end of the 95% confidence interval\np adj: the adjusted p-value for that pair. We interpret it as normal.\n\nHow should we interpret these results?\n\nAre Collarium and Budget significantly different in battery life? Yes (p &lt; 0.05)\nAre Darn Tuff and Budget significantly different in battery life? Yes (p &lt; 0.05)\nAre Darn Tuff and Collarium significantly different in battery life? No (p &gt; 0.05)\n\n\nPractice\nIn your groups, run the Tukey’s HSD test for signal distance and interpret the results.\n\n# Pairwise comparisons for signal distance\nTukeyHSD(signal_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = signal_distance ~ maker, data = collars)\n\n$maker\n                                                   diff        lwr        upr\nCollarium Inc.-Budget Collars LLC            -107.25643 -124.49632  -90.01655\nDarn Tuff Collars Company-Budget Collars LLC -130.34487 -147.30843 -113.38132\nDarn Tuff Collars Company-Collarium Inc.      -23.08844  -40.56972   -5.60716\n                                                p adj\nCollarium Inc.-Budget Collars LLC            0.000000\nDarn Tuff Collars Company-Budget Collars LLC 0.000000\nDarn Tuff Collars Company-Collarium Inc.     0.005997\n\n\nHow should we interpret these results?\n\nAre Collarium and Budget significantly different in signal distance? Yes (p &lt; 0.05)\nAre Darn Tuff and Budget significantly different in signal distance? Yes (p &lt; 0.05)\nAre Darn Tuff and Collarium significantly different in signal distance? Yes (p &lt; 0.05)"
  },
  {
    "objectID": "modules/module_3/module3_1.html",
    "href": "modules/module_3/module3_1.html",
    "title": "3.1: Leopard Seals",
    "section": "",
    "text": "Given the issues with our farmed fish, we will need to supplement our diets with some fish that are wild-caught for a while. These fish are caught by members of Team Antarctica, but there is quite a bit of risk involved.\n\nOne of the main hazards to these fishing teams are leopard seals. They’re apex predators in this ecosystem and are quite large, averaging 2.4–3.5 m (7.9–11.5 ft) in length and 200-600 kilograms (440-1,320 lb) in weight. Due to their large size and predatory nature, they can cause serious injury, especially when large schools of fish are involved (a researcher was killed by a leopard seal in Antarctica in 2003).\n\nOne way we’re working to remove some of this danger is by working with data from large mammal researchers. They have been tracking leopard seals using radio collars, one of many ways to track wildlife.\nWildlife Tracking Powerpoint\nThey have radio collared on a number of seals that live in areas that could be fished. Fishing boats are equipped with radios that can detect the presence of seals in the area and avoid high-risk sites.\nIn recent months, a problem has arisen. Some of the collars are failing, leading to some very close calls! While attempting to untangle a net, one team member was pulled into the water when a seal lunged at some fish trapped in the net.\nThe large mammal team replaces collars frequently based on the two manufacturers’ recommendations for battery life and general wear and tear, but it seems as if some of the collars are dying earlier than expected and putting our team in danger.\nWe’ve been tasked with determining why the units are failing and if we can tie it to a particular manufacturer.\n\n\n\nOur data on collars is in a file called “collar_data.csv.” Read in this data and our tidyverse.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\ncollars &lt;- read_csv(\"data/collar_data.csv\")"
  },
  {
    "objectID": "modules/module_3/module3_1.html#introduction-to-the-problem",
    "href": "modules/module_3/module3_1.html#introduction-to-the-problem",
    "title": "3.1: Leopard Seals",
    "section": "",
    "text": "Given the issues with our farmed fish, we will need to supplement our diets with some fish that are wild-caught for a while. These fish are caught by members of Team Antarctica, but there is quite a bit of risk involved.\n\nOne of the main hazards to these fishing teams are leopard seals. They’re apex predators in this ecosystem and are quite large, averaging 2.4–3.5 m (7.9–11.5 ft) in length and 200-600 kilograms (440-1,320 lb) in weight. Due to their large size and predatory nature, they can cause serious injury, especially when large schools of fish are involved (a researcher was killed by a leopard seal in Antarctica in 2003).\n\nOne way we’re working to remove some of this danger is by working with data from large mammal researchers. They have been tracking leopard seals using radio collars, one of many ways to track wildlife.\nWildlife Tracking Powerpoint\nThey have radio collared on a number of seals that live in areas that could be fished. Fishing boats are equipped with radios that can detect the presence of seals in the area and avoid high-risk sites.\nIn recent months, a problem has arisen. Some of the collars are failing, leading to some very close calls! While attempting to untangle a net, one team member was pulled into the water when a seal lunged at some fish trapped in the net.\nThe large mammal team replaces collars frequently based on the two manufacturers’ recommendations for battery life and general wear and tear, but it seems as if some of the collars are dying earlier than expected and putting our team in danger.\nWe’ve been tasked with determining why the units are failing and if we can tie it to a particular manufacturer.\n\n\n\nOur data on collars is in a file called “collar_data.csv.” Read in this data and our tidyverse.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\ncollars &lt;- read_csv(\"data/collar_data.csv\")"
  },
  {
    "objectID": "modules/module_3/module3_1.html#explore-the-data",
    "href": "modules/module_3/module3_1.html#explore-the-data",
    "title": "3.1: Leopard Seals",
    "section": "Explore the Data",
    "text": "Explore the Data\n\nTwo manufacturers\nBattery life: the average number of days a particular collar lasts (this is recorded in the unit and stored when the battery dies)\nSignal distance: the maximum signal distance at which that particular collar has been recorded\nFail: collars that have failed in the past (e.g. they’ve been recovered from seals that were noticed by the team but that didn’t ping the radio equipment).\n\n\nGroup Challenge 1: Summarizing\nLet’s practice. Write some code to do the following:\n\nCalculate the mean and standard deviation for:\n\nbattery life\nsignal distance\n\nDo this for both manufacturers\n\n\nbat_life_and_signal &lt;- collars %&gt;%\n  group_by(maker) %&gt;%                                 # Group by manufacturer\n  summarize(bat_life_mean = mean(battery_life),       # Mean (battery)\n            bat_life_sd = sd(battery_life),           # SD (battery)\n            signal_mean = mean(signal_distance),      # Mean (signal)\n            signal_sd = min(signal_distance))         # SD (signal)\n\n# View results\nbat_life_and_signal\n\n# A tibble: 2 × 5\n  maker              bat_life_mean bat_life_sd signal_mean signal_sd\n  &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Budget Collars LLC          86.8        9.64       4302.     4222.\n2 Collarium Inc.             121.        11.8        4195.     4127.\n\n\nWe also might want to count up how many collar failures are attributed to each maker.\n\n# Number of failed collars per maker\n\n# Option 1: summarize + sum\ncollars %&gt;% \n  group_by(maker) %&gt;% \n  summarize(fail_count = sum(fail))\n\n# A tibble: 2 × 2\n  maker              fail_count\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Budget Collars LLC         16\n2 Collarium Inc.              4\n\n# Option 2: count\ncollars %&gt;% \n  group_by(maker) %&gt;% \n  count(fail)\n\n# A tibble: 4 × 3\n# Groups:   maker [2]\n  maker               fail     n\n  &lt;chr&gt;              &lt;dbl&gt; &lt;int&gt;\n1 Budget Collars LLC     0    37\n2 Budget Collars LLC     1    16\n3 Collarium Inc.         0    43\n4 Collarium Inc.         1     4\n\n\nLooks like one of these makers is definitely the one having some problems!"
  },
  {
    "objectID": "modules/module_3/module3_1.html#visualizing-the-data",
    "href": "modules/module_3/module3_1.html#visualizing-the-data",
    "title": "3.1: Leopard Seals",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nAfter summarizing our data, our next step in our data exploration is usually data visualization. Let’s practice some plotting in ggplot2 to remind ourselves how this works!\n\nOne Continuous Variable at a Time\nWhat types of plots can we use to explore one continuous variable (and one categorical variable — in this case, maker)?\n\nMultiple histogram\nMultiple density plot\nBox-and-whisker plot\n\nWe can start with a histogram of the battery life of the collars.\n\n# Histogram\nggplot(collars, aes(battery_life)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe probably want to add in the collar maker to see some differences there.\n\n# Histogram by maker\nggplot(collars, aes(battery_life, fill = maker)) +\n  geom_histogram(alpha = 0.5, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWhat if we want to add the mean value of each collar maker to the plot? How do we do that?\nOne of the neat things about ggplot2 is that you can actually reference different data frames within the same plot. We’ve already calculated the mean values for each collar maker and saved those values in a data frame called bat_life_and_signal. We can now reference that data frame and those values and add them to our plot.\nWe will use the geom_vline() function to add a vertical line onto our plot at the point in the x-axis that represents the mean value. In this function, we will specifically reference the bat_life_and_signal data frame.\nLet’s take a look at the help file for geom_vline() to see what arguments it takes. Scroll down to the “Aesthetics” section.\n\n# What is geom_vline?\n? geom_vline\n\nIt looks like we will need to specify the data and the x-intercept, the place on the x-axis that matches the mean value.\n\n# Histogram by maker, with mean line\nggplot(collars, aes(battery_life, fill = maker)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  geom_vline(data = bat_life_and_signal, aes(xintercept = bat_life_mean))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNice! We have name the data argument in the geom_vline() function because otherwise the function assumes we still want to use the data from the ggplot() function and gets confused. We are essentially overwriting that data and telling geom_vline() that we need to use a different data frame.\nLet’s wrap this up and make it extra nice by adding color, labels, and a theme.\n\n# Histogram by maker, with mean line and labels\nggplot(collars, aes(battery_life, fill = maker)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  geom_vline(data = bat_life_and_signal, aes(xintercept = bat_life_mean, color = maker)) + \n  labs(x = \"Battery Life\",\n       y = \"Frequency\",\n       color = \"Maker\",\n       fill = \"Maker\") +\n  theme_light()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nGroup Challenge!\nMake the same plot that we made above expect for the signal distance.\n\n# Same as above, but for signal distance\nggplot(collars, aes(signal_distance, fill = maker)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  geom_vline(data = bat_life_and_signal, aes(xintercept = signal_mean, color = maker)) + \n  labs(x = \"Signal Distance\",\n       y = \"Frequency\",\n       color = \"Maker\",\n       fill = \"Maker\") +\n  theme_light()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nTwo Continuous Variables at a Time\nWe’ve explored both battery life and signal distance individually. Now let’s bring them together into one plot to see what the relationship between them is like. What type of plot will be use for that?\n\n# Scatterplot\nggplot(collars, aes(x = signal_distance, y = battery_life, color = maker)) +\n  geom_point()\n\n\n\n\nHow do we interpret this plot? What is it telling us about the relationship between signal distance and battery life? What is it telling us about that relationship between the two collar makers?\nAs it turns out, this trade-off between signal distance and battery length is very real when it comes to wildlife tracking! Why do you think that is?"
  },
  {
    "objectID": "modules/module_2/module2_6.html",
    "href": "modules/module_2/module2_6.html",
    "title": "2.6: Exploring geom Functions",
    "section": "",
    "text": "Head over to data-to-viz.com and do some exploration! In your groups, choose one of the following types of plots. Hunt down as many locations in the flow chart where your plot type is found as you can find.\n\nhistogram: geom_histogram()\nscatter plot: geom_point()\nbox-and-whisker: geom_boxplot() + geom_jitter()\n\nIf you’ve finished, click on your plot type and read more about it. Check out the examples that were made in R, including many using ggplot2. Take note of the following:\n\nwhich geom function is being used to create the plot\nyou’ll see a number of other things in the ggplot code, some which will be new. What are they? Any idea what they are doing?\n\n\n\n\n\nLet’s practice making different kinds of plots with various geom functions to see how they work.\n\nlibrary(tidyverse)\nfish &lt;- read_csv(\"data/fish_sick_data.csv\")\n\n\n\nFirst, let’s remind ourselves of the general structure of how we make plots using the ggplot2 syntax.\n\n# ggplot(data = &lt;DATA&gt;, mapping = aes(x = &lt;COLUMN1&gt;, y = &lt;COLUMN2)) +\n# geom_function() +\n# labs() +\n# theme()\n\n\n\n\n\nAs we learned last modules, histograms are plots which let us look at one continuous variable.\nThey help us get a feel for the distribution of that data. To make histograms in ggplot2, we use the geom_histogram() function. Let’s look at the number of fish.\n\nggplot(fish, aes(num_fish)) +\n  geom_histogram(bins = 20)\n\n\n\n# we can change the number of bins (essentially, the number of columns) by modifying the bins argument in the geom_histogram function\n\n\n\nMake a histogram of the number of sick fish in the tanks. Create the histogram with 10 bins (10 groupings).\n\nggplot(fish, aes(num_sick)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nWhen we create a multiple histogram, we have to add one additional argument to geom_histogram(). Let’s see what happens if we just specify fill.\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10)\n\n\n\n\nWhat is happening in the column with both green and red? Perhaps the teal histogram is in front of the red histogram and as blocking us from seeing some red?\nLet’s change the transparency using an argument called alpha, which allows us to make layers transparent. The scale for alpha goes from 0 (completely transparent) to 1 (not transparent at all).\nWe can set the transparency to 0.5 to see if there is any overlap.\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5)\n\n\n\n\nNot too much changed. It still doesn’t look like we can see any red points behind the teal. Perhaps the teal values and red values are stacked on top of one another?\nLet’s take a look at what happens when we add the argument position = \"identity\".\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\")\n\n\n\n\nAha! This is different from above. Instead of red being stacked vertically on top of teal, we can now see that the red values start at 0 on the y-axis and are overlapping with the teal.\nThe position = \"identity\" argument tell geom_histogram to plot the data for each group starting from 0 in the y-axis rather than stacking values from the same group, which is the default.\nAny time we plot a multiple histogram, we need to change the transparency and add position = \"identity\" when creating a multiple histogram; if we don’t, we won’t see potential overlap!\n\n\nMake a multiple histogram of the number of sick fish per species. Make sure your plot has 10 bins, is partially transparent, and the data are not stacked.\n\nggplot(fish, aes(num_sick, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\n\nA quick note about the aes() function. It’s one of the more confusing bits of ggplot2.\nWhen do I put the color (or size or linetype or fill or whatever) inside the aes() function versus in the geom function but outside of aes()? When we made our density plots, why did the color argument go inside of aes() but alpha went outside?\nEssentially, it boils down to this:\n\nif you want something (color, size, etc.) on your plot to change based on a variable from a data frame, you will want to put the argument within the aes() function.\nif you want something (color, size, etc.) on the plot to be constant, you will specify it outside of the function.\n\nFor some additional examples and explanation, check out this Stack Overflow page.\n\n\n\nAs a reminder, we use the geom_point() function to make a scatter plot of the relationship between two continuous variables.\n\nggplot(fish, aes(avg_daily_temp, num_fish)) +\n  geom_point()\n\n\n\n\n\n\nUsing what you’ve learned about making histograms, see if you can create a “multiple scatterplot,” where the color of the points are determined by the fish species.\nHint: you’ll want to use an argument called color.\n\nggplot(fish, aes(avg_daily_temp, num_fish, color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nBox-and whisker-plots (also known as box plots) are another great option for looking at one continuous variable and one or more categorical variables. They are particularly nice when you want to see measures of central tendency and variation in the same plot.\nLet’s build one and then talk through what each component means. We use geom_boxplot to make these types of plots.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() \n\n\n\n\nSo what does the box represent? And the whiskers?\n\nthe box represents the middle 50% of the values in the data set.\nthe line that runs through the middle of the box represents the median (middle value) of the data\nthe whiskers represent the spread of the data (we won’t get into the mathematical details of exactly how they are calculated) and are roughly comparable to 95% confidence intervals (we will cover this in another module)\nvalues that fall outside of the whiskers can be considered outliers and are plotted individually\n\n\n\n\nOne of the beautiful parts of working with ggplot2 is that you can add multiple layers to each plot.\nOne of the key things missing from box-and-whisker plots is any indication of how many data points we have. In the plot above, there could be 5 tanks per species or 500 tanks per species.\nHow can we add an indication of how many points there are? We can layer each individual data point on top of the boxes!\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.5)\n\n\n\n\nThis is nice, but there is still some overlap in points that makes it hard for us to see exactly how many points are there.\nThe geom_jitter() function is a special version of geom_point(). It adds a little bit of randomness to the points (both horizontally and vertically) so that they don’t overlap as much.\nWe can control how much randomness we allow with the width and/or height arguments. I usually ignore height and set the width argument to 0.1.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) \n\n\n\n\nThat is looking really nice! We can keep improving it, though, with better labels for the axes and the legend as well as a nice theme.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Number of Fish Per Tank\",\n       color = \"Species\") +\n  theme_light()\n\n\n\n\n\n\n\nWe have used two different arguments in the aes() function to specify that we want to colors in our plots to change based on one of the columns in our dataset. When do you want to use which one?\n\n\nFor the most part, the color argument refers to the color of the lines in a plot (e.g., in box plots, the colors of the lines change).\nThe one exception to this is in geom_point(). We change the color of the points with the color argument, as well.\n\n\n\nWhen there is space that we want to fill in with color depending on the values in a column, we want to use the fill argument.\nFor histograms, the fill argument changes the color inside the bins. If we use color in a histogram instead, only the outlines of the columns will change, but the columns will remain filled with gray.\nIn boxplots, the inside area of the box will be filled with color, but the lines will stay black.\n\n\n\n\nIf we have specified either color or fill (or both) in the aes() function, ggplot will automatically create a legend for us. The key is determined by whichever argument we used.\nSo, if you use the color argument in the aes() function, you would want to use the color argument in the labs() function to change the title of the legend. If you used the fill argument, you would then use the fill argument in the labs() function.\nIf you have both color and fill in aes(), you will need to add both arguments to the labs() function. If you give the same label to each, it will create one key.\n\n\nMake 2 different types of plots with the same data: the average daily temperature of the tanks and fish species.\nFirst, think about the variables we are plotting. How many are there? Are they qualitative or quantitative?\nBased on those answers, determine which plot types you can produce for those variable types.\nNow, make your two plot! To each, add labels and a theme.\n\n# Boxplot\nggplot(fish, aes(species, avg_daily_temp, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Average Daily Temp (C)\",\n       y = \"Frequency\", \n       color = \"Fish Species\") +\n  theme_bw()\n\n\n\n# Histogram\nggplot(fish, aes(avg_daily_temp, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\") + \n  labs(x = \"Average Daily Temp (C)\", \n       y = \"Frequency\",\n       fill = \"Fish Species\") +\n  theme_bw()\n\n\n\n\nFinished? Let’s add another variable. Now, we want to plot average daily temperatures, the number of sick fish, and the fish species. Work through the same steps as above.\n\nggplot(fish, aes(avg_daily_temp, num_sick, color = species)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Average Daily Temp (C)\",\n       y = \"Number of Sick Fish per Tank\",\n       color = \"Fish Species\") +\n  theme_light()\n\n\n\n\n\n\n\n\nLet’s summarize some of what we’ve learned in this module\n\n\nGood for looking at the distribution of one continuous variable\n\none continuous variable (x-axis)\n\n\n\n\nGood for looking at differences in the distributions of one continuous variable based on a categorical variable\n\none continuous variable (x-axis)\none categorical variable via the fill or color argument in the aes() function\nwe always want to add transparency (alpha) and, for histograms, position = \"identity\"\n\n\n\n\nGood for looking for the relationship between two continuous variables\n\ntwo continuous variables (x-axis and y-axis)\ncan add in a categorical variable via aes(), but the main relationship is between the two continuous variables\n\n\n\n\nGood for looking at measures of central tendency and variation for a continuous variable and the differences between those measures between categories\n\none continuous variable (y-axis)\nat least one categorical (x-axis and additional via aes())\n\n\n\n\nWe can add multiple layers to ggplots, which is part of what makes them so useful!\n\nwe can add multiple geom functions to a single plot\nwe use the labs() function to rename axes labels and legends\nwe use a theme function to make the plot more aesthetically pleasing and easier to understand"
  },
  {
    "objectID": "modules/module_2/module2_6.html#plotting-in-ggplot2",
    "href": "modules/module_2/module2_6.html#plotting-in-ggplot2",
    "title": "2.6: Exploring geom Functions",
    "section": "",
    "text": "Head over to data-to-viz.com and do some exploration! In your groups, choose one of the following types of plots. Hunt down as many locations in the flow chart where your plot type is found as you can find.\n\nhistogram: geom_histogram()\nscatter plot: geom_point()\nbox-and-whisker: geom_boxplot() + geom_jitter()\n\nIf you’ve finished, click on your plot type and read more about it. Check out the examples that were made in R, including many using ggplot2. Take note of the following:\n\nwhich geom function is being used to create the plot\nyou’ll see a number of other things in the ggplot code, some which will be new. What are they? Any idea what they are doing?\n\n\n\n\n\nLet’s practice making different kinds of plots with various geom functions to see how they work.\n\nlibrary(tidyverse)\nfish &lt;- read_csv(\"data/fish_sick_data.csv\")\n\n\n\nFirst, let’s remind ourselves of the general structure of how we make plots using the ggplot2 syntax.\n\n# ggplot(data = &lt;DATA&gt;, mapping = aes(x = &lt;COLUMN1&gt;, y = &lt;COLUMN2)) +\n# geom_function() +\n# labs() +\n# theme()\n\n\n\n\n\nAs we learned last modules, histograms are plots which let us look at one continuous variable.\nThey help us get a feel for the distribution of that data. To make histograms in ggplot2, we use the geom_histogram() function. Let’s look at the number of fish.\n\nggplot(fish, aes(num_fish)) +\n  geom_histogram(bins = 20)\n\n\n\n# we can change the number of bins (essentially, the number of columns) by modifying the bins argument in the geom_histogram function\n\n\n\nMake a histogram of the number of sick fish in the tanks. Create the histogram with 10 bins (10 groupings).\n\nggplot(fish, aes(num_sick)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nWhen we create a multiple histogram, we have to add one additional argument to geom_histogram(). Let’s see what happens if we just specify fill.\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10)\n\n\n\n\nWhat is happening in the column with both green and red? Perhaps the teal histogram is in front of the red histogram and as blocking us from seeing some red?\nLet’s change the transparency using an argument called alpha, which allows us to make layers transparent. The scale for alpha goes from 0 (completely transparent) to 1 (not transparent at all).\nWe can set the transparency to 0.5 to see if there is any overlap.\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5)\n\n\n\n\nNot too much changed. It still doesn’t look like we can see any red points behind the teal. Perhaps the teal values and red values are stacked on top of one another?\nLet’s take a look at what happens when we add the argument position = \"identity\".\n\nggplot(fish, aes(num_fish, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\")\n\n\n\n\nAha! This is different from above. Instead of red being stacked vertically on top of teal, we can now see that the red values start at 0 on the y-axis and are overlapping with the teal.\nThe position = \"identity\" argument tell geom_histogram to plot the data for each group starting from 0 in the y-axis rather than stacking values from the same group, which is the default.\nAny time we plot a multiple histogram, we need to change the transparency and add position = \"identity\" when creating a multiple histogram; if we don’t, we won’t see potential overlap!\n\n\nMake a multiple histogram of the number of sick fish per species. Make sure your plot has 10 bins, is partially transparent, and the data are not stacked.\n\nggplot(fish, aes(num_sick, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\n\nA quick note about the aes() function. It’s one of the more confusing bits of ggplot2.\nWhen do I put the color (or size or linetype or fill or whatever) inside the aes() function versus in the geom function but outside of aes()? When we made our density plots, why did the color argument go inside of aes() but alpha went outside?\nEssentially, it boils down to this:\n\nif you want something (color, size, etc.) on your plot to change based on a variable from a data frame, you will want to put the argument within the aes() function.\nif you want something (color, size, etc.) on the plot to be constant, you will specify it outside of the function.\n\nFor some additional examples and explanation, check out this Stack Overflow page.\n\n\n\nAs a reminder, we use the geom_point() function to make a scatter plot of the relationship between two continuous variables.\n\nggplot(fish, aes(avg_daily_temp, num_fish)) +\n  geom_point()\n\n\n\n\n\n\nUsing what you’ve learned about making histograms, see if you can create a “multiple scatterplot,” where the color of the points are determined by the fish species.\nHint: you’ll want to use an argument called color.\n\nggplot(fish, aes(avg_daily_temp, num_fish, color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nBox-and whisker-plots (also known as box plots) are another great option for looking at one continuous variable and one or more categorical variables. They are particularly nice when you want to see measures of central tendency and variation in the same plot.\nLet’s build one and then talk through what each component means. We use geom_boxplot to make these types of plots.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() \n\n\n\n\nSo what does the box represent? And the whiskers?\n\nthe box represents the middle 50% of the values in the data set.\nthe line that runs through the middle of the box represents the median (middle value) of the data\nthe whiskers represent the spread of the data (we won’t get into the mathematical details of exactly how they are calculated) and are roughly comparable to 95% confidence intervals (we will cover this in another module)\nvalues that fall outside of the whiskers can be considered outliers and are plotted individually\n\n\n\n\nOne of the beautiful parts of working with ggplot2 is that you can add multiple layers to each plot.\nOne of the key things missing from box-and-whisker plots is any indication of how many data points we have. In the plot above, there could be 5 tanks per species or 500 tanks per species.\nHow can we add an indication of how many points there are? We can layer each individual data point on top of the boxes!\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_point(alpha = 0.5)\n\n\n\n\nThis is nice, but there is still some overlap in points that makes it hard for us to see exactly how many points are there.\nThe geom_jitter() function is a special version of geom_point(). It adds a little bit of randomness to the points (both horizontally and vertically) so that they don’t overlap as much.\nWe can control how much randomness we allow with the width and/or height arguments. I usually ignore height and set the width argument to 0.1.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) \n\n\n\n\nThat is looking really nice! We can keep improving it, though, with better labels for the axes and the legend as well as a nice theme.\n\nggplot(fish, aes(species, num_fish, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Number of Fish Per Tank\",\n       color = \"Species\") +\n  theme_light()\n\n\n\n\n\n\n\nWe have used two different arguments in the aes() function to specify that we want to colors in our plots to change based on one of the columns in our dataset. When do you want to use which one?\n\n\nFor the most part, the color argument refers to the color of the lines in a plot (e.g., in box plots, the colors of the lines change).\nThe one exception to this is in geom_point(). We change the color of the points with the color argument, as well.\n\n\n\nWhen there is space that we want to fill in with color depending on the values in a column, we want to use the fill argument.\nFor histograms, the fill argument changes the color inside the bins. If we use color in a histogram instead, only the outlines of the columns will change, but the columns will remain filled with gray.\nIn boxplots, the inside area of the box will be filled with color, but the lines will stay black.\n\n\n\n\nIf we have specified either color or fill (or both) in the aes() function, ggplot will automatically create a legend for us. The key is determined by whichever argument we used.\nSo, if you use the color argument in the aes() function, you would want to use the color argument in the labs() function to change the title of the legend. If you used the fill argument, you would then use the fill argument in the labs() function.\nIf you have both color and fill in aes(), you will need to add both arguments to the labs() function. If you give the same label to each, it will create one key.\n\n\nMake 2 different types of plots with the same data: the average daily temperature of the tanks and fish species.\nFirst, think about the variables we are plotting. How many are there? Are they qualitative or quantitative?\nBased on those answers, determine which plot types you can produce for those variable types.\nNow, make your two plot! To each, add labels and a theme.\n\n# Boxplot\nggplot(fish, aes(species, avg_daily_temp, color = species)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Average Daily Temp (C)\",\n       y = \"Frequency\", \n       color = \"Fish Species\") +\n  theme_bw()\n\n\n\n# Histogram\nggplot(fish, aes(avg_daily_temp, fill = species)) +\n  geom_histogram(bins = 10, alpha = 0.5, position = \"identity\") + \n  labs(x = \"Average Daily Temp (C)\", \n       y = \"Frequency\",\n       fill = \"Fish Species\") +\n  theme_bw()\n\n\n\n\nFinished? Let’s add another variable. Now, we want to plot average daily temperatures, the number of sick fish, and the fish species. Work through the same steps as above.\n\nggplot(fish, aes(avg_daily_temp, num_sick, color = species)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Average Daily Temp (C)\",\n       y = \"Number of Sick Fish per Tank\",\n       color = \"Fish Species\") +\n  theme_light()\n\n\n\n\n\n\n\n\nLet’s summarize some of what we’ve learned in this module\n\n\nGood for looking at the distribution of one continuous variable\n\none continuous variable (x-axis)\n\n\n\n\nGood for looking at differences in the distributions of one continuous variable based on a categorical variable\n\none continuous variable (x-axis)\none categorical variable via the fill or color argument in the aes() function\nwe always want to add transparency (alpha) and, for histograms, position = \"identity\"\n\n\n\n\nGood for looking for the relationship between two continuous variables\n\ntwo continuous variables (x-axis and y-axis)\ncan add in a categorical variable via aes(), but the main relationship is between the two continuous variables\n\n\n\n\nGood for looking at measures of central tendency and variation for a continuous variable and the differences between those measures between categories\n\none continuous variable (y-axis)\nat least one categorical (x-axis and additional via aes())\n\n\n\n\nWe can add multiple layers to ggplots, which is part of what makes them so useful!\n\nwe can add multiple geom functions to a single plot\nwe use the labs() function to rename axes labels and legends\nwe use a theme function to make the plot more aesthetically pleasing and easier to understand"
  },
  {
    "objectID": "modules/module_2/module2_4.html",
    "href": "modules/module_2/module2_4.html",
    "title": "2.4: A Visualization Primer",
    "section": "",
    "text": "As a group, take 5 minutes to compare your notes and come to a group consensus on your top 3 reasons. Be sure to choose someone to report out!"
  },
  {
    "objectID": "modules/module_2/module2_4.html#why-does-data-visualization-matter",
    "href": "modules/module_2/module2_4.html#why-does-data-visualization-matter",
    "title": "2.4: A Visualization Primer",
    "section": "",
    "text": "As a group, take 5 minutes to compare your notes and come to a group consensus on your top 3 reasons. Be sure to choose someone to report out!"
  },
  {
    "objectID": "modules/module_2/module2_4.html#types-of-visualization",
    "href": "modules/module_2/module2_4.html#types-of-visualization",
    "title": "2.4: A Visualization Primer",
    "section": "Types of Visualization",
    "text": "Types of Visualization\n\nVisual cues for communicating data\nHere is a decent overview of some of the core concepts of data visualization.\nThis website is pretty great and goes into a lot of detail about good practices in data visualization. If this is something that really piques your interest, I encourage you to check it out!\nIn this course, our main take-away from this website is the use of visual cues to communicate data and which ones are better than others.\n\n\n\nSmall group activity\nIn groups, discuss as many all types of visualization that come to mind. It’s okay if you don’t know what they are called! Make a quick list or draw them out if you prefer.\nTypes of data visualizations:\nThis website is an amazing reference for data visualization methods and when to use what. It also has examples of each type plotted in ggplot2.\nWe will talk more about how to choose the right visualization for your data now and also in the rest of the module.\n\n\nData matching activity\nStill in your groups, open the PDF called “data_viz_matching”\nSpend a few minutes seeing if you can match the data descriptions to the types of data visualizations.\nEach “dataset” is pulled from the same overall dataset: measurements from 344 individual penguins from different species and on different islands over 3 years. Some datasets created multiple plots.\n\nPenguin body mass and flipper lengths\nPenguin flipper lengths\nPenguin species and flipper lengths\nPenguins body mass, species, and flipper lengths\n\n\n\nMatches\n\n\n\n\n\n\n\n\nSee the PowerPoint on D2L for full details!"
  },
  {
    "objectID": "modules/module_2/module2_4.html#figure-critique",
    "href": "modules/module_2/module2_4.html#figure-critique",
    "title": "2.4: A Visualization Primer",
    "section": "Figure Critique",
    "text": "Figure Critique\nFirst, read through this blog post on the “Dos and Don’ts of Data Visualization”\nNow that you have some insights, let’s critique these figures below. Yes, these are actual figures in the wild…\n\nWhat aspects don’t work?\nWhat aspects do work?\nHow would you present the data?\n\nFigures:"
  },
  {
    "objectID": "modules/module_2/module2_2.html",
    "href": "modules/module_2/module2_2.html",
    "title": "2.2: Writing Functions",
    "section": "",
    "text": "Students will be able to apply fundamental components of the biology of recirculating aquaculture systems to hypothesize how these components could lead to unhealthy fish.\nStudents will be able to write custom functions that perform simple tasks.\nStudents will be able to describe the utility of iteration."
  },
  {
    "objectID": "modules/module_2/module2_2.html#learning-outcomes",
    "href": "modules/module_2/module2_2.html#learning-outcomes",
    "title": "2.2: Writing Functions",
    "section": "",
    "text": "Students will be able to apply fundamental components of the biology of recirculating aquaculture systems to hypothesize how these components could lead to unhealthy fish.\nStudents will be able to write custom functions that perform simple tasks.\nStudents will be able to describe the utility of iteration."
  },
  {
    "objectID": "modules/module_2/module2_2.html#lecture-code-along",
    "href": "modules/module_2/module2_2.html#lecture-code-along",
    "title": "2.2: Writing Functions",
    "section": "⊳ Lecture / Code-along",
    "text": "⊳ Lecture / Code-along\n\nAquaculture, Immunology, and Disease\nWhat is aquaculture anyway, and how might it contribute to disease? Here’s a link to the presentation.\nData Exploration\nGiven the correlation we found earlier, it does seem like the fish are our likely culprit.\nSince we’ve narrowed down the issue, we should ask our aquaculture specialists to provide us with some data about the tanks.\nGroup Brainstorm\nBased on what we know about aquaculture systems, what types of data should we ask for to get to the bottom of this issue? Spend about 3 minutes brainstorming with your group and be ready to report back.\nThe Data We Have\n\n\n# Load the tidyverse\nlibrary(tidyverse)\n\n# Read in the data\ntank_data &lt;- read_csv(\"data/fish_tank_data.csv\")\n\n# Look at the data\nglimpse(tank_data)\n\nRows: 1,000\nColumns: 7\n$ tank_id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ species        &lt;chr&gt; \"tilapia\", \"tilapia\", \"tilapia\", \"tilapia\", \"tilapia\", …\n$ avg_daily_temp &lt;dbl&gt; 23.57285, 23.75280, 23.07280, 23.74601, 24.40522, 23.25…\n$ num_fish       &lt;dbl&gt; 105, 102, 109, 98, 103, 97, 101, 99, 102, 95, 100, 102,…\n$ day_length     &lt;dbl&gt; 10, 10, 10, 12, 10, 10, 11, 10, 9, 10, 10, 12, 11, 11, …\n$ tank_volume    &lt;dbl&gt; 400.7182, 399.4300, 399.4878, 400.8874, 399.8796, 399.8…\n$ size_day_30    &lt;dbl&gt; 2779.726, 2785.846, 2781.342, 2784.785, 2786.340, 2782.…\n\n\n\nDiscussion of the problem\nTrout are a cold-water species whereas tilapia are a warm-water species. After doing some reading, we’ve come up with some critical values for each species.\nWater temperatures below 59º F for trout and below 75º F for tilapia are critically low temperatures and can result in suppression of the immune system.\nGreat, we now know what the critical temperature cutoffs are for both species — but there’s a problem. The cut-off temperatures that we have are in Fahrenheit but the temperature values in our data frame are in Celsius.\nOur goal with this data is to ascertain if any of the tanks are below the critical temperature. Before we can do this, though, we need to convert tank temperatures from Celsius to Fahrenheit. Luckily for us, we’ve already done this before! (Hint: remember the mutate() function?).\nNow, we are going to run through a slightly different way to accomplish this task. It still involves mutate(), but we need to take some additional steps first before we use it.\nCustom Functions\nWe’ve already used functions a lot in this class, and it turns out that we can write our own functions to perform specific tasks.\nThis has lot of varied uses and can be very powerful. The key takeaway here, though, is that we can write a function that converts Celsius to Fahrenheit and apply it to our entire data set in a few lines of code.\nGeneral Syntax Example\nMaking a function involves creating local variables that act as a stand-in for the values you want to use later on. In the case below, our local variables are x, y, and z. You can see that they are local because they only circulate within the function and are not being pulled in from outside. However, once we are done defining our function we can assign values to the input variables (in this case, x and y) that the function will do its magic with. You can see this in the example chunk below, where x = 2 and y = 4. All manipulations done with the input variables are contained between the curly brackets ( { } ), and of course, we signify the start a function with function().\nFunction syntax follows the structure in the code chunk below, which can be viewed as:\nname_of_new_function &lt;- function(input1, input2){\nwhat do you want to do with your input variables? (add, subtract, etc.)\noutput &lt;- input1 + input2\nreturn(output)\n}\nAnd we can see it in action here:\n\n\n# General syntax\nnew_sum &lt;- function(x,y){\n  z &lt;- x + y\n  return(z)\n}\n\n# Examples when using the new function\nnew_sum(x = 2, y = 4)\n\n[1] 6\n\nnew_sum(2, 4)\n\n[1] 6\n\nnew_sum(7, 19)\n\n[1] 26\n\n\n\nBuilding Our Custom Function\nIn your groups, build a custom function called c_to_f() that converts a single value of Celsius to Fahrenheit. Remember, the equation is Celsius = Fahrenheit * (9/5) + 32. Instead of two arguments like we have above (x, y), this function will only require one input value (c).\nBe prepared to show your code and run some test values.\n\n\nc_to_f &lt;- function(c = NULL){\n  f &lt;- (c * (9/5)) + 32\n  return(f)\n}\n\nc_to_f(c = 40)\n\n[1] 104\n\n\n\nThinking about Iteration\nSo, we have a great function that we can use to convert a Celsius value to Fahrenheit, but how do we apply that to every value in the temperature column, not just one? We’ve built the function for one value, not for a whole vector of values…\nThis is where mutate() comes in! As we’ve seen in the past, mutate() applies whatever function or mathematical formula we’ve given to it to each value in the column. It essentially is going row by row: applying the function or formula to one row, returning the new value in a new column, then moving to the next row to do the same.\n\n\n# Let's first demonstrate applying our new function to a vector\nc_to_f(tank_data$avg_daily_temp)\n\n   [1] 74.43113 74.75503 73.53104 74.74281 75.92940 73.86383 74.31040 76.83799\n   [9] 74.61511 74.87889 75.97796 75.12147 73.19913 73.29417 73.93870 74.66364\n  [17] 74.83470 75.66626 73.70134 75.70409 76.46928 74.38196 75.24622 75.11160\n  [25] 74.12701 74.80457 75.34147 74.86631 76.01130 75.44187 74.71827 75.01918\n  [33] 76.51237 76.51730 73.27106 73.85835 74.27350 75.86871 73.20866 74.63055\n  [41] 75.45388 73.82945 74.25658 74.59991 72.82957 75.27582 75.93890 74.83474\n  [49] 74.41354 73.97254 75.02370 74.11599 76.44499 74.47102 75.26821 76.40062\n  [57] 75.50111 75.18067 76.86133 75.24810 74.88691 75.63106 75.25761 73.64460\n  [65] 75.89521 74.11780 75.15073 76.32487 74.95296 74.79653 76.51101 75.83362\n  [73] 75.55690 75.14052 74.82020 75.08606 75.52449 76.95815 75.80831 73.10074\n  [81] 75.02855 76.22341 75.64616 75.23696 75.17291 74.70379 74.43949 75.45942\n  [89] 75.49829 74.67938 74.98023 73.80162 74.95824 74.66393 76.18906 74.98713\n  [97] 74.83729 76.66777 75.90083 74.58555 76.30435 76.06749 75.01741 74.85544\n [105] 74.43440 75.31092 74.50235 74.78281 74.95587 75.59466 74.66381 74.97921\n [113] 73.58013 75.04360 73.21218 74.02326 73.64192 75.28364 74.45243 73.64938\n [121] 75.93994 74.58181 76.25739 75.00178 74.22845 76.31685 74.41358 74.44657\n [129] 74.83803 74.55838 74.66919 75.35266 75.08914 74.66152 74.05380 74.39329\n [137] 76.60488 75.98699 74.62135 76.51778 73.53161 74.85214 75.05827 74.05406\n [145] 74.46643 75.30342 75.05014 74.63223 75.44543 72.75165 73.48342 73.80455\n [153] 73.50949 74.64755 75.00175 74.57649 74.11708 73.87874 75.27300 75.28975\n [161] 74.98249 75.48612 76.41550 75.16506 74.98727 74.96898 74.74179 74.24449\n [169] 75.27430 73.67490 75.65908 75.04681 76.79113 75.26938 74.79354 74.01127\n [177] 74.76849 75.36104 73.74682 74.30743 75.67318 75.99384 75.66748 74.10130\n [185] 76.20572 75.83375 74.37115 73.50744 73.73622 74.50636 73.27653 76.26933\n [193] 75.71124 75.86047 75.73344 73.18854 74.28251 74.65673 74.65480 75.40234\n [201] 73.87881 76.52272 75.22239 75.24770 75.48636 76.14890 76.46642 74.57311\n [209] 74.99249 76.16651 75.82809 75.16704 73.94868 76.90079 74.84575 74.34058\n [217] 75.61323 74.84086 75.29631 74.81182 75.49209 76.00486 76.05246 75.87944\n [225] 74.79180 77.26703 73.58652 74.54782 76.88003 76.32634 74.80925 74.98680\n [233] 73.75446 76.27250 75.27961 74.37445 74.76251 73.88835 74.85354 75.71155\n [241] 73.60666 75.28570 76.18128 73.27245 74.02696 74.37579 74.73682 74.73227\n [249] 73.37703 74.59072 74.23939 74.41314 74.90766 74.66029 75.12877 75.62673\n [257] 76.32683 75.72618 74.85168 76.70432 76.28197 74.71700 75.30343 75.91907\n [265] 75.24439 75.65435 75.19472 73.92471 73.60380 75.20492 75.66565 75.39755\n [273] 75.25493 75.77523 75.66162 73.38134 74.86907 75.68101 73.85461 74.98361\n [281] 75.46433 75.10175 76.63125 75.85701 75.17118 74.53800 75.81667 75.88907\n [289] 74.43176 74.44114 74.48623 74.91623 74.28739 75.51695 75.94118 76.48571\n [297] 76.14303 74.66342 74.97728 74.53391 73.52488 73.58812 76.11815 75.46769\n [305] 75.19182 74.13072 76.31003 75.30696 75.02037 75.74875 75.73139 75.77816\n [313] 74.34561 73.84045 75.24158 75.76953 73.13234 74.61631 74.81177 75.26779\n [321] 74.23635 73.82120 76.31054 74.35616 76.07601 73.95509 74.05772 74.22910\n [329] 75.43011 73.42268 76.79148 75.21833 74.80185 73.36899 73.14975 75.79115\n [337] 74.61919 75.98506 73.42636 75.22286 73.99257 75.46905 73.89113 75.37155\n [345] 74.05730 74.66263 74.66438 74.18706 74.49813 74.32892 74.23642 74.67756\n [353] 75.04411 74.30414 74.70724 75.08641 75.71841 75.51808 74.93100 75.81793\n [361] 74.78730 74.63517 74.97826 75.51747 74.52114 75.12071 78.13482 75.46619\n [369] 74.46580 74.73910 75.67454 75.50536 75.52796 74.30964 76.21489 74.70730\n [377] 76.88057 74.58457 74.43064 75.03877 74.36608 75.06382 75.98336 74.87293\n [385] 74.22894 75.06329 74.27155 75.68821 74.24253 73.94639 73.49113 75.11725\n [393] 75.85721 76.07618 75.18791 76.24006 75.62590 75.02484 74.42331 72.60027\n [401] 73.52044 76.51052 74.28500 75.40433 73.90875 76.99178 76.35279 75.41119\n [409] 74.13315 75.19382 75.50201 74.48111 75.90511 75.11015 75.25285 74.99648\n [417] 74.47208 75.85869 74.68220 73.34307 75.04739 76.44738 75.31309 75.52913\n [425] 76.26223 75.73930 76.33215 73.74962 73.33994 74.70288 74.19174 75.01108\n [433] 74.63574 74.76168 74.78990 74.75733 73.00951 74.26312 76.02842 74.63533\n [441] 75.14596 74.96193 74.11402 74.59574 76.09305 74.53926 74.74604 74.40459\n [449] 77.27062 74.78709 74.36770 72.54340 75.09833 74.88731 73.50395 74.78807\n [457] 75.05173 75.12390 74.90934 75.99915 74.33275 74.38525 74.10922 73.42221\n [465] 76.64918 74.47975 75.12581 75.25714 75.35792 74.69494 75.62008 73.53747\n [473] 75.71740 74.33907 75.53669 73.35504 74.45043 76.12163 76.03898 75.61239\n [481] 74.80586 73.71626 73.48871 74.12853 73.84396 77.17008 76.43512 73.98444\n [489] 76.04977 74.46583 76.01115 73.91292 75.92073 74.29250 74.63153 75.00484\n [497] 74.52828 74.66825 75.29207 74.77686 76.19662 75.30999 75.69891 73.15895\n [505] 73.85873 74.07277 75.41135 74.74135 74.26356 75.63118 75.28181 74.17680\n [513] 74.68805 76.18352 74.81882 75.24053 75.04616 76.11425 75.36269 75.19598\n [521] 74.37596 76.61887 73.65574 75.51871 74.48345 76.03400 75.57605 75.55502\n [529] 75.65533 76.53720 75.03611 73.88219 74.41748 74.08832 76.36027 74.93291\n [537] 75.66116 75.06712 74.36560 75.25447 75.49461 76.83779 75.88841 75.24546\n [545] 73.47890 73.74035 74.00327 76.70369 74.94458 76.21075 74.29925 75.06979\n [553] 76.01745 75.82861 75.54548 75.42884 73.98611 75.54789 76.09116 75.88274\n [561] 76.46166 74.96008 75.43254 74.86664 74.29002 74.87314 74.85198 76.87914\n [569] 74.06174 75.85227 75.67760 74.07233 75.48141 74.66367 74.93047 74.96150\n [577] 74.49903 73.48692 73.68900 75.15058 75.11194 74.39278 74.14565 75.62441\n [585] 74.94064 75.99647 74.85537 76.45970 75.82955 75.49968 75.89981 74.64502\n [593] 74.74139 75.52825 75.47099 74.43561 75.00259 76.13283 75.72467 74.57316\n [601] 75.02189 73.51909 75.54535 74.09301 73.49334 75.41178 74.95289 74.11120\n [609] 74.07679 74.21184 74.83747 74.92889 74.04358 74.96736 74.76160 73.12468\n [617] 75.29538 75.22185 75.34399 75.06791 76.37536 74.09531 73.46627 75.90204\n [625] 76.25114 75.31286 77.02115 74.14008 75.03220 75.24348 73.99906 74.97442\n [633] 75.66312 73.25093 75.45421 74.73571 73.21914 74.86915 75.43225 74.53833\n [641] 75.61747 74.80382 76.44066 74.81603 76.04032 75.44944 74.26456 75.84708\n [649] 74.68048 75.09555 73.92737 75.45199 74.56045 75.22312 75.03210 75.12464\n [657] 75.52245 76.29832 74.50342 74.45557 74.70536 75.87068 74.94073 75.33082\n [665] 74.72678 75.03815 75.78069 74.17345 75.36708 75.10770 74.96127 77.29781\n [673] 76.23836 73.81312 74.40241 75.18181 74.61472 74.84399 74.73681 74.81869\n [681] 75.35093 74.39910 73.61451 74.36612 73.33844 75.57199 73.31930 74.69243\n [689] 75.09463 75.37939 76.62129 75.74483 75.38812 73.98151 75.04188 75.28739\n [697] 75.18751 76.01130 74.61666 74.21020 75.79813 76.06483 75.31910 74.31577\n [705] 74.67929 76.07411 73.80797 74.82510 73.71311 75.91244 75.04805 76.57705\n [713] 75.72923 73.86751 75.28411 74.13344 75.53956 73.85239 73.03549 73.49739\n [721] 76.82092 74.90142 74.57528 75.56377 75.34453 75.50219 76.00495 74.77230\n [729] 75.33662 74.45439 74.17300 74.23845 73.32387 73.52121 76.13298 73.78813\n [737] 75.41284 76.59846 75.35748 74.94102 74.53078 75.59105 74.86910 74.57358\n [745] 75.53979 75.67722 74.04404 75.08682 74.43477 73.90495 58.71655 58.60596\n [753] 59.81279 58.92998 58.74580 61.62783 58.89574 57.72173 58.00128 58.23492\n [761] 57.46868 58.14439 58.33435 59.21383 58.26875 59.21490 59.93676 57.84037\n [769] 58.93440 57.99766 58.40757 58.86463 58.19380 58.86269 58.96014 59.35988\n [777] 58.73265 58.99126 58.88987 58.61232 59.43563 57.55058 58.64003 59.29693\n [785] 59.85673 58.43464 58.53746 59.59014 57.99131 60.11249 59.61312 58.65781\n [793] 58.48004 57.33794 59.90118 55.96181 58.86370 59.51051 59.25745 59.36329\n [801] 60.21101 58.21819 60.41045 59.13623 57.49084 61.21628 58.54168 60.98868\n [809] 59.25941 59.30273 58.45916 58.29524 59.36254 58.77167 59.30608 58.18048\n [817] 58.51745 60.10645 57.31615 59.57920 59.00116 59.34116 58.70359 58.77237\n [825] 58.91715 58.88182 59.58683 59.02189 60.04784 59.61932 59.35197 60.69102\n [833] 59.12942 59.34041 59.78739 58.66497 60.09224 58.85611 59.29852 60.08068\n [841] 59.05809 58.44424 59.68582 59.47273 59.26000 59.36474 60.53905 59.03931\n [849] 58.38699 59.14683 60.18130 57.86961 58.60957 58.99026 59.13557 58.51197\n [857] 59.14811 59.47670 57.76631 58.73476 59.12640 60.01568 58.48686 57.62616\n [865] 59.13953 58.66865 58.50021 59.29785 57.51375 61.24982 59.90547 58.47528\n [873] 57.63368 58.72005 58.95607 58.73648 60.54165 57.87001 59.11123 59.45932\n [881] 61.56960 58.72222 58.80271 58.80800 58.79589 59.49097 59.32121 58.48496\n [889] 58.10383 58.12429 59.86692 59.20175 59.21730 58.58847 60.27672 58.10896\n [897] 58.29106 59.94312 59.80183 58.42179 59.85023 58.97875 59.70343 59.56826\n [905] 58.55075 58.60945 59.08787 58.03444 57.76502 59.16565 57.11930 58.16243\n [913] 60.70687 58.89522 58.23583 60.20542 58.14132 58.27318 59.95025 57.69363\n [921] 58.26243 59.61055 58.53237 58.24523 59.25394 59.63806 57.57931 58.95890\n [929] 58.91506 60.05179 60.07972 58.30900 58.69114 60.87270 59.38890 60.01367\n [937] 56.88199 57.53978 58.52811 59.30322 60.71852 60.31373 57.77659 59.33987\n [945] 58.45096 58.57272 59.70338 60.33623 58.94100 59.03894 59.13207 59.44900\n [953] 59.19860 58.80944 58.32800 58.39747 60.27306 59.47031 58.28749 57.83506\n [961] 60.66958 58.08557 58.57839 58.88083 58.54565 58.98934 59.74796 59.64448\n [969] 57.56594 59.74780 57.28264 58.39450 60.28522 59.68049 58.99770 57.25823\n [977] 58.25759 59.30419 59.18225 57.00076 59.96050 57.83463 59.84659 58.17613\n [985] 59.47178 59.16409 59.76432 58.55604 58.55329 58.18867 58.57248 60.31412\n [993] 59.65095 58.74366 60.13083 60.32541 58.37594 60.20369 60.00230 59.09448\n\n# Mutate is basically doing the same thing\ntank_data &lt;- tank_data %&gt;%\n  mutate(avg_daily_temp_F = c_to_f(avg_daily_temp))\n\n# View mutate() results\ntank_data\n\n# A tibble: 1,000 × 8\n   tank_id species avg_daily_temp num_fish day_length tank_volume size_day_30\n     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1       1 tilapia           23.6      105         10        401.       2780.\n 2       2 tilapia           23.8      102         10        399.       2786.\n 3       3 tilapia           23.1      109         10        399.       2781.\n 4       4 tilapia           23.7       98         12        401.       2785.\n 5       5 tilapia           24.4      103         10        400.       2786.\n 6       6 tilapia           23.3       97         10        400.       2783.\n 7       7 tilapia           23.5      101         11        400.       2784.\n 8       8 tilapia           24.9       99         10        401.       2789.\n 9       9 tilapia           23.7      102          9        400.       2788.\n10      10 tilapia           23.8       95         10        399.       2785.\n# ℹ 990 more rows\n# ℹ 1 more variable: avg_daily_temp_F &lt;dbl&gt;\n\n\n\nWhy Functions?\nWe can easily fit the equation for converting from Celsius to Fahrenheit into a mutate call. We certainly didn’t need to write our own function to do it. So why did we learn how to do that?\nMain Ways to Iterate\nThere are many different approaches to iterating in R, especially because we are typically working with vectorized data (columns in data frames). We will talk about the other options down the road. Today, we used #3, the mutate() function from the tidyverse.\n\nfor loops\napply/map functions\nusing dplyr (tidyverse) functions\n\nFiltering Our Data\nThankfully, we’ve solved our first problem! However, we haven’t yet answered the original question.\nAre any tanks are below the temperature cutoffs for each species? If so, how many?\nLet’s tackle that question:\n\n\n# count() is a new function for most of us\n# It does what it sounds like -- count the number of results\n\n# Count number of tanks below temperature cutoff for TILAPIA\ntank_data %&gt;% \n  filter(species == \"tilapia\" & avg_daily_temp_F &lt; 75) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   383\n\n# for TROUT\ntank_data %&gt;% \n  filter(species == \"trout\" & avg_daily_temp_F &lt; 59) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   131\n\n# Let's introduce some additional syntax: \"&\" and \"|\"\n\n# Now we can get counts for both fish species in one go!\ntank_data %&gt;%\n  filter(species == \"tilapia\" & avg_daily_temp_F &lt; 75 |\n         species == \"trout\" & avg_daily_temp_F &lt; 59) %&gt;%\n  group_by(species) %&gt;%\n  summarize(n = n()) # Another way to get the count is summarize() with n()\n\n# A tibble: 2 × 2\n  species     n\n  &lt;chr&gt;   &lt;int&gt;\n1 tilapia   383\n2 trout     131"
  },
  {
    "objectID": "modules/module_1/module1_3.html",
    "href": "modules/module_1/module1_3.html",
    "title": "1.3: 2-Dimensional Data and the tidyverse",
    "section": "",
    "text": "Students will be able to load packages.\nStudents will be able to use some functions of the tidyverse: select, filter, the pipe, mutate, summarize, and group by.\nStudents will compare and contrast base R and tidyverse methodology for sub-setting data frames.\nStudents will able to use tidyverse functions to summarize real-world data."
  },
  {
    "objectID": "modules/module_1/module1_3.html#learning-outcomes",
    "href": "modules/module_1/module1_3.html#learning-outcomes",
    "title": "1.3: 2-Dimensional Data and the tidyverse",
    "section": "",
    "text": "Students will be able to load packages.\nStudents will be able to use some functions of the tidyverse: select, filter, the pipe, mutate, summarize, and group by.\nStudents will compare and contrast base R and tidyverse methodology for sub-setting data frames.\nStudents will able to use tidyverse functions to summarize real-world data."
  },
  {
    "objectID": "modules/module_1/module1_3.html#lecture-code-along",
    "href": "modules/module_1/module1_3.html#lecture-code-along",
    "title": "1.3: 2-Dimensional Data and the tidyverse",
    "section": "⊳ Lecture / Code-along",
    "text": "⊳ Lecture / Code-along\n\nThe tidyverse: What is it?\nDifferent programming languages have different syntax (language structure). The tidyverse is a package (more accurately, a set of packages) offered in R that all have similar goals and a unified syntax designed to work particularly well with 2-dimensional data.\nUntil now, all of the coding we have done is in the original R language, which is often called “base R.” The syntax in the tidyverse is often pretty different from base R. Both are useful, and many people often combine them, which is why we start with base R.\nThat said, we will be primarily using the tidyverse for the rest of the semester. With the exception of Module 1 Assignment 3, which is exclusively about tidyverse, I will never punish you for using base R in place of tidyverse, as long as you get the same answer!\nWait, what is a package??\nPackages are one of the neatest things of working in an open-source environment like R! They contains bits of code (often in the form of functions) that can be reused, making them a core component of reproducible data science. Anyone can develop a package, and there are thousands of them doing all sorts of things.\nExplore the tidyverse\nIf you want to learn more about the tidyverse, head over to www.tidyverse.org and browse the site. Below is a brief summary of some of the packages I think you might find the most useful.\n\ntidyr: creating data that is consistent in form/shape\ndplyr: creating data that is clean, easily wrangled, and summarized\nggplot2: publication-worthy plots using The Grammar of Graphics\ntibble: data frames but better!\nreadr: fast and friendly ways to read data into R\nstringr: easy manipulation of strings (character data)\nlubridate: easy manipulation of time and date values\n\nPractice with the tidyverse\nDownload and install\nIn most scenarios, you will need to download a package from the internet onto your computer before you can use it in RStudio. However, with RStudio Cloud, I’ve already done this step for you!\nFor future reference, though:\n\nyou usually only need to go through this process once until you update R\nwe use the function install.packages() to download the package\n\n\n\n# Download and install the tidyverse package(s)\n\n# To run the line of code below, remove the # in front of the line below and run this chunk:\n\n# install.packages(\"tidyverse\")\n\n\nLoad into R\nAny time we open R/RStudio and want to use functions from the tidyverse, we need to “load” the package. We use the library() function to do this.\nWhen you run this code, you’ll see a message that says “Attaching packages” and “Conflicts.” Don’t panic!\n\nThe first bit tells us that the core packages have been brought into our R session.\nThe “conflict” part is a little more complicated but we don’t need to worry too much about it.\n\nIf you’re curious, though, it is telling us that there are some functions in the tidyverse that have the same names as functions that are automatically installed with R and that the tidyverse versions of those functions will be the ones that get used by default unless we specify otherwise.\n\n\n\n\n# Load the tidyverse (tell RStudio we want to use this package in this session)\nlibrary(tidyverse)\n\n\nClimate Data\nTo learn about the tidyverse syntax, we’re going to use a real data set on climate change from Berkeley, CA, USA. It outlines temperatures in major cities across the world since 1750.\n\n\n# Read in the data file\n\n# `read_csv() is part of the `tidyverse`\n# It gives us nice options when reading in data\nclimate_df &lt;- read_csv(\"data/global_temps.csv\")\n\n# Let's take a look at the climate data\nclimate_df\n\n# A tibble: 239,177 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1849-01-01               26.7                     1.44 Abid… Côte D… 5.63N   \n 2 1849-02-01               27.4                     1.36 Abid… Côte D… 5.63N   \n 3 1849-03-01               28.1                     1.61 Abid… Côte D… 5.63N   \n 4 1849-04-01               26.1                     1.39 Abid… Côte D… 5.63N   \n 5 1849-05-01               25.4                     1.2  Abid… Côte D… 5.63N   \n 6 1849-06-01               24.8                     1.40 Abid… Côte D… 5.63N   \n 7 1849-07-01               24.1                     1.25 Abid… Côte D… 5.63N   \n 8 1849-08-01               23.6                     1.26 Abid… Côte D… 5.63N   \n 9 1849-09-01               23.7                     1.23 Abid… Côte D… 5.63N   \n10 1849-10-01               25.3                     1.18 Abid… Côte D… 5.63N   \n# ℹ 239,167 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n\n\nThe tidyverse converts 2D data into something called a tibble! For our intents and purposes, it is basically the same as a data frame (and I’ll probably call it a data frame, in reality).\nLet’s take a look at our tibble (A.K.A. data frame).\n\n\n# Explore the data set\n\n# First 6 rows\nhead(climate_df)\n\n# A tibble: 6 × 7\n  dt         AverageTemperature AverageTemperatureUncer…¹ City  Country Latitude\n  &lt;date&gt;                  &lt;dbl&gt;                     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n1 1849-01-01               26.7                      1.44 Abid… Côte D… 5.63N   \n2 1849-02-01               27.4                      1.36 Abid… Côte D… 5.63N   \n3 1849-03-01               28.1                      1.61 Abid… Côte D… 5.63N   \n4 1849-04-01               26.1                      1.39 Abid… Côte D… 5.63N   \n5 1849-05-01               25.4                      1.2  Abid… Côte D… 5.63N   \n6 1849-06-01               24.8                      1.40 Abid… Côte D… 5.63N   \n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n# Last 6 rows\nstr(climate_df)\n\nspc_tbl_ [239,177 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ dt                           : Date[1:239177], format: \"1849-01-01\" \"1849-02-01\" ...\n $ AverageTemperature           : num [1:239177] 26.7 27.4 28.1 26.1 25.4 ...\n $ AverageTemperatureUncertainty: num [1:239177] 1.44 1.36 1.61 1.39 1.2 ...\n $ City                         : chr [1:239177] \"Abidjan\" \"Abidjan\" \"Abidjan\" \"Abidjan\" ...\n $ Country                      : chr [1:239177] \"Côte D'Ivoire\" \"Côte D'Ivoire\" \"Côte D'Ivoire\" \"Côte D'Ivoire\" ...\n $ Latitude                     : chr [1:239177] \"5.63N\" \"5.63N\" \"5.63N\" \"5.63N\" ...\n $ Longitude                    : chr [1:239177] \"3.23W\" \"3.23W\" \"3.23W\" \"3.23W\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   dt = col_date(format = \"\"),\n  ..   AverageTemperature = col_double(),\n  ..   AverageTemperatureUncertainty = col_double(),\n  ..   City = col_character(),\n  ..   Country = col_character(),\n  ..   Latitude = col_character(),\n  ..   Longitude = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nselect()ing columns\nLet’s use our first function, select(). Select allows us to pick out specific columns from our data. You can use names or their position in the data frame.\nFirst, let’s remind ourselves how we would accomplish this in base R.\n\n\n# Column selection in base R\n# Option 1: climate_df$dt\n\n# Option 2:\n# Select multiple columns, in this case the first two\nclimate_df[, 1:2]\n\n# A tibble: 239,177 × 2\n   dt         AverageTemperature\n   &lt;date&gt;                  &lt;dbl&gt;\n 1 1849-01-01               26.7\n 2 1849-02-01               27.4\n 3 1849-03-01               28.1\n 4 1849-04-01               26.1\n 5 1849-05-01               25.4\n 6 1849-06-01               24.8\n 7 1849-07-01               24.1\n 8 1849-08-01               23.6\n 9 1849-09-01               23.7\n10 1849-10-01               25.3\n# ℹ 239,167 more rows\n\n\n\nThe select() function does the same thing but with more power (and, in my opinion, more easily). The first argument in the function is the data frame. Any following arguments are the columns we want to select.\n\n\n# First argument is the data frame, then the columns\nselect(climate_df, dt)\n\n# A tibble: 239,177 × 1\n   dt        \n   &lt;date&gt;    \n 1 1849-01-01\n 2 1849-02-01\n 3 1849-03-01\n 4 1849-04-01\n 5 1849-05-01\n 6 1849-06-01\n 7 1849-07-01\n 8 1849-08-01\n 9 1849-09-01\n10 1849-10-01\n# ℹ 239,167 more rows\n\n# Multiple columns:\n\n# City and Country\nselect(climate_df, dt, City, Country)\n\n# A tibble: 239,177 × 3\n   dt         City    Country      \n   &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;        \n 1 1849-01-01 Abidjan Côte D'Ivoire\n 2 1849-02-01 Abidjan Côte D'Ivoire\n 3 1849-03-01 Abidjan Côte D'Ivoire\n 4 1849-04-01 Abidjan Côte D'Ivoire\n 5 1849-05-01 Abidjan Côte D'Ivoire\n 6 1849-06-01 Abidjan Côte D'Ivoire\n 7 1849-07-01 Abidjan Côte D'Ivoire\n 8 1849-08-01 Abidjan Côte D'Ivoire\n 9 1849-09-01 Abidjan Côte D'Ivoire\n10 1849-10-01 Abidjan Côte D'Ivoire\n# ℹ 239,167 more rows\n\n# All columns until Country\nselect(climate_df, dt:Country)\n\n# A tibble: 239,177 × 5\n   dt         AverageTemperature AverageTemperatureUncertainty City    Country  \n   &lt;date&gt;                  &lt;dbl&gt;                         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    \n 1 1849-01-01               26.7                          1.44 Abidjan Côte D'I…\n 2 1849-02-01               27.4                          1.36 Abidjan Côte D'I…\n 3 1849-03-01               28.1                          1.61 Abidjan Côte D'I…\n 4 1849-04-01               26.1                          1.39 Abidjan Côte D'I…\n 5 1849-05-01               25.4                          1.2  Abidjan Côte D'I…\n 6 1849-06-01               24.8                          1.40 Abidjan Côte D'I…\n 7 1849-07-01               24.1                          1.25 Abidjan Côte D'I…\n 8 1849-08-01               23.6                          1.26 Abidjan Côte D'I…\n 9 1849-09-01               23.7                          1.23 Abidjan Côte D'I…\n10 1849-10-01               25.3                          1.18 Abidjan Côte D'I…\n# ℹ 239,167 more rows\n\n# All columns except City\nselect(climate_df, -City)\n\n# A tibble: 239,177 × 6\n   dt         AverageTemperature AverageTemperatureUncertainty Country  Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n 1 1849-01-01               26.7                          1.44 Côte D'… 5.63N   \n 2 1849-02-01               27.4                          1.36 Côte D'… 5.63N   \n 3 1849-03-01               28.1                          1.61 Côte D'… 5.63N   \n 4 1849-04-01               26.1                          1.39 Côte D'… 5.63N   \n 5 1849-05-01               25.4                          1.2  Côte D'… 5.63N   \n 6 1849-06-01               24.8                          1.40 Côte D'… 5.63N   \n 7 1849-07-01               24.1                          1.25 Côte D'… 5.63N   \n 8 1849-08-01               23.6                          1.26 Côte D'… 5.63N   \n 9 1849-09-01               23.7                          1.23 Côte D'… 5.63N   \n10 1849-10-01               25.3                          1.18 Côte D'… 5.63N   \n# ℹ 239,167 more rows\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n\n\nYou might have noticed that we haven’t put any column names in quotations, unlike what we did with selecting columns by name in base R. This is one quirk of the tidyverse to which you will need to pay special attention. We usually will not need to put column names in quotations.\nLet’s practice!\nWrite a line of code to select the following data from the climate_df: average temperature, latitude and longitude\n\n\n# Select three select columns\nselect(climate_df, AverageTemperature, Latitude, Longitude)\n\n# A tibble: 239,177 × 3\n   AverageTemperature Latitude Longitude\n                &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n 1               26.7 5.63N    3.23W    \n 2               27.4 5.63N    3.23W    \n 3               28.1 5.63N    3.23W    \n 4               26.1 5.63N    3.23W    \n 5               25.4 5.63N    3.23W    \n 6               24.8 5.63N    3.23W    \n 7               24.1 5.63N    3.23W    \n 8               23.6 5.63N    3.23W    \n 9               23.7 5.63N    3.23W    \n10               25.3 5.63N    3.23W    \n# ℹ 239,167 more rows\n\n\n\nIt is important to remember that the computer interprets everything literally. We need to tell the function the exact names of the columns. R will interpret latitude and Latitude as different things; it doesn’t know that they are probably the same!\nfilter()ing rows\nfilter() allows you filter rows by certain conditions. Recall that we did this a bit with base R.\n\n\n# Base R\n# Select rows where the average temperature was greater than 25\nclimate_df[climate_df$AverageTemperature &gt; 25, ]\n\n# A tibble: 79,690 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1849-01-01               26.7                     1.44 Abid… Côte D… 5.63N   \n 2 1849-02-01               27.4                     1.36 Abid… Côte D… 5.63N   \n 3 1849-03-01               28.1                     1.61 Abid… Côte D… 5.63N   \n 4 1849-04-01               26.1                     1.39 Abid… Côte D… 5.63N   \n 5 1849-05-01               25.4                     1.2  Abid… Côte D… 5.63N   \n 6 1849-10-01               25.3                     1.18 Abid… Côte D… 5.63N   \n 7 1849-11-01               26.3                     1.51 Abid… Côte D… 5.63N   \n 8 1849-12-01               25.4                     1.84 Abid… Côte D… 5.63N   \n 9 1850-01-01               25.8                     1.94 Abid… Côte D… 5.63N   \n10 1850-02-01               27.9                     1.43 Abid… Côte D… 5.63N   \n# ℹ 79,680 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n\n\nThe code above is, in my opinion, a bit unwieldy. Filter feels more intuitive. We still need the double equal signs, though!\n\n\n# Filter same content as above, but with `tidyverse`\nfilter(climate_df, AverageTemperature &gt; 25)\n\n# A tibble: 68,688 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1849-01-01               26.7                     1.44 Abid… Côte D… 5.63N   \n 2 1849-02-01               27.4                     1.36 Abid… Côte D… 5.63N   \n 3 1849-03-01               28.1                     1.61 Abid… Côte D… 5.63N   \n 4 1849-04-01               26.1                     1.39 Abid… Côte D… 5.63N   \n 5 1849-05-01               25.4                     1.2  Abid… Côte D… 5.63N   \n 6 1849-10-01               25.3                     1.18 Abid… Côte D… 5.63N   \n 7 1849-11-01               26.3                     1.51 Abid… Côte D… 5.63N   \n 8 1849-12-01               25.4                     1.84 Abid… Côte D… 5.63N   \n 9 1850-01-01               25.8                     1.94 Abid… Côte D… 5.63N   \n10 1850-02-01               27.9                     1.43 Abid… Côte D… 5.63N   \n# ℹ 68,678 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n# It is easy to write multiple conditions and to chain stuff together:\n\n# Only rows that meet both conditions; you could also use \"&\" instead of \",\"\nfilter(climate_df, AverageTemperature &gt; 25, Country == \"United States\")   \n\n# A tibble: 61 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1761-07-01               27.8                    2.39  Chic… United… 42.59N  \n 2 1868-07-01               25.1                    0.699 Chic… United… 42.59N  \n 3 1900-08-01               25.2                    0.49  Chic… United… 42.59N  \n 4 1921-07-01               25.6                    0.264 Chic… United… 42.59N  \n 5 1947-08-01               26.4                    0.199 Chic… United… 42.59N  \n 6 1955-08-01               25.3                    0.153 Chic… United… 42.59N  \n 7 1959-08-01               25.1                    0.205 Chic… United… 42.59N  \n 8 1988-08-01               25.4                    0.305 Chic… United… 42.59N  \n 9 1995-08-01               25.9                    0.283 Chic… United… 42.59N  \n10 2012-07-01               25.9                    0.516 Chic… United… 42.59N  \n# ℹ 51 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n# Rows that meet one or the other condition; the \"|\" symbol means \"or\"\nfilter(climate_df, AverageTemperature &gt; 25 | Country == \"United States\" ) \n\n# A tibble: 77,082 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1849-01-01               26.7                     1.44 Abid… Côte D… 5.63N   \n 2 1849-02-01               27.4                     1.36 Abid… Côte D… 5.63N   \n 3 1849-03-01               28.1                     1.61 Abid… Côte D… 5.63N   \n 4 1849-04-01               26.1                     1.39 Abid… Côte D… 5.63N   \n 5 1849-05-01               25.4                     1.2  Abid… Côte D… 5.63N   \n 6 1849-10-01               25.3                     1.18 Abid… Côte D… 5.63N   \n 7 1849-11-01               26.3                     1.51 Abid… Côte D… 5.63N   \n 8 1849-12-01               25.4                     1.84 Abid… Côte D… 5.63N   \n 9 1850-01-01               25.8                     1.94 Abid… Côte D… 5.63N   \n10 1850-02-01               27.9                     1.43 Abid… Côte D… 5.63N   \n# ℹ 77,072 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n# Pulls rows in which the Country column has either \"United States\" OR \"Mexico\"\nfilter(climate_df, Country == \"United States\" | Country == \"Mexico\")      \n\n# A tibble: 10,600 × 7\n   dt         AverageTemperature AverageTemperatureUnce…¹ City  Country Latitude\n   &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   \n 1 1743-11-01               5.44                     2.20 Chic… United… 42.59N  \n 2 1743-12-01              NA                       NA    Chic… United… 42.59N  \n 3 1744-01-01              NA                       NA    Chic… United… 42.59N  \n 4 1744-02-01              NA                       NA    Chic… United… 42.59N  \n 5 1744-03-01              NA                       NA    Chic… United… 42.59N  \n 6 1744-04-01               8.77                     2.36 Chic… United… 42.59N  \n 7 1744-05-01              11.6                      2.10 Chic… United… 42.59N  \n 8 1744-06-01              18.0                      1.99 Chic… United… 42.59N  \n 9 1744-07-01              21.7                      1.79 Chic… United… 42.59N  \n10 1744-08-01              NA                       NA    Chic… United… 42.59N  \n# ℹ 10,590 more rows\n# ℹ abbreviated name: ¹​AverageTemperatureUncertainty\n# ℹ 1 more variable: Longitude &lt;chr&gt;\n\n# Worth noting here that we haven't saved any of this\n# To do that, we need to write to a new object\nus_df &lt;- filter(climate_df, AverageTemperature &gt; 25)\n\n\nLet’s practice using select() and filter()\nWork with the climate data we’ve been using this class period. Construct a small set of code that does the following:\n\nSlims down the full data frame to one that contains the columns dt, AverageTemperature and City. Assign this to an object called slim.\nFilters the data for Paris with an average temperature less than 22.\nName this new data frame “cold_paris”\n\n\n\n# Not piped\n\n# (1)\nslim &lt;- select(climate_df, dt, AverageTemperature, City)\n\n# (2)\nfiltered &lt;- filter(slim, City == \"Paris\", AverageTemperature &lt; 22)\n\n# (3)\ncold_paris &lt;- filtered\n\n\nThe pipe %&gt;%\nYou can use the pipe operator to chain tidyverse functions together. You can think of the pipe as automatically sending the output from the first line into the next line as the input.\nThis is helpful for a lot of reasons, including:\n\nremoving the clutter of creating a lot of intermediate objects in your work space, which reduces the chance of errors caused by using the wrong input object\nmakes things more human-readable (in addition to computer-readable)\n\nThe shortcut for typing a pipe is Ctrl + Shift + M (or Cmd + Shift + M on a Mac)\n\n\n# Open climate_df %&gt;% \n# Select City column\n# (Returns City column)\nclimate_df %&gt;% \n  select(dt, City)\n\n# A tibble: 239,177 × 2\n   dt         City   \n   &lt;date&gt;     &lt;chr&gt;  \n 1 1849-01-01 Abidjan\n 2 1849-02-01 Abidjan\n 3 1849-03-01 Abidjan\n 4 1849-04-01 Abidjan\n 5 1849-05-01 Abidjan\n 6 1849-06-01 Abidjan\n 7 1849-07-01 Abidjan\n 8 1849-08-01 Abidjan\n 9 1849-09-01 Abidjan\n10 1849-10-01 Abidjan\n# ℹ 239,167 more rows\n\n# (1) Will store results in cold_paris\n# (2) Open climate_df\ncold_paris &lt;- climate_df %&gt;%\n  # (3) Select two columns\n  select(dt, AverageTemperature, City) %&gt;%   \n  # (4) Filter to Paris, then filter data where where avg. temp &lt; 22\n  filter(City == \"Paris\", AverageTemperature &lt; 22)  \n\n\nLet’s practice!\nIn small groups, use pipes to create a new data frame called warm_nigeria that includes the following:\n\nthe columns AverageTemperature, City, Country\nonly rows for the country Nigeria and temperatures that are greater than 30 degrees\n\n\n\nwarm_nigeria &lt;- climate_df %&gt;% \n  # Select columns\n  select(AverageTemperature, City, Country) %&gt;% \n  # Filter\n  filter(Country == \"Nigeria\", AverageTemperature &gt; 30)\n\n\nCreating new variables with mutate()\nSometimes our data doesn’t have our data in exactly the format we want. For example, we might want our temperature data in Fahrenheit instead of Celsius.\nThe tidyverse has a function called mutate() that lets us create a new column. Often, we want to apply a function to the entire column or perform some type of calculation, such as converting temp from F to C.\nTo help us out, here is the equation for converting: Fahrenheit = Celsius * (9/5) + 32\n\n\nclimate_df %&gt;% \n  # Select temperature column\n  select(dt, AverageTemperature) %&gt;% \n  # Create a new column for converted temperatures in Fahrenheit\n  mutate(AverageTemperature_F = AverageTemperature * (9/5) + 32)\n\n# A tibble: 239,177 × 3\n   dt         AverageTemperature AverageTemperature_F\n   &lt;date&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n 1 1849-01-01               26.7                 80.1\n 2 1849-02-01               27.4                 81.4\n 3 1849-03-01               28.1                 82.6\n 4 1849-04-01               26.1                 79.1\n 5 1849-05-01               25.4                 77.8\n 6 1849-06-01               24.8                 76.7\n 7 1849-07-01               24.1                 75.3\n 8 1849-08-01               23.6                 74.4\n 9 1849-09-01               23.7                 74.6\n10 1849-10-01               25.3                 77.5\n# ℹ 239,167 more rows\n\n\n\nThe first part of the argument in the mutate function (before the =) is the name of the new column we want to create (or, sometime, the name of a column we want to overwrite). After the = is what we want the new column to contain.\nUnderstanding data through summarize()\nLike we have talked about in previous classes, some of the best ways for us to understand our data is through what we call summary statistics such as the mean, standard deviation, minimums, maximums, etc.\nFortunately, the tidyverse has a handy-dandy function to make this easy to do with data frames.\nThe summarize() function creates a new dataframe with columns and values we give it. Similar to mutate(), what is on the left of the = is the name of the new column, and what is on the right of the = is the value(s) to put in the new column.\n\n\nclimate_df %&gt;% \n  # Make new data frame with one column containing: \n  summarise(mean_temp = mean(AverageTemperature),    # average temperature and\n            sd_temp = sd(AverageTemperature))        # the other the sd\n\n# A tibble: 1 × 2\n  mean_temp sd_temp\n      &lt;dbl&gt;   &lt;dbl&gt;\n1        NA      NA\n\n\n\nWait a second! Those are some weird values!\nNA is used to represent missing data. So what is happening here? We know that there are numbers to calculate these values.\nIt is important to note that if any of the values in the column that you are trying to summarize are missing (NA), you might get some wonky values, like you did above.\nFortunately, mean() and sd() and some other functions have an argument to remove the missing values: na.rm = TRUE\n\n\n# Same as above, but with NA values omitted from calculation\nclimate_df %&gt;% \n  summarise(mean_temp = mean(AverageTemperature, na.rm = TRUE),\n            sd_temp = sd(AverageTemperature, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_temp sd_temp\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      18.1    10.0\n\n\n\nPay attention to where the na.rm = TRUE argument is placed. We are putting it inside the parentheses for the mean() and sd() function, not as an argument in the summarize() function.\nSplit, Apply, Combine with group_by()\nOne common way we analyze data is through something we call the “split, apply, combine” approach. This means that we:\n\nsplit data up into groups via some type of categorization\napply some type of analysis to each group independently and\ncombine the data back together\n\nThe group_by() function lets us do this. It is most often used in combination with mutate() or summarize().\nFor example, we can use this method to calculate the mean temperatures of each country instead of the overall mean of the entire dataset. In order to do these, we create groups in the data based on the country.\n\n\n# Group data by country, then find mean and standard deviation for each\nclimate_df %&gt;% \n  group_by(Country) %&gt;% \n  summarise(mean_temp = mean(AverageTemperature, na.rm = TRUE), \n            sd_temp = sd(AverageTemperature, na.rm = TRUE))\n\n# A tibble: 49 × 3\n   Country     mean_temp sd_temp\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan     14.3     8.65\n 2 Angola          23.7     1.98\n 3 Australia       15.2     3.70\n 4 Bangladesh      25.5     3.88\n 5 Brazil          22.8     2.95\n 6 Burma           26.7     1.87\n 7 Canada           5.11   10.7 \n 8 Chile            5.69    4.75\n 9 China           11.8    11.4 \n10 Colombia        20.9     1.15\n# ℹ 39 more rows\n\n\n\nLet’s practice!\nPractice using the combination of group_by() and summarize() to calculate the minimum (min()) and maximum (max()) average temperatures for each city. Save this data frame as city_min_max\n\n\n# Group data by city, then find minimum and maximum for each\nclimate_df %&gt;% \n  group_by(City) %&gt;% \n  summarize(min_temp = min(AverageTemperature, na.rm = TRUE),\n            max_temp = max(AverageTemperature, na.rm = TRUE))\n\n# A tibble: 100 × 3\n   City           min_temp max_temp\n   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 Abidjan           22.4      29.9\n 2 Addis Abeba       14.5      21.2\n 3 Ahmadabad         16.8      35.4\n 4 Aleppo             0.67     32.6\n 5 Alexandria        10.2      28.8\n 6 Ankara            -6.28     26.0\n 7 Baghdad            4.24     38.3\n 8 Bangalore         20.3      29.7\n 9 Bangkok           21.9      31.1\n10 Belo Horizonte    15.9      25.2\n# ℹ 90 more rows\n\n\n\nAlready accomplished this task? Try to figure out how you can keep the “Country” column in the final data frame. This is trickier than you might think!\n\n\n# Group data by country, then group by city, then find minimum and maximum for each\nclimate_df %&gt;% \n  group_by(Country, City) %&gt;% \n  summarize(min_temp = min(AverageTemperature, na.rm = TRUE),\n            max_temp = max(AverageTemperature, na.rm = TRUE))\n\n# A tibble: 100 × 4\n# Groups:   Country [49]\n   Country     City           min_temp max_temp\n   &lt;chr&gt;       &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 Afghanistan Kabul             -2.08     27.6\n 2 Angola      Luanda            18.7      27.2\n 3 Australia   Melbourne          6.63     23.0\n 4 Australia   Sydney            12.0      22.0\n 5 Bangladesh  Dhaka             15.1      30.7\n 6 Brazil      Belo Horizonte    15.9      25.2\n 7 Brazil      Brasília          17.2      25.9\n 8 Brazil      Fortaleza         24.3      30.0\n 9 Brazil      Rio De Janeiro    18.5      28.8\n10 Brazil      Salvador          21.0      28.3\n# ℹ 90 more rows"
  },
  {
    "objectID": "modules/module_1/module1_1.html",
    "href": "modules/module_1/module1_1.html",
    "title": "1.1: Introduction to RStudio",
    "section": "",
    "text": "Students will be able to describe what computer programming languages (code) are, with R as an example.\nStudents will be able to describe the role of RStudio in programming with R.\nStudents will be able to describe the utility of each panel in RStudio.\nStudents will be able to perform basic math functions in the R console."
  },
  {
    "objectID": "modules/module_1/module1_1.html#learning-outcomes",
    "href": "modules/module_1/module1_1.html#learning-outcomes",
    "title": "1.1: Introduction to RStudio",
    "section": "",
    "text": "Students will be able to describe what computer programming languages (code) are, with R as an example.\nStudents will be able to describe the role of RStudio in programming with R.\nStudents will be able to describe the utility of each panel in RStudio.\nStudents will be able to perform basic math functions in the R console."
  },
  {
    "objectID": "modules/module_1/module1_1.html#lecture-code-along",
    "href": "modules/module_1/module1_1.html#lecture-code-along",
    "title": "1.1: Introduction to RStudio",
    "section": "⊳ Lecture / Code-along",
    "text": "⊳ Lecture / Code-along\n\nCheck-in (5 minutes)\nLet’s all sign-up for RStudio!\nRstudio Mini-tour\nPerform a live-coding mini-tour of RStudio.\n\nWhat do the different panels do?\nConsole versus a script/Rmarkdown file\n\nUsing R as a Calculator\nUsing the Console\nYou can do basic math in the Console (the bottom left part of the screen). The Console only understands R code, but it can also be treated as a calculator! Hence, we can type in some numbers and mathematical symbols.\nTry multiplying 5 and 3 in the Console (hint: * means multiply). Hit Enter to run that line of code.\nR Markdown and Code Chunks\nR Markdown (.Rmd) is a file format that lets us incorporate text and code into one document seamlessly. In fact, it is the file format for this document!\n\nFor writing text, you can type as you normally would.\nCode chunks are a bit different:\n\nNear the top right of your screen you can toggle between viewing this document as under Source or Visual.\nIf you view this document under Source, you will see that all code chunks are sandwiched between ” ```{r} ” and ” ``` “.\nBut under Visual, you can type R code in the lines under {r}.\nTo include text in chunks, you will need to put a # in front. R will not read anything in the line after # as code.\n\n\nCode chunks look like this:\n\n\n# This is a code chunk!\n\n\nA quick shortcut for adding a code chunk is Ctrl + Alt + i (Cmd + Opt + i on a Mac).\nAlternatively, you can go to Code &gt; Insert Chunk.\nTo run a chunk of code, click the green arrow on the top right corner of the chunk.\nYou can also run one or a few lines of code at a time by having your cursor on the line or highlighting multiple lines and hitting Ctrl + Enter (or Cmd + Enter on a Mac).\nSome Code Chunk Practice\nLet’s work with an example code chunk.\n\n\n# I am adding a comment to my code here.\n\n432 - 11     # subtraction\n\n[1] 421\n\n12 + 18      # addition\n\n[1] 30\n\n8 / 4        # division\n\n[1] 2\n\n3 * 5        # multiplication\n\n[1] 15\n\n\n\nNotice that if you run code in the Console rather than the code chunk in an R Markdown (.Rmd) file, you don’t need to add the {r} to tell it that you are typing R code. The Console only understands R code anyway, so we don’t need to tell it what it is!\nMath: Small Group Challenge\nWrite a line of code to raise 2 to the 3rd power (often represented as \\(2^3\\)). This will likely require a little bit of Googling to figure out.\nFirst, write and run the code in the console.\nNext, write and run the code in the code chunk below. What did you have to do differently?\n\n\n# 1) Console: 2^3 + Enter\n\n# 2) Code chunk: 2^3 + Green arrow *OR* 2^3 + Ctrl + Shift\n\n2^3\n\n[1] 8"
  },
  {
    "objectID": "course_materials.html",
    "href": "course_materials.html",
    "title": "Overview",
    "section": "",
    "text": "Under Course Materials, you will find lessons taught in WFSC 223 at the University of Arizona as well as links download lectures, assignments, discussions, and other assets used for instruction."
  },
  {
    "objectID": "course_materials.html#how-to-navigate-this-site",
    "href": "course_materials.html#how-to-navigate-this-site",
    "title": "Overview",
    "section": "",
    "text": "Under Course Materials, you will find lessons taught in WFSC 223 at the University of Arizona as well as links download lectures, assignments, discussions, and other assets used for instruction."
  },
  {
    "objectID": "course_materials.html#overall-learning-objectives",
    "href": "course_materials.html#overall-learning-objectives",
    "title": "Overview",
    "section": "Overall Learning Objectives",
    "text": "Overall Learning Objectives\nBy the end of the semester, you will be able to…\n\nDefine, differentiate, and explain the nature and application of computational methods\nfor acquiring, managing, analyzing, visualizing, and sharing data as it relates to real-\nworld natural resource scenarios.\nAssociate, examine, and compare how to infer meaning and insight from data through\nwritten, visual, and verbal communication to multiple audiences.\nSummarize, implement, and appraise multiple perspectives and make meaningful\nconnections across disciplines and social positions, think conceptually and critically, and\nsolve problems with data informed approaches."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resources",
    "section": "",
    "text": "[Add helpful things here]"
  },
  {
    "objectID": "about.html#helpful-things",
    "href": "about.html#helpful-things",
    "title": "Resources",
    "section": "",
    "text": "[Add helpful things here]"
  },
  {
    "objectID": "about.html#contact-us",
    "href": "about.html#contact-us",
    "title": "Resources",
    "section": "Contact Us",
    "text": "Contact Us\n\nGithub page\nBiodiversity Data Science Corp.\n\n\nContact Information\nEmail: hello@email.com"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data In The Wild",
    "section": "Welcome!",
    "text": "Welcome!\nThis course introduces students in the life sciences to data science through the application of quantitative reasoning and the R programming language to “real-world” problems. Below, you will find the overall narrative and material that will be covered. There are four modules that guide the student from their first steps on the Land of the Penguins, to building new roads for access to fishing sites. Each module will lead them through basic programming, data visualization, statistics, machine learning, and interpretation of quantitative concepts."
  },
  {
    "objectID": "index.html#the-narrative",
    "href": "index.html#the-narrative",
    "title": "Data In The Wild",
    "section": "The Narrative",
    "text": "The Narrative\n\n⊳ MODULE 1: Mission Antarctica!\nA new effort to establish a permanent, sustainable colony in Antarctica is being launched. Students are introduced to the field of data science, the application (RStudio, RMarkdown, Jupyter Notebooks), and programming language (R, Python) used in the course.\n\n\n⊳ MODULE 2: Good Food Gone Bad\nThere is a food poisoning outbreak among team members. Students use data visualization to determine where the problem lies (fish, not plants) and simulations to determine the root of the problem (fish tank density, not fish tank temperature).\n\n\n⊳ MODULE 3: Follow That Seal\nThe fish tanks need to be restocked, but we want to avoid fishing in places with high leopard seal density, so we track the seals. However, the radio collars on leopard seals are failing and there is a deadly conflict between fish collectors and seals. The collars come from two different manufacturers, but we need to tell how the collars are failing (days to recharge, not signal distance) and how to classify collars of unknown provenance.\n\n\n⊳ MODULE 4: March of the Penguins\nA new road is needed to access fishing sites with low leopard seal density. There are several possible routes, but we want to avoid crossing through Gentoo penguin nesting grounds. Students will build models to determine predictors of nesting success, first with bootstrapping for confidence intervals, then with linear regression."
  },
  {
    "objectID": "index.html#footnotes-for-instructors",
    "href": "index.html#footnotes-for-instructors",
    "title": "Data In The Wild",
    "section": "Footnotes (For Instructors)",
    "text": "Footnotes (For Instructors)\nThis course was developed by Drs. Katy Prudic, Jeff Oliver, Keaton Wilson, and Ellen Bledsoe. This was taught in pilot form at the University of Arizona in Spring 2020 as Settlers of Antarctica, and it has undergone multiple revisions to better communicate and optimize the material to both students and instructors over the years. At the end of the day, with the help of funding from the NSF’s Harnessing the Data Revolution, the material is designed to be adaptable to other disciplines and serve as a template for your own courses. R was used as the primary programming language here, but the course is modifiable to other languages such as Python."
  },
  {
    "objectID": "modules/module_1/module1_2.html",
    "href": "modules/module_1/module1_2.html",
    "title": "1.2: Introduction to Coding",
    "section": "",
    "text": "Students will be able to define the following terms:\n\nobject\nassignment\nvector\nfunction\ndata frame\n\nStudents will be able to run code line-by-line and as code chunks from an Rmarkdown file.\nStudents will be able to comment their code effectively.\nStudents will be able to write code assign values to variables and use these variables to perform various operations.\nStudents will be able to use help files to learn how to use functions.\nStudents will be able to recall and explain how functions operate, and the basic syntax around functions (arguments, auto-completion, parentheses).\nStudents will be able to differentiate different data classes in R.\nStudents will learn how to create their own data structures (vectors, data frames)."
  },
  {
    "objectID": "modules/module_1/module1_2.html#learning-outcomes",
    "href": "modules/module_1/module1_2.html#learning-outcomes",
    "title": "1.2: Introduction to Coding",
    "section": "",
    "text": "Students will be able to define the following terms:\n\nobject\nassignment\nvector\nfunction\ndata frame\n\nStudents will be able to run code line-by-line and as code chunks from an Rmarkdown file.\nStudents will be able to comment their code effectively.\nStudents will be able to write code assign values to variables and use these variables to perform various operations.\nStudents will be able to use help files to learn how to use functions.\nStudents will be able to recall and explain how functions operate, and the basic syntax around functions (arguments, auto-completion, parentheses).\nStudents will be able to differentiate different data classes in R.\nStudents will learn how to create their own data structures (vectors, data frames)."
  },
  {
    "objectID": "modules/module_1/module1_2.html#lecture-code-along",
    "href": "modules/module_1/module1_2.html#lecture-code-along",
    "title": "1.2: Introduction to Coding",
    "section": "⊳ Lecture / Code-along",
    "text": "⊳ Lecture / Code-along\n\nAssigning Objects\nAssignments are really key to almost everything we do in R. This is how we create permanence in R. Anything can be saved to an object, and we do this with the assignment operator, &lt;-.\nThe short-cut for &lt;- is Alt + - (or Option + - on a Mac)\n\n\n# Assigning Objects\nmass &lt;- 47.5                        # assign 47.5 to \"mass\" (in kg)\nage &lt;- 122                          # assign 122 to \"age\"\nmass &lt;- mass * 2                    # multiply mass by 2\nage &lt;- age - 20                     # subtract 20 from age\nmass_index &lt;- mass/age              # divide mass by age\nmass_sq &lt;- mass^2                   # raise to an exponent (2)\n\n\nThis is simple and you’ll rarely do it in real-world scenarios.\n1-Dimensional Data: Vectors\nWe can also assign more complex group of elements of the same type to a particular object. This is called a vector, a basic data structure in R.\n\n\n# A group of mass values assigned to \"mass_kg\"\nmass_kg &lt;- c(3, 2, 4, 9, 7, 3, 6)\n\n# View object\nmass_kg\n\n[1] 3 2 4 9 7 3 6\n\n# A group of animal names assigned to \"animals\"\nanimals &lt;- c(\"cat\", \"rat\", \"bat\")\n\n# View object\nanimals\n\n[1] \"cat\" \"rat\" \"bat\"\n\n\n\nR does everything in vectors\nData classes\nThere are a few main types in R, and they behave differently.\n\nNumeric: numbers\n\nInteger (no decimals allowed)\nDouble (decimals allowed — interchangeable with numeric)\n\nCharacter: letters or mixture\nLogical: True or False; T or F\nFactors: best used for data that need to be in a specific order; levels indicate the order\n\n\n\n# Examples of different data classes\n\n# We can view what class of data is in each of our objects if we call them\n\nmass_kg       # numeric, integer, double\n\n[1] 3 2 4 9 7 3 6\n\nanimals       # character\n\n[1] \"cat\" \"rat\" \"bat\"\n\nanimal_size &lt;- as.factor(c(\"small\", \"medium\", \"large\"))\nanimal_size   # factor puts in order \n\n[1] small  medium large \nLevels: large medium small\n\nlogic &lt;- c(T, F, F, T)  \nlogic         # logical\n\n[1]  TRUE FALSE FALSE  TRUE\n\n\n\nVectors have to contain elements that are all of the same class.\n\n\n# Will this become an object?\n# We will try putting in an integer, double, and character\nvec &lt;- c(1, 1.000, \"1\")\n\n\nSub-setting Vectors\nSometimes we want to pull out and work with specific values from a vector. This is called sub-setting (taking a smaller set of the original). To signify which element of the vector we’d like to pull out, we can use a square bracket and the number of the element we are interested in taking.\n\n\n# Use square brackets\n\nmass_kg[2]        # Pull out second element in mass_kg\n\n[1] 2\n\nmass_kg[2:4]      # Pull out 2nd, 3rd, and 4th elements from mass_kg\n\n[1] 2 4 9\n\n\n\nFunctions\nFunctions are pre-written bits of codes that perform specific tasks for us. There are ones built into R (base R), but later in the course we will see that there are more specialized ones that we can equipt using another function called library().\nFunctions are always followed by parentheses. Anything you type into the parentheses are called arguments.\n\n\n# Functions\n\n# Average the mass_kg vector from above\nmass_kg_mean &lt;- mean(mass_kg)   \n\n# View\nmass_kg_mean\n\n[1] 4.857143\n\n# Round the result in mass_kg mean\nround(mass_kg_mean)           \n\n[1] 5\n\n# Maybe we want to round to 2 digits past 0\nround(mass_kg_mean, digits = 2) \n\n[1] 4.86\n\n\n\nTo get more information about a function, use the help() function or ? name_of_function.\n\n\nhelp(round) # or type ? help\n\n\nWe can use a function called class() to figure out the data type of a vector.\n\n\n# A function that tells us the class\nclass(mass_kg)\n\n[1] \"numeric\"\n\n\n\nGroup Challenge\nLet’s practice! Write a few lines of code that do the following:\n\nCreate a vector with numbers from 6 to 1 (6, 5, 4, 3, 2, 1)\nAssign the vector to an object named vec\nSubset vec to include the last 3 numbers (should include 3, 2, 1)\nFind the sum of the numbers (hint: use the sum() function)\n\nAnswer: 6\n\n\n# Make vector\nvec &lt;- c(6, 5, 4, 3, 2, 1)\n\n# Check work\nvec\n\n[1] 6 5 4 3 2 1\n\n# Subset last three elements\nvec &lt;- vec[4:6]\n\n# Check work\nvec\n\n[1] 3 2 1\n\n# Sum final vector\nsum(vec)\n\n[1] 6\n\n\n\nAlready finished? See if you can condense your code down any further or turn around and help out a neighbor.\n\n\n# Auto-generate sequence of numbers 6 to 1\nvec &lt;- seq(6, 1)\n\n# From inside to outside:\n# (1) subset last 3 elements in vector\n# (2) sum the resulting vector\nsum(vec[4:6])\n\n[1] 6\n\n\n\n2-Dimensional Data: Data Frames\nMost of the data you will encounter is two-dimensional, i.e., it has columns and rows. Its structure resembles a spreadsheet. R is really good with these types of data.\n\nRows go side-to-side\nColumns go up-and-down\n\n\nData frames are made up of multiple vectors. Each vector becomes a column.\n\n\n# Create a simple data frame\n# First column is plant heights\n# Next column is nitrogen T/F\nplants &lt;- data.frame(height = c(55, 17, 42, 47, 68, 39),\n                     nitrogen = c(\"Y\", \"N\", \"N\", \"Y\", \"Y\", \"N\"))\n\n# View our new data frame\nplants\n\n  height nitrogen\n1     55        Y\n2     17        N\n3     42        N\n4     47        Y\n5     68        Y\n6     39        N\n\n\n\nSub-setting Data Frames\nBecause data frames are two-dimensional, we can subset data in different ways. We can select specific columns, specific rows, or filter rows by values.\nR always takes information for the row first, then the column.\n\n\n# Sub-setting data frames\n\n# 2-dimensional, so you need to specify row and then column as [row, column]\n\n# plants[3] # Will not work\n\n# Get row 4, column 1\nplants[4,1]\n\n[1] 47\n\n# Get column 2\nplants[, 2]\n\n[1] \"Y\" \"N\" \"N\" \"Y\" \"Y\" \"N\"\n\n\n\nAnother way to pull out a single column from a data frame is with the $ operator. This can really come in handy when you know the name of the column but not the position.\n\n\n# Get \"height\" column\nplants$height\n\n[1] 55 17 42 47 68 39\n\n\n\nIdeally, after typing $, R should give you a drop down list of all the column names that you can select from. Sometimes that doesn’t happen though, and that’s okay! You can also find column names / numbers by opening and looking through your data frame object in the Environment.\nDiscussion Point\nThis is a simple data set, but let’s come up with some questions.\nExample: Height of plants treated with nitrogen vs. those not treated.\n\n\n# Filter rows based on values in the nitrogen column\n\n# Isolate plants data where nitrogen is \"Y\"\nplants[plants$nitrogen == \"Y\", ]\n\n  height nitrogen\n1     55        Y\n4     47        Y\n5     68        Y\n\n# Find the mean of plants treated with nitrogen\nmean(plants[plants$nitrogen == \"Y\", 1])\n\n[1] 56.66667\n\n\n\nGroup Challenge (5 min)\nUsing help files on functions\nAs a group, find the standard deviation (sd()) of the height of plants treated with nitrogen and those not treated with nitrogen. Which group has the larger standard deviation?\n\n\n# Standard deviation of plants treated with nitrogen\nsd(plants[plants$nitrogen == \"Y\", 1])\n\n[1] 10.59874\n\n# Standard deviation of plants NOT treated with nitrogen\nsd(plants[plants$nitrogen == \"N\", 1])\n\n[1] 13.6504\n\n\n\nCome up with a definition of standard deviation (Google is your friend!), use the help file (help() or ? function) to find out how the sd() function works, and be prepared to show the code you used.\nHelpful Functions\nBelow are some functions that I often find very helpful when working with vectors and data frames:\n\nstr()\nhead() and tail()\nlength()\nncol() and nrow()\nnames()\n\n\n\n# Structure of the object\nstr(plants) \n\n'data.frame':   6 obs. of  2 variables:\n $ height  : num  55 17 42 47 68 39\n $ nitrogen: chr  \"Y\" \"N\" \"N\" \"Y\" ...\n\n# First 6 values or rows (default)\nhead(plants) \n\n  height nitrogen\n1     55        Y\n2     17        N\n3     42        N\n4     47        Y\n5     68        Y\n6     39        N\n\n# First n values or rows\nhead(plants, n = 4) \n\n  height nitrogen\n1     55        Y\n2     17        N\n3     42        N\n4     47        Y\n\n# Last n values or rows\ntail(plants, n = 4) \n\n  height nitrogen\n3     42        N\n4     47        Y\n5     68        Y\n6     39        N\n\n# For a dataframe, length() gives the number of columns\nlength(plants)  \n\n[1] 2\n\n# For a column or vector, length() gives number of rows\nlength(plants$height) \n\n[1] 6\n\n# Number of columns\nncol(plants)  \n\n[1] 2\n\n# Number of rows\nnrow(plants)  \n\n[1] 6\n\n# List of column or object names\nnames(plants) \n\n[1] \"height\"   \"nitrogen\""
  },
  {
    "objectID": "modules/module_2/module2_1.html",
    "href": "modules/module_2/module2_1.html",
    "title": "2.1: Introduction to Descriptive Statistics and Data Visualization",
    "section": "",
    "text": "Students will be able to apply basic data science knowledge to find the cause of a real-world scenario–food poisoning!\nStudents will be able to generate two types of plots using base R syntax to visualize a single continuous variable (histograms and distributions) and two continuous variables (scatter plots).\nStudents will be able to use visual-thinking skills to create visualizations that allow them to explore patterns in data, draw inferences, and create solutions."
  },
  {
    "objectID": "modules/module_2/module2_1.html#learning-outcomes",
    "href": "modules/module_2/module2_1.html#learning-outcomes",
    "title": "2.1: Introduction to Descriptive Statistics and Data Visualization",
    "section": "",
    "text": "Students will be able to apply basic data science knowledge to find the cause of a real-world scenario–food poisoning!\nStudents will be able to generate two types of plots using base R syntax to visualize a single continuous variable (histograms and distributions) and two continuous variables (scatter plots).\nStudents will be able to use visual-thinking skills to create visualizations that allow them to explore patterns in data, draw inferences, and create solutions."
  },
  {
    "objectID": "modules/module_2/module2_1.html#lecture-code-along",
    "href": "modules/module_2/module2_1.html#lecture-code-along",
    "title": "2.1: Introduction to Descriptive Statistics and Data Visualization",
    "section": "⊳ Lecture / Code-along",
    "text": "⊳ Lecture / Code-along\n\nIntroduction to The Problem\n\n\n\nWe have a wave of people getting sick across the team. People are coming in complaining of stomach sickness. Doctors have ruled out a communicable viral infection like norovirus, so it seems likely to be a food contamination issue.\nThe two main sources of food that are grown on site and distributed to team members are plants grown in hydroponic greenhouses (mostly Swiss chard, cucumbers, and radishes) and fish (tilapia — a tolerant warm-water species — and rainbow trout — a cold-water species). The combination of aquaculture and hydroponics is called aquaponics.\nTeam members’ diets vary in composition; people are allowed to choose how much of different food sources they eat.\nFortunately, we have some data we can use to investigate! We have data on the following:\n\nWhich team members are sick and how many times they’ve gone to the doctor\nSome information about each team member, such as:\n\nSex, age, height, occupation\nHow much fish and/or plant material they incorporate into their diets\n\n\nGroup Discussion\nHow might we figure out what is causing the problem? Try to focus on potential solutions that involve the data we already have (listed above).\nSpend 5 minutes brainstorming in your groups how you might figure out whether plants or fish are the culprits? Be ready to report out.\nDescriptive Statistics and Data Visualization\nIn order to begin addressing the question of what might be causing illness in our crew (and lots of other questions!), we often want to start with descriptive statistics and data visualization.\nIn this course, we will be working with two types of statistics: descriptive and inferential.\n\nDescriptive statistics — also called summary statistics — are ways of presenting, organizing, and summarizing data. Data visualization is often associated with descriptive statistics\nInferential statistics help us draw reasonable conclusions about a population based on the data we observed in a sample.\n\nIn Module 2, we will be focusing exclusively on descriptive statistics and data visualization techniques.\nAs a quick overview, descriptive statistics often include 3 elements:\n\nDistribution of the data\nMeasures of central tendency: mean, median, and mode\nMeasures of variation: range and standard deviation\n\nThe slide deck with more information about descriptive statistics is linked here.\nFor a nice overview of descriptive statistics, check out this website I showed in class. It goes a bit further than we go in this class, but it is a nice place to review!\nThe Data\nFirst we’re going to pull in the data and give it a quick inspection/exploration before we start.\nAs usual, we start by calling the tidyverse.\n\n\n# Open up the library\nlibrary(tidyverse)\n\n\nTo bring our data into R, we use a function called read_csv(). CSV (comma-separated values) are efficient ways to save 2-dimensional (“spreadsheet”) data.\n\n\n# Put data in Environment\nsick &lt;- read_csv(\"data/sick_data.csv\")\n\n# View first few rows\nhead(sick)\n\n# A tibble: 6 × 10\n  last    first sex     age height_cm weight_kg specialties perc_fish perc_plant\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Gonzal… Ange… M        35      169.      51.4 Hydrology       0.994    0.00620\n2 Navrat… John  M        19      112.      96.3 Genetics        0.297    0.703  \n3 Duff    Josh… M        26      133.      52.1 Horticultu…     0.514    0.486  \n4 Dottson Juli… M        36      140.      52.6 Climatology     0.686    0.314  \n5 al-Sul… Mune… M        26      194.      52.2 Geology         0.292    0.708  \n6 Galleg… Rich… M        29      153.      98.1 Climatology     0.329    0.671  \n# ℹ 1 more variable: doctor_trips &lt;dbl&gt;\n\n\n\nHow many observations (rows) do we have? How about variables (columns)?\nWhich variables are we particularly interested in? Are they continuous or categorical? Does it matter?\nLet’s Practice\nNow that we have a more formalized understanding of measures of central tendency and measures of variation, let’s put them into use to learn a little bit more about our data. In small groups, calculate the mean (mean()) and standard deviation (sd()) for the percent fish in our sick crew’s diets.\n\n\n# Find mean and standard deviation of %fish in people's diets\nsick %&gt;% \n  summarize(mean_fish = mean(perc_fish),\n            sd_fish = sd(perc_fish))\n\n# A tibble: 1 × 2\n  mean_fish sd_fish\n      &lt;dbl&gt;   &lt;dbl&gt;\n1     0.534   0.250\n\n\n\nData Visualization in R\nCombining data visualization with descriptive statistics is a great way to understand our data!\nThere are two main ways to make data visualizations in R: through base R, which is the syntax we learned in our first week of coding, and through ggplot2, which is a package in the tidyverse.\nFor the majority of the semester, we will be plotting in with ggplot2, which is a fun and powerful tool. However, to plot even a simple plot, ggplot2 takes some explanation. We will talk about ggplot2 in our next class but for today, let’s first make some quick-and-dirty plots in base R.\nHistograms\nWe’ve calculated some descriptive statistics about the percents of fish and plants in our sick crew members’ diets, but it didn’t tell us too much. Let’s try some data visualization to see if that gives us any additional information.\n\n\n# Histogram in base R\nhist(sick$perc_plant)\n\n\n\n\n\nGroup Discussion\nDiscuss this histogram with your group members. Some questions to consider:\n\nWhat is a histogram?\nWhat does a histogram tell us?\nWhat does each axis mean? (x-axis is horizontal, y-axis is vertical)\nWhat, if any, conclusions can we draw from this histogram in regards to the percent plants in diets and sickness?\nHow can we improve this visualization?\n\nTake about 5 minutes. Be ready to report out.\nGroup Brainstorm\nWe’ve covered one type of visualization that just shows one variable…but what we’re really interested in is figuring out if fish or plants are the culprit in food poisoning. Spend about 5 minutes in your groups sketching out a visualization that might give us insight into this.\nScatter plots\nScatter plots allow us to visualize the relationship between two continuous (or numeric) variables. For example, we can use a scatter plot to see if there is a relationship between how old a crew member is and how much fish is in their diet.\n\n\n# Scatter plot in base R\nplot(x = sick$age, y = sick$perc_fish)\n\n\n\n\n\nThis isn’t super informative, because there isn’t really a relationship between a person’s age and the amount of fish they consume. At least not in this sample.\nBuilding and Interpreting\nIn your group, create a scatter plot to determine whether there is a correlation between the percentage of fish eaten and the number of trips to the doctor in the past 6 months.\n\n\n# Another scatterplot in base R\nplot(x = sick$perc_fish, y = sick$doctor_trips)\n\n\n\n\n\nHow do we interpret this plot?"
  },
  {
    "objectID": "modules/module_2/module2_3.html",
    "href": "modules/module_2/module2_3.html",
    "title": "2.3: Plotting with ggplot2",
    "section": "",
    "text": "So far, we’ve used the base R plotting syntax. While quick plots in base R can still be really useful ways to do preliminary data exploration and visualization, we often want plots that go beyond the basics without too much additional effort. This is where ggplot2 comes in and really shines!"
  },
  {
    "objectID": "modules/module_2/module2_3.html#get-to-know-ggplot2",
    "href": "modules/module_2/module2_3.html#get-to-know-ggplot2",
    "title": "2.3: Plotting with ggplot2",
    "section": "",
    "text": "So far, we’ve used the base R plotting syntax. While quick plots in base R can still be really useful ways to do preliminary data exploration and visualization, we often want plots that go beyond the basics without too much additional effort. This is where ggplot2 comes in and really shines!"
  },
  {
    "objectID": "modules/module_2/module2_3.html#example",
    "href": "modules/module_2/module2_3.html#example",
    "title": "2.3: Plotting with ggplot2",
    "section": "Example",
    "text": "Example\nBefore we get into the nitty-gritty of how ggplot2 works, Let’s run an example using the data about our sick crew members from earlier.\nFirst, we need to load in both the tidyverse package and our data. We can remind oursevles what the data look like using the head() function.\n\n# Load package\nlibrary(tidyverse)\n\n# Load data\nsick &lt;- read_csv(\"data/sick_data.csv\")\n\n# View first few rows\nhead(sick)\n\n# A tibble: 6 × 10\n  last    first sex     age height_cm weight_kg specialties perc_fish perc_plant\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Gonzal… Ange… M        35      169.      51.4 Hydrology       0.994    0.00620\n2 Navrat… John  M        19      112.      96.3 Genetics        0.297    0.703  \n3 Duff    Josh… M        26      133.      52.1 Horticultu…     0.514    0.486  \n4 Dottson Juli… M        36      140.      52.6 Climatology     0.686    0.314  \n5 al-Sul… Mune… M        26      194.      52.2 Geology         0.292    0.708  \n6 Galleg… Rich… M        29      153.      98.1 Climatology     0.329    0.671  \n# ℹ 1 more variable: doctor_trips &lt;dbl&gt;\n\n\nHere is code to make a scatter plot of the relationship between percent fish in diets and how many trips to the doctor.\n\n# Scatterplot of % fish diet and # of doctor visits\nggplot(sick, aes(x = perc_fish, y = doctor_trips)) +\n  geom_point() +\n  labs(x = \"Percent Fish in Diet\",\n       y = \"Number of Trips to the Doctor\") +\n  theme_light()\n\n\n\n\nNice, right? In the next few classes, we will really start to see the power of ggplot. For now, though, let’s focus on how this works."
  },
  {
    "objectID": "modules/module_2/module2_3.html#ggplot2",
    "href": "modules/module_2/module2_3.html#ggplot2",
    "title": "2.3: Plotting with ggplot2",
    "section": "ggplot2",
    "text": "ggplot2\nThe package ggplot2 is part of the tidyverse.\nHere are some resources you might find helpful now or in the future:\n\nggplot2 Book\nUC Business Analytics ggplot2 intro\nR for Data Science Data Visualization chapter\n\nThe gg in ggplot2 stands for “Grammar of Graphics.” The “grammar” part is based on an idea that all statistical plots have the same fundamental features: data and mapping (and specific components of mapping).\nThe design is that you work iteratively, building up layer upon layer until you have your final plot.\nThe typical structure looks like this:\n\n# ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  \n# &lt;GEOM_FUNCTION&gt;()\n\nA few things to note:\n\nWe always start with the ggplot() function\nWe specify the dataset we want to use\nWe specify the mappings (x- and y-axes and some other bits) with the aes() function\nWe use a + to add layers\nWe specify the type of plot, or geom using one of many possible geom functions\nWe use the labs() function to clean up the labels\nWe add a theme function to make it more visually readable\n\nLet’s iteratively build up to the plot we have made above:\n\nSpecify the data\n\n\nggplot(data = sick)\n\n\n\n\n\nSpecify the x-axis (horizontal) and the y-axis (vertical) in the aes() function.\n\n\nggplot(data = sick, mapping = aes(x = perc_fish, y = doctor_trips))\n\n\n\n\n\nAdd the type of plot we want using a geom function.\n\n\nggplot(data = sick, mapping = aes(x = perc_fish, y = doctor_trips)) +\n  geom_point()\n\n\n\n\n\nClean up the axis labels with the lab() function so they are more easily interpreted.\n\n\nggplot(data = sick, mapping = aes(x = perc_fish, y = doctor_trips)) +\n  geom_point() +\n  labs(x = \"Percent Fish in Diet\",\n       y = \"Number of Trips to the Doctor\") \n\n\n\n\n\nChoose a theme function to make the plot more aesthetically pleasing. My favorites are theme_bw(), theme_classic(), and theme_light().\n\n\nggplot(sick, aes(x = perc_fish, y = doctor_trips)) +\n  geom_point() +\n  labs(x = \"Percent Fish in Diet\",\n       y = \"Number of Trips to the Doctor\") +\n  theme_light()"
  },
  {
    "objectID": "modules/module_2/module2_5.html",
    "href": "modules/module_2/module2_5.html",
    "title": "2.5: Sick Fish",
    "section": "",
    "text": "Here is our current scenario.\nWhat we know: We found a correlation between eating fish and people getting sick, but that isn’t really the root of the problem. We have also found that a number of tanks are below the average temperature that might cause issues with fish immune systems.\nWhat we don’t know: We don’t actually know if rates of disease are present above average levels in the tanks, and we definitely don’t know what kinds of factors are contributing to this problem (if there is one!).\nOur first challenge is that we can’t sample the disease rate in all tanks. It’s too expensive and takes too long. Instead of sampling all 1000 tanks, we’ve tasked our aquaculture scientists to take a sub-sample (50 tanks). Our aquaculture scientists have shared this data with us.\nLet’s scope it out:\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Load data\nsick_fish &lt;- read_csv(\"data/fish_sick_data.csv\")\n\n# View first few rows\nhead(sick_fish)\n\n# A tibble: 6 × 7\n  tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     388 tilapia           24.3       93         10        399.        3\n2     425 tilapia           24.6       98         11        400.        4\n3     420 tilapia           23.0      103          9        399.        2\n4     819 trout             14.1       85         11        401.       14\n5     176 tilapia           23.3       98         10        400.        3\n6     926 trout             13.8       79         12        400.       10\n\n\nOh no, we might need to add a Fahrenheit column… let’s practice.\n\n# Make temperature conversion function\nc_to_f &lt;- function(c = NULL){\n  f &lt;- (c * (9/5)) + 32\n  return(f)\n}\n\n# Convert temperatures and put converted value in a new column\nsick_fish &lt;- sick_fish %&gt;% \n  mutate(avg_daily_tempF = c_to_f(avg_daily_temp))"
  },
  {
    "objectID": "modules/module_2/module2_5.html#scenario",
    "href": "modules/module_2/module2_5.html#scenario",
    "title": "2.5: Sick Fish",
    "section": "",
    "text": "Here is our current scenario.\nWhat we know: We found a correlation between eating fish and people getting sick, but that isn’t really the root of the problem. We have also found that a number of tanks are below the average temperature that might cause issues with fish immune systems.\nWhat we don’t know: We don’t actually know if rates of disease are present above average levels in the tanks, and we definitely don’t know what kinds of factors are contributing to this problem (if there is one!).\nOur first challenge is that we can’t sample the disease rate in all tanks. It’s too expensive and takes too long. Instead of sampling all 1000 tanks, we’ve tasked our aquaculture scientists to take a sub-sample (50 tanks). Our aquaculture scientists have shared this data with us.\nLet’s scope it out:\n\n# Load tidyverse\nlibrary(tidyverse)\n\n# Load data\nsick_fish &lt;- read_csv(\"data/fish_sick_data.csv\")\n\n# View first few rows\nhead(sick_fish)\n\n# A tibble: 6 × 7\n  tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     388 tilapia           24.3       93         10        399.        3\n2     425 tilapia           24.6       98         11        400.        4\n3     420 tilapia           23.0      103          9        399.        2\n4     819 trout             14.1       85         11        401.       14\n5     176 tilapia           23.3       98         10        400.        3\n6     926 trout             13.8       79         12        400.       10\n\n\nOh no, we might need to add a Fahrenheit column… let’s practice.\n\n# Make temperature conversion function\nc_to_f &lt;- function(c = NULL){\n  f &lt;- (c * (9/5)) + 32\n  return(f)\n}\n\n# Convert temperatures and put converted value in a new column\nsick_fish &lt;- sick_fish %&gt;% \n  mutate(avg_daily_tempF = c_to_f(avg_daily_temp))"
  },
  {
    "objectID": "modules/module_2/module2_5.html#data-exploration",
    "href": "modules/module_2/module2_5.html#data-exploration",
    "title": "2.5: Sick Fish",
    "section": "Data Exploration",
    "text": "Data Exploration\nOne of the best ways to get an idea of what our data look like and are telling us is by calculating some summary statistics.\n\nGroup Challenge\nLet’s use the tidyverse to calculate some summary statistics for the data we have been given.\nWrite some code that produces the following values per species.\n\nMean (average) number of sick fish\nMean (average) percentage of sick fish per tank\nHow many tanks\n\n\nsick_fish %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_sick = mean(num_sick),             # mean number of fish\n            mean_perc = mean(num_sick/num_fish),    # mean percent of fish\n            n = n())                                # counts\n\n# A tibble: 2 × 4\n  species mean_sick mean_perc     n\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 tilapia      3.39    0.0336    31\n2 trout       14.7     0.193     19\n\n\nSummary statistics (such as means) are helpful for giving us some ideas about our data, but they don’t tell us the full story. Plotting data can give us some additional insights.\nBefore we plot, let’s create two different data frames: one for tilapia and one for trout.\n\n# New data frame for just tilapia\nsick_tilapia &lt;- sick_fish %&gt;% \n  filter(species == \"tilapia\")\n\n# New data frame for just trout\nsick_trout &lt;- sick_fish %&gt;% \n  filter(species == \"trout\")\n\nNow, let’s plot our data to get an idea of the distribution (spread) of the data. What might the distribution of the data tell us that an average can’t?\nWe can make histograms to see if there are some tanks that have a lot of sick fish and are increasing this average or if most of the tanks seem to have about the same number of sick fish.\nWhat would these two different scenarios tell us?\n\n# Sick tilapia histogram\nhist(sick_tilapia$num_sick)\n\n\n\n# Sick trout histogram\nhist(sick_trout$num_sick)\n\n\n\n\nLet’s compare values from tanks which are below the temperature cutoffs to those which are above the cutoffs to see if there are major difference or we can figure out some answers."
  },
  {
    "objectID": "modules/module_2/module2_5.html#using-the-if_else-function",
    "href": "modules/module_2/module2_5.html#using-the-if_else-function",
    "title": "2.5: Sick Fish",
    "section": "Using the if_else() function",
    "text": "Using the if_else() function\nBefore we start calculating means and plotting, let’s create a new column in our data frame to indicate whether the tank temperature is above or below the cutoff temperature. We will use our fish-specific data frames to do this.\nA useful function that we can use when we want to create a new column based on values in another column is the if_else() function. It operates the following way:\n\nIF a condition is true, do [something].\nIF a condition is false, do [something ELSE].\n\n\n# Start with tilapia dataframe\nsick_tilapia %&gt;% \n  mutate(temp_cutoff = if_else(condition = avg_daily_tempF &gt;= 75,\n                               true = \"above\",\n                               false = \"below\"))\n\n# A tibble: 31 × 9\n   tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1     388 tilapia           24.3       93         10        399.        3\n 2     425 tilapia           24.6       98         11        400.        4\n 3     420 tilapia           23.0      103          9        399.        2\n 4     176 tilapia           23.3       98         10        400.        3\n 5     454 tilapia           23.8      104         10        400.        0\n 6     204 tilapia           24.0       99         10        399.        6\n 7     515 tilapia           23.8      100         10        399.        7\n 8     715 tilapia           24.0      102         11        401.        5\n 9     186 tilapia           24.4      100         10        402.        0\n10     678 tilapia           23.8      102         10        400.        4\n# ℹ 21 more rows\n# ℹ 2 more variables: avg_daily_tempF &lt;dbl&gt;, temp_cutoff &lt;chr&gt;\n\n\n\nIndividual Challenge\nPractice using the if_else() function as we did above. This time, use the sick_trout dataframe. Remember to change the bits of the code that you need to!\n\n# Modify the code below to do the same operation for the trout\nsick_tilapia %&gt;% \n  mutate(temp_cutoff = if_else(condition = avg_daily_tempF &gt;= 75,\n                               true = \"above\",\n                               false = \"below\"))\n\n# A tibble: 31 × 9\n   tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1     388 tilapia           24.3       93         10        399.        3\n 2     425 tilapia           24.6       98         11        400.        4\n 3     420 tilapia           23.0      103          9        399.        2\n 4     176 tilapia           23.3       98         10        400.        3\n 5     454 tilapia           23.8      104         10        400.        0\n 6     204 tilapia           24.0       99         10        399.        6\n 7     515 tilapia           23.8      100         10        399.        7\n 8     715 tilapia           24.0      102         11        401.        5\n 9     186 tilapia           24.4      100         10        402.        0\n10     678 tilapia           23.8      102         10        400.        4\n# ℹ 21 more rows\n# ℹ 2 more variables: avg_daily_tempF &lt;dbl&gt;, temp_cutoff &lt;chr&gt;"
  },
  {
    "objectID": "modules/module_2/module2_5.html#another-type-of-iteration-for-loop",
    "href": "modules/module_2/module2_5.html#another-type-of-iteration-for-loop",
    "title": "2.5: Sick Fish",
    "section": "Another Type of Iteration: for loop",
    "text": "Another Type of Iteration: for loop\nAnother way we could write something like this is by using the if_else() function in something we call a for loop.\nBefore we do that, though, let’s talk through the general structure of a for loop. It essentially says for each value in a list, do a certain task. The “loop” is because we are “looping” through a list of values, performing the task for one value then looping back to the beginning to perform the task for the next value. We type the “task” within curly brackets, similar to a function that we write.\n\n# for loop structure:\n\n# for ([value] in [list]){\n  # Do the things I've written between the curly brackets\n# }\n\n# for loop example with years\nfor (year in 2020:2022){\n  print(paste(\"The year is\", year))\n}\n\n[1] \"The year is 2020\"\n[1] \"The year is 2021\"\n[1] \"The year is 2022\"\n\n# What is happening in this loop?\n\n# We start with year = 2020,\n# So the for loop will print \"The year is 2020\"\n# We then go back to the beginning and do this again,\n# this time year = 2021\n# So now the for loop will print \"The year is 2021\"\n# The last value in our loop is year = 2022\n# And as you would expect, that would give \"The year is 2022\"\n\nNow that we know the general structure of a for loop, we can combine it with the if_else() function to create a new column.\n\n# Create an empty column in sick_tilapia\nsick_tilapia$temp_cutoff &lt;- NA\n\n# What do we want to do?\n\n# for each value (i) in a list going from 1 to the number of rows in sick_tilapia,\n# put either \"above\" or \"below\" in the same place as (i) in the dataframe, but this time in the new column\n\n# In this case, i is equivalent to the row \n\n# So this loop will repeat however many rows are present in sick_tilapia\nfor (i in 1:nrow(sick_tilapia)){\n  \n  sick_tilapia$temp_cutoff[i] &lt;- \n    if_else(condition = sick_tilapia$avg_daily_tempF[i] &gt;= 75,\n            true = \"above\",\n            false = \"below\")\n  \n}\n\nNote: I will never ask you to write a for loop completely from scratch. I might have you copy and paste one or change some values in one, but you won’t have to write one out yourself.\n\nGroup Challenge\nTry your hand at using the for loop we wrote above to create a new temperature cutoff column in the sick_trout data frame. Remember, the cutoff for trout was 59°F.\n\n# New column\nsick_trout$temp_cutoff &lt;- NA\n\n# Same operation done on trout as done on tilapia above\nfor (i in 1:nrow(sick_trout)){\n  \n  sick_trout$temp_cutoff[i] &lt;- \n    if_else(condition = sick_trout$avg_daily_tempF[i] &gt;= 59,\n            true = \"above\",\n            false = \"below\")\n  \n}\n\n\n\nWhy for loops?\nLike with our last lesson about functions, I’ve asked you to perform a task in a new and complicated way than you need to for that task. Why?\nYou’ll find some examples here in code written for my Ph.D. dissertation."
  },
  {
    "objectID": "modules/module_2/module2_5.html#back-to-data-exploration",
    "href": "modules/module_2/module2_5.html#back-to-data-exploration",
    "title": "2.5: Sick Fish",
    "section": "Back to Data Exploration",
    "text": "Back to Data Exploration\nWe now have 2 data frames, one with tilapia data and one with trout data. Each data frame also has a new column called temp_cutoff. On your own or with a partner, start exploring the data to figure out if there are differences between warm and cold tilapia and warm and cold trout.\nCan you pinpoint an issue? Let’s start by comparing means.\n\n# Mean number of sick tilapia per temperature cutoff\nsick_tilapia %&gt;% \n  group_by(temp_cutoff) %&gt;% \n  summarise(mean_sick = mean(num_sick))\n\n# A tibble: 2 × 2\n  temp_cutoff mean_sick\n  &lt;chr&gt;           &lt;dbl&gt;\n1 above            2.58\n2 below            3.89\n\n# Mean number of sick tilapia per temperature cutoff\nsick_trout %&gt;% \n  group_by(temp_cutoff) %&gt;% \n  summarise(mean_sick = mean(num_sick))\n\n# A tibble: 2 × 2\n  temp_cutoff mean_sick\n  &lt;chr&gt;           &lt;dbl&gt;\n1 above            16  \n2 below            13.8\n\n# Alternative:\n\n# Put tilapia and trout data back together\nsick_fish &lt;- bind_rows(sick_tilapia, sick_trout)\n\n# Find mean number of sick fish for each species, for each temperature cutoff\nsick_fish %&gt;% \n  group_by(species, temp_cutoff) %&gt;% \n  summarise(mean_sick = mean(num_sick))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   species [2]\n  species temp_cutoff mean_sick\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n1 tilapia above            2.58\n2 tilapia below            3.89\n3 trout   above           16   \n4 trout   below           13.8 \n\n\nNot much popping out in the means. Next thing to check would be histograms of the number of sick fish for both species, above and below the cutoffs.\nFor now, I would recommend making 4 different data frames (this isn’t “best practice” but it is really helpful while you are learning).\n\n# Separate tilapia below temperature cutoff\ncold_tilapia &lt;- sick_tilapia %&gt;% \n  filter(temp_cutoff == \"below\")\n\n# Separate tilapia above temperature cutoff\nwarm_tilapia &lt;- sick_tilapia %&gt;% \n  filter(temp_cutoff == \"above\")\n\n# Separate trout below temperature cutoff\ncold_trout &lt;- sick_trout %&gt;% \n  filter(temp_cutoff == \"below\")\n\n# Separate trout above temperature cutoff\nwarm_trout &lt;- sick_trout %&gt;% \n  filter(temp_cutoff == \"above\")\n\n# Tilapia histograms\nhist(cold_tilapia$num_sick)\n\n\n\nhist(warm_tilapia$num_sick)\n\n\n\n# Trout histograms\nhist(cold_trout$num_sick)\n\n\n\nhist(warm_trout$num_sick)"
  },
  {
    "objectID": "modules/module_2/module2_7.html",
    "href": "modules/module_2/module2_7.html",
    "title": "2.7: Wrap-Up",
    "section": "",
    "text": "At the beginning of Module 2, we set out to discover what was causing food poisoning among our colleagues at our Antarctic base. Let’s put everything we’ve learned about descriptive statistics and data visualization to use to try to hunt down what the problem is.\n\n\nLet’s load the package and data we will need.\n\n\n\n# Load library\nlibrary(tidyverse)\n\n\n\n\nFirst, we need our dataset!\n\n# Load dataset\nsick_fish &lt;- read_csv(\"data/fish_sick_data.csv\") \n\nLet’s check out our data and remind ourselves what we are working with.\n\n# View first few rows of data\nhead(sick_fish)\n\n# A tibble: 6 × 7\n  tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     388 tilapia           24.3       93         10        399.        3\n2     425 tilapia           24.6       98         11        400.        4\n3     420 tilapia           23.0      103          9        399.        2\n4     819 trout             14.1       85         11        401.       14\n5     176 tilapia           23.3       98         10        400.        3\n6     926 trout             13.8       79         12        400.       10\n\n\n\n\n\n\nLast class, we plotted the number of sick fish. Let’s remind ourselves what that looked like. Make a plot that compares the numbers of sick fish per species. We actually have a few options!\n\n# Density plot\nggplot(sick_fish, aes(num_sick, color = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Number of Sick Fish in Tanks\",\n       y = \"Density\",\n       fill = \"Species\") +\n  theme_classic()\n\n\n\n\n\n\nWait a second! Take a look back at the data. There is a “number of fish” column, indicating the total number of fish in the tank, and it looks like those numbers can differ pretty widely.\nWe should probably take into account how many fish there are in the tank to begin with. 12 sick fish out of 50 is probably a bigger deal than 12 sick fish out of 100!\nWhat we need to do is calculate a density of fish — number of sick fish / number of total fish.\n\n# Take into account the number of fish in the tank: \n# Density of sick fish\nsick_fish &lt;- sick_fish %&gt;% \n  mutate(density = num_sick/num_fish) \n\n\n\n\nLet’s make sure our conclusions about trout being the true culprits still hold when we account for the total number of fish in the tank.\nWork in small groups to do the following:\n\nFind the average number and standard deviation of sick fish for both species\nMake a plot that compares the distributions of sick fish numbers for both species (you have multiple options here!)\n\n\n# Mean and standard deviation of density of sick fish for each species\nsick_fish %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean_sick_fish = mean(density),\n            sd_sick_fish = sd(density))\n\n# A tibble: 2 × 3\n  species mean_sick_fish sd_sick_fish\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n1 tilapia         0.0336       0.0207\n2 trout           0.193        0.0327\n\n# Plot density sick fish for each species\nggplot(sick_fish, aes(species, density, color = species)) +\n  geom_boxplot() + \n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Density of Sick Fish in Tanks\",\n       color = \"Species\") +\n  theme_light()\n\n\n\n# And a histogram\nggplot(sick_fish, aes(density, fill = species)) +\n  geom_histogram(alpha = 0.5, bins = 15)\n\n\n\n\nUh oh…the trout densities look even worse than just the number of sick fish. We need to take a closer look at what is going on in the trout tanks!\nWe should create a data frame that only contains trout to work with for the rest of our analyses. Take a few minutes to work on that; call it sick_trout.\n\n# Only trout\nsick_trout &lt;- sick_fish %&gt;% \n  filter(species == \"trout\")\n\n\n\n\n\nTake a look back at the data frame. Which columns are environmental variables that could be driving the issues?\nAre those columns continuous or categorical? What plot type have we talked about that might help us find a relationship between density and each of these variables (one at a time…)?\nIn small groups, make plots using density column in the sick_trout data to try to figure out which environmental factor is causing problems in the trout. Treat your variables as continuous.\n\n# Is it the number of fish?\nggplot(sick_trout, aes(x = density, y = num_fish)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point() \n\n\n\n# Is it the average daily temperature?\nggplot(sick_trout, aes(x = density, y = avg_daily_temp)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point()\n\n\n\n# Is it the day length?\nggplot(sick_trout, aes(density, day_length)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point() \n\n\n\n\nWhat do we think is the environmental driver causing issues with the trout?"
  },
  {
    "objectID": "modules/module_2/module2_7.html#what-is-causing-the-food-poisoning",
    "href": "modules/module_2/module2_7.html#what-is-causing-the-food-poisoning",
    "title": "2.7: Wrap-Up",
    "section": "",
    "text": "At the beginning of Module 2, we set out to discover what was causing food poisoning among our colleagues at our Antarctic base. Let’s put everything we’ve learned about descriptive statistics and data visualization to use to try to hunt down what the problem is.\n\n\nLet’s load the package and data we will need.\n\n\n\n# Load library\nlibrary(tidyverse)\n\n\n\n\nFirst, we need our dataset!\n\n# Load dataset\nsick_fish &lt;- read_csv(\"data/fish_sick_data.csv\") \n\nLet’s check out our data and remind ourselves what we are working with.\n\n# View first few rows of data\nhead(sick_fish)\n\n# A tibble: 6 × 7\n  tank_id species avg_daily_temp num_fish day_length tank_volume num_sick\n    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     388 tilapia           24.3       93         10        399.        3\n2     425 tilapia           24.6       98         11        400.        4\n3     420 tilapia           23.0      103          9        399.        2\n4     819 trout             14.1       85         11        401.       14\n5     176 tilapia           23.3       98         10        400.        3\n6     926 trout             13.8       79         12        400.       10\n\n\n\n\n\n\nLast class, we plotted the number of sick fish. Let’s remind ourselves what that looked like. Make a plot that compares the numbers of sick fish per species. We actually have a few options!\n\n# Density plot\nggplot(sick_fish, aes(num_sick, color = species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Number of Sick Fish in Tanks\",\n       y = \"Density\",\n       fill = \"Species\") +\n  theme_classic()\n\n\n\n\n\n\nWait a second! Take a look back at the data. There is a “number of fish” column, indicating the total number of fish in the tank, and it looks like those numbers can differ pretty widely.\nWe should probably take into account how many fish there are in the tank to begin with. 12 sick fish out of 50 is probably a bigger deal than 12 sick fish out of 100!\nWhat we need to do is calculate a density of fish — number of sick fish / number of total fish.\n\n# Take into account the number of fish in the tank: \n# Density of sick fish\nsick_fish &lt;- sick_fish %&gt;% \n  mutate(density = num_sick/num_fish) \n\n\n\n\nLet’s make sure our conclusions about trout being the true culprits still hold when we account for the total number of fish in the tank.\nWork in small groups to do the following:\n\nFind the average number and standard deviation of sick fish for both species\nMake a plot that compares the distributions of sick fish numbers for both species (you have multiple options here!)\n\n\n# Mean and standard deviation of density of sick fish for each species\nsick_fish %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean_sick_fish = mean(density),\n            sd_sick_fish = sd(density))\n\n# A tibble: 2 × 3\n  species mean_sick_fish sd_sick_fish\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n1 tilapia         0.0336       0.0207\n2 trout           0.193        0.0327\n\n# Plot density sick fish for each species\nggplot(sick_fish, aes(species, density, color = species)) +\n  geom_boxplot() + \n  geom_jitter(alpha = 0.5, width = 0.1) +\n  labs(x = \"Species\",\n       y = \"Density of Sick Fish in Tanks\",\n       color = \"Species\") +\n  theme_light()\n\n\n\n# And a histogram\nggplot(sick_fish, aes(density, fill = species)) +\n  geom_histogram(alpha = 0.5, bins = 15)\n\n\n\n\nUh oh…the trout densities look even worse than just the number of sick fish. We need to take a closer look at what is going on in the trout tanks!\nWe should create a data frame that only contains trout to work with for the rest of our analyses. Take a few minutes to work on that; call it sick_trout.\n\n# Only trout\nsick_trout &lt;- sick_fish %&gt;% \n  filter(species == \"trout\")\n\n\n\n\n\nTake a look back at the data frame. Which columns are environmental variables that could be driving the issues?\nAre those columns continuous or categorical? What plot type have we talked about that might help us find a relationship between density and each of these variables (one at a time…)?\nIn small groups, make plots using density column in the sick_trout data to try to figure out which environmental factor is causing problems in the trout. Treat your variables as continuous.\n\n# Is it the number of fish?\nggplot(sick_trout, aes(x = density, y = num_fish)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point() \n\n\n\n# Is it the average daily temperature?\nggplot(sick_trout, aes(x = density, y = avg_daily_temp)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point()\n\n\n\n# Is it the day length?\nggplot(sick_trout, aes(density, day_length)) +\n  #geom_smooth(method = 'lm', se = FALSE) +\n  geom_point() \n\n\n\n\nWhat do we think is the environmental driver causing issues with the trout?"
  },
  {
    "objectID": "modules/module_3/module3_2.html",
    "href": "modules/module_3/module3_2.html",
    "title": "3.2: T-Tests",
    "section": "",
    "text": "When we are comparing data from two groups, we often want to compare the mean values of the different groups to see if there are differences. We can do this in a number of ways:\n\nNumerically (descriptive statistics)\nVisually\nStatistically (inferential statistics)\n\nIn this course so far, we have done the first 2: by calculating the mean values of groups and by plotting histograms, density plots, and box-plots.\nToday, we will be exploring the statistical side, using inferential statistics. Once again, we will be using our collars data, focusing on the battery life and the signal distance.\nLet’s get set-up by loading the tidyverse and reading in our data.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\ncollars &lt;- read_csv(\"data/collar_data.csv\")\n\nLet’s take a quick look at the data structure to remind ourselves what data we are using.\n\n# View first few rows\nhead(collars)\n\n# A tibble: 6 × 5\n  collar_id maker          battery_life signal_distance  fail\n      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1 Collarium Inc.         141.           4171.     0\n2         2 Collarium Inc.         121.           4134.     0\n3         3 Collarium Inc.         126.           4277.     1\n4         4 Collarium Inc.         127.           4198.     0\n5         5 Collarium Inc.         141.           4173.     1\n6         6 Collarium Inc.         105.           4175.     0\n\n\nBefore we dive in, if you need a refresher on inferential statistics, check out this powerpoint."
  },
  {
    "objectID": "modules/module_3/module3_2.html#comparing-two-means",
    "href": "modules/module_3/module3_2.html#comparing-two-means",
    "title": "3.2: T-Tests",
    "section": "",
    "text": "When we are comparing data from two groups, we often want to compare the mean values of the different groups to see if there are differences. We can do this in a number of ways:\n\nNumerically (descriptive statistics)\nVisually\nStatistically (inferential statistics)\n\nIn this course so far, we have done the first 2: by calculating the mean values of groups and by plotting histograms, density plots, and box-plots.\nToday, we will be exploring the statistical side, using inferential statistics. Once again, we will be using our collars data, focusing on the battery life and the signal distance.\nLet’s get set-up by loading the tidyverse and reading in our data.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\ncollars &lt;- read_csv(\"data/collar_data.csv\")\n\nLet’s take a quick look at the data structure to remind ourselves what data we are using.\n\n# View first few rows\nhead(collars)\n\n# A tibble: 6 × 5\n  collar_id maker          battery_life signal_distance  fail\n      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1 Collarium Inc.         141.           4171.     0\n2         2 Collarium Inc.         121.           4134.     0\n3         3 Collarium Inc.         126.           4277.     1\n4         4 Collarium Inc.         127.           4198.     0\n5         5 Collarium Inc.         141.           4173.     1\n6         6 Collarium Inc.         105.           4175.     0\n\n\nBefore we dive in, if you need a refresher on inferential statistics, check out this powerpoint."
  },
  {
    "objectID": "modules/module_3/module3_2.html#t-tests",
    "href": "modules/module_3/module3_2.html#t-tests",
    "title": "3.2: T-Tests",
    "section": "T-Tests",
    "text": "T-Tests\nWhen we have a categorical variable with 2 categories, the statistical test that we use to determine if the two categories are statistically significantly different from each other is called a t-test.\n\nWhen to Use a t-test\nTo run a t-test, the following things need to be true:\n\nThe variable that we use to create our groups (independent variable) is categorical and has only 2 categories.\nThe variable that we want to compare between groups (dependent or response variable) is numeric.\n\n\nIndependent vs. Dependent Variables\nWhy are the variables called independent and dependent, do you think?\n\nThe independent variable is the cause. It does not change in response to other variables in the data.\nThe dependent variable is the effect. We expect that it does change in response to the independent variable.\n\nWant more info on independent vs. dependent variables? Check out this link.\n\n\n\nHypothesis Testing\nThe first step in running any statistical test (t-test or otherwise) is hypothesis testing.\nWe build the our hypotheses around our independent and dependent variables.\nIn this case, we want to know if there is a meaningful (read: statistical) difference between each collar manufacturer in their average values for battery life. To start, let’s identify our independent and dependent variables:\n\nIndependent: Maker, categorical\nDependent: Battery_life, continuous/numerical\n\nBased on our question, we can now set up two different statistical hypotheses: the null hypothesis and the alternative hypothesis.\nThe null hypothesis always states that there is no difference or no relationship. We can think of the null hypothesis as our starting place—this is our default assumption. The alternative hypothesis is, well, the alternative to the null hypothesis. For example:\n\nNull Hypothesis (\\(H_{0}\\)): There is no difference in the means of battery life between the two collar makers\nAlternative Hypothesis (\\(H_{A}\\)): There is no difference in the means of battery life between the two collar makers\n\nHow can we determine whether we should not reject (“accept”) or reject the null hypothesis? That’s where statistics come in.\n(Note: For reasons I won’t go into, we never accept the alternative hypothesis, we only reject the null hypothesis.)\n\n\nRunning a t-test\nThere is a set of statistical tools that can help us whether or not there is a difference between 2 means. This group of tools are called t-tests.\nLet’s briefly remind ourselves of the logic here.\n\nOur data are a sample of a larger population (think of the population as all of the collars ever produced by both companies).\n\nRemember how we sampled our fish tank data for sick fish instead of getting data for every single tank? Same idea.\n\nWe’re interested in the the difference in the means between the two groups.\n\nIf they’re exactly the same, the difference in means would be 0.\nIf they’re different, the difference between the means will be something either larger or smaller than 0.\n\nHowever, because we only have data from a random sample of collars, there will be some variation in our numbers due to sampling error.\nWe want to know if the difference in means is due to sampling error alone or due to an actual difference between manufacturers.\n\nThe t-test allows us to determine if the means are different due to the random variation or if it represents an actual difference.\nLet’s run some code to perform our first t-test!\nWe use a function called t.test().\n\nThe first argument describes the test we want to run using column names. The structure is always dependent ~ independent.\nThe second argument is the data frame we are referencing.\n\n\n# A t-test for battery life\nt.test(battery_life ~ maker, data = collars)\n\n\n    Welch Two Sample t-test\n\ndata:  battery_life by maker\nt = -15.966, df = 89.015, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Budget Collars LLC and group Collarium Inc. is not equal to 0\n95 percent confidence interval:\n -38.98179 -30.35307\nsample estimates:\nmean in group Budget Collars LLC     mean in group Collarium Inc. \n                        86.79449                        121.46192 \n\n\nThankfully, the code isn’t too onerous. Interpreting the results is a different matter, though…\n\n\nInterpreting the Results of a t-test\nLet’s talk through it:\n\nt: this is what we call the t-value, or t-statistic.\n\nHere, it is a metric of how different the difference of the two means is from 0.\nNote that it doesn’t correspond directly to the difference — it is taking into account the actual difference as well as the variation in the data sets.\nTake home: big values (can be positive or negative) indicate a big difference from 0 while small values are mean the difference is close to 0.\n\ndf: stands for degrees of freedom\n\nIt’s a measure of your sample size and some other stuff — honestly, this is not something we’re really going to focus on here.\n\np-value: a measure of certainty of the difference outlined in the t-statistic above\n\nFor the test above, our p-value is very small: 0.00000000000000022\n\nWait, how did I get from 2.2e-16 to 0.00000000000000022?\nThe e-16 means that we need to move the decimal space to the left 16 times, creating a very small number!\nIf the number after the e were positive (e.g., e+5), I would move the decimal 5 places to the right, creating a very big number.\n\nThis means that there is an extremely low probability that the difference in means is due to random variation in our sample alone.\nThere are a lot of benchmarks in different fields for what this value should be below to consider the difference significant; typically a p-value below 0.05 is considered significant.\n\n95 percent confidence interval: this is the range that we can expect the test statistic (difference in means) to fall in 95% of the time given the data.\n\nOur test statistic is the difference in means of Budget Collars and Collarium, or 86.8 - 121.5 = -34.7\nIf we randomly sample a group of collars from the “population”, the difference in means will fall between -30.35 and -38.98 about 95% of the time\n\n\nLet’s quickly remind ourselves how p-values work:\n\nGiven our very small p-value here, what can we conclude?\n\nIs the p-value below our cut-off for significance (0.05)?\nIs our result statistically significant?\nDoes that mean we should or should not reject the null hypothesis?\nSo…is there a “real” difference in the means between the two companies or not?\n\n\nLet’s Practice!\nWe want to know if there is a significant difference in the average signal distances for the two companies.\n\nIdentify your independent and dependent variables.\n\n\nIndependent: maker, categorical\nDependent: signal_distance, continuous\n\n\nWrite out your null and alternative hypotheses.\n\n\nNull Hypothesis (\\(H_{0}\\)): there is no difference in average signal distance between collar makers\nAlternative Hypothesis (\\(H_{A}\\)): there is a difference in average signal distance between collar makers\n\n\nRun the t-test\n\n\n# A t-test for signal distance\nt.test(signal_distance ~ maker, data = collars)\n\n\n    Welch Two Sample t-test\n\ndata:  signal_distance by maker\nt = 15.837, df = 92.104, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Budget Collars LLC and group Collarium Inc. is not equal to 0\n95 percent confidence interval:\n  93.80549 120.70737\nsample estimates:\nmean in group Budget Collars LLC     mean in group Collarium Inc. \n                        4302.074                         4194.817 \n\n\n\nInterpret the t-test\n\n\nIs the p-value below our cut-off for significance (0.05)? Yes, p &lt; 0.05\nIs our result statistically significant? Yes!\nDoes that mean we should or should not reject the null hypothesis? REJECT\nSo… is there a “real” difference in the means between the two companies or not? Yes, there is a real difference"
  },
  {
    "objectID": "modules/module_3/module3_4.html",
    "href": "modules/module_3/module3_4.html",
    "title": "3.4: Combining Data (Joins and Binds)",
    "section": "",
    "text": "After the series of incidents where a number of the collars made by Budget Collars LLC seem to be failing, our team decided to try and replace as many of them as possible. Besides, their battery life is far inferior.\nWe’re placing as many collars on as many seals as we can, and we are starting to run short on collars.\nOne of our intrepid data science team members found an old box of collars and some data on all of the collars (it was a real challenge getting this data off of an old floppy drive, but she managed).\n\n\nWe’re going to spend the next lessons tackling two tasks:\n1) First, we’ll work to join data sets together.\nOur main goal is to create a single data set that includes the data we have previously looked at for collars, the data on the new collars, as well as the additional data on the old collars.\nThis is a little tricky, as the collar IDs need to be matched up with their counterparts across datasets, and new collars need be added. Unfortunately, the new collars have IDs and some additional data, but we don’t know the maker of the new collars.\n2) Second, we will try to identify the maker of mystery collars.\nIn order to do this, we will skim the surface of machine learning. We can use a common classification algorithm, K-Nearnest Neighbors (KNN), to predict which maker made which of our unidentified collars. Don’t worry, we won’t go into too much detail here, but I want to you to at least have a general idea of how it works."
  },
  {
    "objectID": "modules/module_3/module3_4.html#the-context",
    "href": "modules/module_3/module3_4.html#the-context",
    "title": "3.4: Combining Data (Joins and Binds)",
    "section": "",
    "text": "After the series of incidents where a number of the collars made by Budget Collars LLC seem to be failing, our team decided to try and replace as many of them as possible. Besides, their battery life is far inferior.\nWe’re placing as many collars on as many seals as we can, and we are starting to run short on collars.\nOne of our intrepid data science team members found an old box of collars and some data on all of the collars (it was a real challenge getting this data off of an old floppy drive, but she managed).\n\n\nWe’re going to spend the next lessons tackling two tasks:\n1) First, we’ll work to join data sets together.\nOur main goal is to create a single data set that includes the data we have previously looked at for collars, the data on the new collars, as well as the additional data on the old collars.\nThis is a little tricky, as the collar IDs need to be matched up with their counterparts across datasets, and new collars need be added. Unfortunately, the new collars have IDs and some additional data, but we don’t know the maker of the new collars.\n2) Second, we will try to identify the maker of mystery collars.\nIn order to do this, we will skim the surface of machine learning. We can use a common classification algorithm, K-Nearnest Neighbors (KNN), to predict which maker made which of our unidentified collars. Don’t worry, we won’t go into too much detail here, but I want to you to at least have a general idea of how it works."
  },
  {
    "objectID": "modules/module_3/module3_4.html#combining-data-joins-and-binds",
    "href": "modules/module_3/module3_4.html#combining-data-joins-and-binds",
    "title": "3.4: Combining Data (Joins and Binds)",
    "section": "Combining Data (Joins and Binds)",
    "text": "Combining Data (Joins and Binds)\nOften times, we have a lot of data for one project that are related but storing all of the data in one file would add unnecessary redundancy (e.g., data in certain rows would need to be repeated too often). Other times, data has been collected separately and needs to be combined before analysis.\nBeing able to join together data from related tables is a key skill in data science, and for working with larger data structures (databases with their own languages, like SQL).\n\nThe Data\nLet’s load in the tidyverse and the data we’re working.\n\n# Load the tidyverse\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\nWarning: package 'forcats' was built under R version 4.2.2\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n# Loading in the data\ncollars &lt;- read_csv(\"data/collar_data.csv\")\nnew_collars &lt;- read_csv(\"data/new_collars.csv\")\nold_collars_new_data &lt;- read_csv(\"data/old_collars_new_data.csv\")\n\n# Tell me about these data, how are we going to join them?\n\nFirst, let’s explore our data. We want to focus on 2 things here:\n\nThe columns: Which ones match columns in other datasets?\nCollar identity: Which datasets have matching collars or new collars?\n\n\n# View first few rows of each dataset\nhead(collars)\n\n# A tibble: 6 × 5\n  collar_id maker          battery_life signal_distance  fail\n      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1         1 Collarium Inc.         141.           4171.     0\n2         2 Collarium Inc.         121.           4134.     0\n3         3 Collarium Inc.         126.           4277.     1\n4         4 Collarium Inc.         127.           4198.     0\n5         5 Collarium Inc.         141.           4173.     1\n6         6 Collarium Inc.         105.           4175.     0\n\nhead(new_collars)\n\n# A tibble: 6 × 6\n  collar_id maker battery_life signal_distance antenna_length weight\n      &lt;dbl&gt; &lt;lgl&gt;        &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n1       129 NA           104.            4322.           5.86   21.8\n2       138 NA            91.2           4297.           6.78   20.1\n3       134 NA            88.0           4279.           6.77   23.0\n4       140 NA            79.9           4253.           7.91   22.9\n5       104 NA           127.            4179.           4.82   23.3\n6       118 NA           127.            4208.           5.67   23.6\n\nhead(old_collars_new_data)\n\n# A tibble: 6 × 4\n  collar_id maker              antenna_length weight\n      &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt;\n1        30 Collarium Inc.               6.74   20.2\n2        51 Budget Collars LLC           5.99   25.1\n3        60 Budget Collars LLC           5.56   18.4\n4        19 Collarium Inc.               4.94   23.1\n5        53 Budget Collars LLC           4.95   18.6\n6        25 Collarium Inc.               5.17   24.4\n\n\n\n\nDiagramming\nIn small groups, talk through the process of combining these three datasets. Think about the following:\n\nWhich columns match and which ones don’t?\nWhich rows match and which ones don’t?\nDoes the order in which we combine datasets matter?\n\nDraw out a diagram that represents how this process might go."
  },
  {
    "objectID": "modules/module_3/module3_4.html#joins-vs.-binds",
    "href": "modules/module_3/module3_4.html#joins-vs.-binds",
    "title": "3.4: Combining Data (Joins and Binds)",
    "section": "Joins vs. Binds",
    "text": "Joins vs. Binds\nNow that we’ve decided on a process for how to combine our data, let’s figure out which functions we are going to use to accomplish this task.\nWe have 2 main methods of combining data sets, and they work in different ways.\n\nJoins\nJoins are arguably the more complicated of the two types of ways to combine data, but they are, therefore, the more flexible and useful.\nThe magic of comes comes because they match up columns of data based on unique identifiers in each row of data.\nIn the following diagram, the two example data frames have the column x1 in common, and each of the values in x1 are unique (no repeats in the same data frame). When combining the data sets, all of the columns are added, and their rows are matched up to their respective values in the x1 column.\nThis can happen a couple ways, depending on which data frame is the reference and how much data you want to retain.\n\n\n\nBinds\nThe other way we can combine data sets in through binds. Binds act similarly to gluing datasets together.\nThey don’t match up data based on unique identifiers; instead they match up data by column name (bind_rows) or row position (bind_cols)\n\nHow should we go about combining our three data sets? Come up with a plan that you think will work."
  },
  {
    "objectID": "modules/module_3/module3_4.html#task-1-combine-our-data",
    "href": "modules/module_3/module3_4.html#task-1-combine-our-data",
    "title": "3.4: Combining Data (Joins and Binds)",
    "section": "Task 1: Combine Our Data",
    "text": "Task 1: Combine Our Data\n\nStep 1\nOur first step in to merge the old collar data with the new data about those old collars.\n\n# Use a left join\n\n# Full join would accomplish the same thing in this case because we don't have any missing rows\n\nold_collars_combined &lt;- collars %&gt;% \n  left_join(old_collars_new_data, by = c(\"collar_id\", \"maker\"))\n\nIf we take a look our new data frame, we should hopefully see that the values for antenna_length and weight have been matched up with their respective collar id.\nCould we have used another tactic to combine these two datasets? What would the pros and cons be?\n\n\nStep 2\nNow we need to add the new collars to our dataset. What is our best method for combining?\n\n# Add new collars\nfull &lt;- bind_rows(old_collars_combined, new_collars)\n\nLet’s take a look at our new data frame! Have we correctly accomplished our first task?\n\n\nSaving our New Data\nNow that we’ve accomplished our first task, we are going to want to use this combined dataset to accomplish our next task, which is using a classification algorithm to help us predict which maker made the mystery collars.\nTo save our data as a .csv file that we can use in another analysis, we are going to use a function that exports the dataset (write_csv) instead of importing it (read_csv).\nThe write_csv function requires the name of the dataframe to export as the first argument and the name of the file we want to create as the second argument.\n\n# Save our new data\nwrite_csv(full, \"data/all_collar_data.csv\")\n\nIf we look over in our Files tab, you should see your new .csv file!"
  },
  {
    "objectID": "modules/module_4/module4_1.html",
    "href": "modules/module_4/module4_1.html",
    "title": "4.1: Roads and Regressions",
    "section": "",
    "text": "Last module, we tried to figure out which bays we should prioritize for fishing based on the following data sets:\n\nRadio collar data\nFish catch data\nLeopard seal abundance data\n\nBased on our results from Assignment 2, it looks like Sulzberger Bay and Hope Bay are our best bets. Now, we need to make these bays accessible over land to transport the fish we’ve caught to our home base.\nWe want to build a road while minimizing our impact on the delicate antarctic ecosystem.\nOur first order of business is to make sure that we avoid areas with that are well-suited for Antarctica hair grass (Deschamsia antarctica), one of only two flowering species of plants on the continent.\n\nWe want to know what environmental conditions are associated with hair grass. This way, we can avoid areas where these conditions are met and not destroy precious habitat for the hair grass.\nIt would take far too long to survey every bit of land between our base and our fishing spots, so we are going to build a model based on some samples of where hair grass is found to help us predict where else it might be.\n\n\nA model is a way for us to take complex systems and break them down into small, more understandable bits. We can use models to help us understand the relationship between different variables. We can then use those model to make predictions based on those relationships."
  },
  {
    "objectID": "modules/module_4/module4_1.html#the-scenario",
    "href": "modules/module_4/module4_1.html#the-scenario",
    "title": "4.1: Roads and Regressions",
    "section": "",
    "text": "Last module, we tried to figure out which bays we should prioritize for fishing based on the following data sets:\n\nRadio collar data\nFish catch data\nLeopard seal abundance data\n\nBased on our results from Assignment 2, it looks like Sulzberger Bay and Hope Bay are our best bets. Now, we need to make these bays accessible over land to transport the fish we’ve caught to our home base.\nWe want to build a road while minimizing our impact on the delicate antarctic ecosystem.\nOur first order of business is to make sure that we avoid areas with that are well-suited for Antarctica hair grass (Deschamsia antarctica), one of only two flowering species of plants on the continent.\n\nWe want to know what environmental conditions are associated with hair grass. This way, we can avoid areas where these conditions are met and not destroy precious habitat for the hair grass.\nIt would take far too long to survey every bit of land between our base and our fishing spots, so we are going to build a model based on some samples of where hair grass is found to help us predict where else it might be.\n\n\nA model is a way for us to take complex systems and break them down into small, more understandable bits. We can use models to help us understand the relationship between different variables. We can then use those model to make predictions based on those relationships."
  },
  {
    "objectID": "modules/module_4/module4_1.html#data",
    "href": "modules/module_4/module4_1.html#data",
    "title": "4.1: Roads and Regressions",
    "section": "Data",
    "text": "Data\n\nKnowing that we would soon be building roads, we asked our botanists to collect data for us on key components of the hairgrass’ environment. Since it would take too long to sample everywhere hairgrass grows, they collected data from a sample of the hairgrass population.\nOur botanists collected data for the following variables:\n\nsoil pH: most plants prefer mildly acidic to neutral environments\nnitrogen (N) content: as percentage per 100 mL soil sample; important for plant growth and tissue building\nphosphorous (P) content: as percentage per 100 mL soil sample; important for plant growth and tissue building\npercent rock: how rocky the location is; rocks in soil impact water drainage and temperature\nmax windspeed (knots per hour): extreme wind can pose a challenge to plants of all types\naverage summer temperature (C): temperature in the growing season\npenguin density: the number of penguins per 5 m^2 within 100 m of the sample quadrant for hair grass; penguin poop increases nitrogen content in the system\nhairgrass density: number of individual clumps (tussocks) of hairgrass per 1 m^2\n\nThis data is based on this article: Parnikoza, et al. 2007"
  },
  {
    "objectID": "modules/module_4/module4_1.html#summarize-and-visualize",
    "href": "modules/module_4/module4_1.html#summarize-and-visualize",
    "title": "4.1: Roads and Regressions",
    "section": "Summarize and Visualize",
    "text": "Summarize and Visualize\nAs we see above, there are many environmental conditions that may be associated with hair grass density.\nFor this lesson, we are going to focus on two: nitrogen (N) content and soil pH.\n\nSet-Up\nAs usual, we start with loading our packages and our data.\n\n# Load library\nlibrary(tidyverse)\n\n# Load data\nhairgrass &lt;- read_csv(\"data/hairgrass_data.csv\")\n\nLet’s take a look at the data in the data set. What does each row represent?\n\n# View first few rows\nhead(hairgrass)\n\n# A tibble: 6 × 10\n  location_ID soil_pH p_content percent_soil_rock max_windspeed_knots\n        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n1           1    4.9     5.49                44.5                14.9\n2           2    6.94    8.53                50.5                11.0\n3           3    4.36    0.0801              88.5                26.5\n4           4    5.41    1.98                61                  23.6\n5           5    5.32    6.6                 67.1                27.4\n6           6    6.49    4.09                42.8                22.6\n# ℹ 5 more variables: avg_uv_index &lt;dbl&gt;, avg_summer_temp &lt;dbl&gt;,\n#   n_content &lt;dbl&gt;, hairgrass_density_m2 &lt;dbl&gt;, penguin_density_5m2 &lt;dbl&gt;\n\n# View last few rows\ntail(hairgrass)\n\n# A tibble: 6 × 10\n  location_ID soil_pH p_content percent_soil_rock max_windspeed_knots\n        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n1         475    4.57     7.96               96.2               25.5 \n2         476    6.1      0.448              94.5                5.00\n3         477    4.72     4.86               70.3               12.3 \n4         478    6.41     3.8                86.9                4.86\n5         479    6.02     4.62               12                  7.23\n6         480    3.64     2.32               94.9                3.94\n# ℹ 5 more variables: avg_uv_index &lt;dbl&gt;, avg_summer_temp &lt;dbl&gt;,\n#   n_content &lt;dbl&gt;, hairgrass_density_m2 &lt;dbl&gt;, penguin_density_5m2 &lt;dbl&gt;\n\n\n\n\nNitrogen Content\nLet’s start by investigating any relationship between hair grass density and nitrogen content.\nFirst, we should spend a little time thinking about our variables. Spend a few minutes in small groups discussing the answer to the questions below.\n\nWhich columns are we interested in right now? N_content, hairgrass_density\nWhich one is the independent variable and which is the dependent variable?\n\nindependent: N_content\ndependent: hairgrass_density_m2\n\nAre the variables categorical or continuous? Both are continuous\nWhich type of data visualization should we use? Scatterplot\n\nNext, calculate the mean (mean()) and standard deviation (sd()) for the nitrogen content.\n\n# Mean and standard deviation of nitrogen content\nhairgrass %&gt;% \n  summarise(mean_N = mean(n_content),\n            sd_N = sd(n_content))\n\n# A tibble: 1 × 2\n  mean_N  sd_N\n   &lt;dbl&gt; &lt;dbl&gt;\n1   9.93  1.02\n\n\nNow that we have summarized the data, let’s take a look at the data visually.\n\n# Scatterplot\nggplot(hairgrass, aes(n_content, hairgrass_density_m2)) +\n  geom_point() +\n  labs(x = \"N Content\",\n       y = \"Hairgrass Density (per m^2)\") +\n  theme_bw()\n\n\n\n\nDo you see a pattern? What type of relationship do you see?\nHow do we analyze this type of relationship statistically?"
  },
  {
    "objectID": "modules/module_4/module4_1.html#statistical-analysis",
    "href": "modules/module_4/module4_1.html#statistical-analysis",
    "title": "4.1: Roads and Regressions",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nWe are going to use two (related) statistical methods to understand the relationship between two continuous (numeric) variables:\n\nCorrelation coefficient (\\(r\\)) and R-squared (\\(R^2\\))\nLinear regression\n\n\nLine of Best Fit\nTo understand correlation, R-squared, and linear regression, we first need to talk about something we call the line of best fit.\nThe line of best fit aims to minimize the distance between each observation (points) and the line. The distances between each observation and any line are called residuals (the dotted gray lines). The line of best fit is the line that has the smallest residuals.\n \nWe can add a line of best fit to a ggplot by using the geom_smooth() function. We need to specify that the method we want should produce a straight line (“linear model”).\n\n# Scatterplot\nggplot(hairgrass, aes(n_content, hairgrass_density_m2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"N Content\",\n       y = \"Hairgrass Density (per m^2)\") +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCorrelation Coefficient\nThe correlation coefficient, r, is a measurement of the strength of the relationship between two continuous variables.\nThe correlation coefficient is a number between -1 and 1 that looks at the relationship between two numeric variables. If the value is negative, there is a negative relationship between the two variable; if the value is positive, there is a positive relationship.\nIf all the points fall exactly on the line of best fit, r = 1 or -1. If there is no relationship between the variables, r is 0 (or something very close to it).\n\nThe greater the magnitude (size) of the correlation coefficient, the stronger the relationship between the two variables.\n\nIt is also important to recognize that the correlation coefficient has nothing to do with the slope of the line (we use linear regression to assess that!).\n\nBased on the hairgrass plot we did above, do you expect the correlation coefficient to be positive or negative? For this class, we’ll say that any r value with 0.1 of 0 means there is no relationship.\nTo calculate the correlation coefficient, we need to go back to base R and indicate which columns we are referencing with the $ operator. We use the cor() function.\n\n# Find the correlation coefficient\nr &lt;- cor(x = hairgrass$n_content, y = hairgrass$hairgrass_density_m2)\n\n# View result\nr\n\n[1] 0.6556456\n\n\n\n\nR-squared (\\(R^2\\))\nThe R-squared (\\(R^2\\)) value is a representation of how much variation is explained by the line of best fit.\nWhen we have a dependent variable and an independent variable, the R-squared value tells us how much of the variation in the dependent variable is explained by the independent variable.\nSometimes we don’t have obvious dependent and independent variables, but we can still use similar language (\\(R^2\\) tells us how much of the variation in the data is explained).\nTo calculate R-squared, we square the correlation coefficient value we calculated above. It is always positive because we are squaring it r value; that means that R-squared values range from 0 to 1. The closer to 1, the more variation is explained.\n\n# What is r^2?\nr^2\n\n[1] 0.4298711\n\n\nHow would we interpret this r^2 value?\n\n\nLinear Regression Analysis\nA regression analysis approximates the relationship between a dependent variable and one or more independent variables. It evaluates the strength of that relationship, ultimately giving us a p-value.\nSince we are using linear regressions in this course, the regression model will take the form of a line: y = mx + b\n\ny = dependent variable (y-axis)\nx = independent variable (x-axis)\nm = slope of the line is the slope\nb = y-intercept\n\nUsing our variables, what would our linear regression model look like (we don’t know m or b yet…)?\n\n\nHypothesis Testing\nWhat is the null hypothesis? What is the alternative hypothesis?\nNull Hypothesis (\\(H_{0}\\)): There is no relationship between hair grass density and nitrogen content.\nAlternative Hypothesis (\\(H_{A}\\)): There is a relationship between hair grass density and nitrogen content.\nWhat do you think this means for the slopes?\n\n\nRegression Analysis\nThankfully, R can calculate the slope (m) and y-intercept (b) of the line of best fit for us.\nLet’s find the equation for our line of best fit, our test statistic, and our p-value. To do this, we use a function called lm(): this stands for “linear model.”\nLike with ANOVA, we will then want to use the summary() function to get out the values we need.\n\n# Linear model of hairgrass density v. nitrogen content\nnitrogen &lt;- lm(hairgrass_density_m2 ~ n_content, hairgrass)\n\n# Summary of results\nsummary(nitrogen)\n\n\nCall:\nlm(formula = hairgrass_density_m2 ~ n_content, data = hairgrass)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.50324 -0.53853  0.01917  0.49500  2.25562 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.38516    0.34636   3.999 7.36e-05 ***\nn_content    0.65886    0.03471  18.984  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7765 on 478 degrees of freedom\nMultiple R-squared:  0.4299,    Adjusted R-squared:  0.4287 \nF-statistic: 360.4 on 1 and 478 DF,  p-value: &lt; 2.2e-16\n\n\nAs with our other statistical tests — t-tests and ANOVAs — the results give us some important values:\n\nb (y-intercept): part of our line of best fit equation\n\nThe “intercept estimate”, in this case 1.385, is our y-intercept in our line of best fit\n\nm (slope): also part of our line of best fit equation\n\nThis is the estimate for our independent variable (N_content in this case)\nIn this model, m = 0.659\n\nF-statistic: this is our test statistic\np-value: calculated from our regression model, used to determine significance (0.05 cut-off, as usual)\n\nThere are multiple p-values here. Focus either on the p-value for the independent variable (N_content) or the overall p-value displayed in the last line of the results summary (p &lt; 2.2e-16)\n\nR-squared: this is our R-squared value that we calculated earlier\n\nYou can report either the “multiple” or the “adjusted”\nThe “multiple” will typically match the one we calculate with code\n\n\nThis means our equation for the line of best fit is: y = 0.659x + 1.385 and there is a statistically significant relationship between nitrogen content and hairgrass density."
  },
  {
    "objectID": "modules/module_4/module4_1.html#soil-ph",
    "href": "modules/module_4/module4_1.html#soil-ph",
    "title": "4.1: Roads and Regressions",
    "section": "Soil pH",
    "text": "Soil pH\nLet’s do the same series of steps to determine how soil pH impacts hair grass densities. Work on this in your small groups, and we will go over it in about 10 minutes.\nStart with summarizing the data: mean and standard deviation.\n\n# Mean and standard deviation of soil pH\nhairgrass %&gt;% \n  summarize(mean_pH = mean(soil_pH),\n            sd_pH = sd(soil_pH))\n\n# A tibble: 1 × 2\n  mean_pH sd_pH\n    &lt;dbl&gt; &lt;dbl&gt;\n1    5.54  1.30\n\n\nVisualize the data. Remember to add in the line of best fit using ggplot2. Also add labels and a theme for practice.\n\n# Scatterplot\nggplot(hairgrass, aes(soil_pH, hairgrass_density_m2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Soil pH\",\n       y = \"Hairgrass Density per m^2\") + \n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCalculate the correlation coefficient (r)? What does this tell us?\n\n# Correlation coefficient\nr &lt;- cor(x = hairgrass$soil_pH, y = hairgrass$hairgrass_density_m2)\n\nHow much variation does soil pH explain in the hair grass density data?\n\n# What is r^2?\nr^2\n\n[1] 0.003962606\n\n\nWrite out the model for our question about soil pH (without values)?\n\n# hairgrass density  = m(soil_pH) + b\n\nRun the regression model and write out the equation for the line of best fit.\n\n# Linear model of hairgrass density v. soil pH\nsoil_pH &lt;- lm(hairgrass_density_m2 ~ soil_pH, data = hairgrass)\nsummary(soil_pH)\n\n\nCall:\nlm(formula = hairgrass_density_m2 ~ soil_pH, data = hairgrass)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.95914 -0.66321  0.02364  0.65938  2.64477 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.65039    0.20531  37.263   &lt;2e-16 ***\nsoil_pH      0.04972    0.03605   1.379    0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.026 on 478 degrees of freedom\nMultiple R-squared:  0.003963,  Adjusted R-squared:  0.001879 \nF-statistic: 1.902 on 1 and 478 DF,  p-value: 0.1685\n\n\nEquation for line of best fit: y = 0.005x + 7.65\nInterpret the results of the regression (our p-value cutoff is still = 0.05). What do we conclude about the relationship between soil pH and hair grass density? Why?\nBecause p &gt; 0.05 for the regression model, we fail to reject the null hypothesis and conclude that there is no relationship between soil pH and hairgrass density."
  },
  {
    "objectID": "modules/module_4/module4_1.html#data-driven-decision-making",
    "href": "modules/module_4/module4_1.html#data-driven-decision-making",
    "title": "4.1: Roads and Regressions",
    "section": "Data-driven Decision Making",
    "text": "Data-driven Decision Making\nThe reason we are using regression analysis is to inform where we should (or should not) build our road so we don’t harm the sensitive hair grass or take away their prime habitat.\nWhat do results above for nitrogen content and soil pH mean for the road we are building?"
  },
  {
    "objectID": "modules/module_4/module4_3.html",
    "href": "modules/module_4/module4_3.html",
    "title": "Using Functions to Automate Tasks",
    "section": "",
    "text": "In this lesson, we’ll learn how to write functions in R. Functions are essential for automating tasks and making your code more efficient and reusable. Let’s get started!\n\n\nWe’ve been using functions quite a lot in the course, but let’s remind ourselves exactly what a function actually is.\nA function is a block of code designed to perform a specific task. A function is executed when it is called. This means that the block of code is run every time you use the function!\nA function takes specific arguments as input, processes them, and returns the output.\n\nWe’ve used a lot of built-in functions already, like the mean() function.\nNow, we are going to write our own, user-defined functions!"
  },
  {
    "objectID": "modules/module_4/module4_3.html#introduction",
    "href": "modules/module_4/module4_3.html#introduction",
    "title": "Using Functions to Automate Tasks",
    "section": "",
    "text": "In this lesson, we’ll learn how to write functions in R. Functions are essential for automating tasks and making your code more efficient and reusable. Let’s get started!\n\n\nWe’ve been using functions quite a lot in the course, but let’s remind ourselves exactly what a function actually is.\nA function is a block of code designed to perform a specific task. A function is executed when it is called. This means that the block of code is run every time you use the function!\nA function takes specific arguments as input, processes them, and returns the output.\n\nWe’ve used a lot of built-in functions already, like the mean() function.\nNow, we are going to write our own, user-defined functions!"
  },
  {
    "objectID": "modules/module_4/module4_3.html#writing-our-first-function",
    "href": "modules/module_4/module4_3.html#writing-our-first-function",
    "title": "Using Functions to Automate Tasks",
    "section": "Writing Our First Function",
    "text": "Writing Our First Function\nFollowing the syntax from the image above, let’s write a function called add_numbers that takes two numbers as arguments and returns their sum.\n\n# An addition function\nadd_numbers &lt;- function(x, y) {\n  z &lt;- x + y\n  return(z)\n}\n\nNotice that we didn’t put any actual numbers in the function! Instead, we used generic (undefined) arguments (e.g., x and y) to represent the values that we will eventually supply to the function when we execute it.\nUsing our undefined arguments, we set up a statement (or series of statements) for the function to perform: add x and y together and save that value as an object called z.\nWe use the return() function to indicate that z is the value we want the add_numbers function to give back to us when it is executed.\nLet’s test our function with numbers, now!\n\n# Use our new function\nadd_numbers(5, 5)\n\n[1] 10\n\n# Store our result\nsum &lt;- add_numbers(10, 10)\n\n# View our result\nsum\n\n[1] 20\n\n\n\nLet’s Practice\nNow try to write a function called multiply_numbers that takes three numbers as arguments and returns the product.\nMake sure to test your function, and remember a function should have a name, a list of arguments, and return something.\n\n# Multiplication function\nmultiply_numbers &lt;- function(x, y, z) {\n  a &lt;- x * y * z\n  return(z)\n}\n\n# Test function\nmultiply_numbers(1, 3, 3)\n\n[1] 3\n\n\n\n\nExercise 1: Simple Function\nObjective: Write a function called inspect that takes a data frame as an argument and prints the head and tail of the data frame.\nSetup:\n\n# Load in tidyverse first!\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\nWarning: package 'forcats' was built under R version 4.2.2\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n\n# Load the dataframe into your environment by reading the hairgrass_data.csv file\nhairgrass &lt;- read_csv('data/hairgrass_data.csv')\n\nRows: 480 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): location_ID, soil_pH, p_content, percent_soil_rock, max_windspeed_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBefore we attempt to write this function, you’ll need to know about the print() function.\nIn R, if you have multiple expressions (in our case, head and tail) and you want to see the output of each, you need to explicitly print them using the print() function. Otherwise, R will just show the last expression it ran.\n\n# Demonstration of the print function\nprint(mean(hairgrass$soil_pH))\n\n[1] 5.544271\n\n\nNow we can start to write our function! Write a function that prints the head and tail of the dataframe. If you’re up for an extra challenge, have the function print out the first 10 and last 10 rows (instead of 6 and 6).\n\n# Write your function here\n# Remember to use print!\n\nload_and_inspect &lt;- function(dataframe) {\n  \n  # Print first 10 rows\n  print(head(dataframe, 10))\n  \n  # Print last 10 rows\n  print(tail(dataframe, 10))\n  \n}\n\nTest your function with the hairgrass data frame.\n\n# Test our function on hairgrass data\nload_and_inspect(hairgrass)\n\n# A tibble: 10 × 10\n   location_ID soil_pH p_content percent_soil_rock max_windspeed_knots\n         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n 1           1    4.9     5.49                44.5               14.9 \n 2           2    6.94    8.53                50.5               11.0 \n 3           3    4.36    0.0801              88.5               26.5 \n 4           4    5.41    1.98                61                 23.6 \n 5           5    5.32    6.6                 67.1               27.4 \n 6           6    6.49    4.09                42.8               22.6 \n 7           7    5.83    3.05                82.7                2.90\n 8           8    4.91    5.83                78.5               17.9 \n 9           9    5.73    4.35                48.5               20.7 \n10          10    5.61    5.54                54.4                7.49\n# ℹ 5 more variables: avg_uv_index &lt;dbl&gt;, avg_summer_temp &lt;dbl&gt;,\n#   n_content &lt;dbl&gt;, hairgrass_density_m2 &lt;dbl&gt;, penguin_density_5m2 &lt;dbl&gt;\n# A tibble: 10 × 10\n   location_ID soil_pH p_content percent_soil_rock max_windspeed_knots\n         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n 1         471    5.75     2.66               52.9               21.8 \n 2         472    6.28     1.6                64.3                6.40\n 3         473    2.69     7.35               68.2                6.24\n 4         474    6.36     3.76               85.4               12.7 \n 5         475    4.57     7.96               96.2               25.5 \n 6         476    6.1      0.448              94.5                5.00\n 7         477    4.72     4.86               70.3               12.3 \n 8         478    6.41     3.8                86.9                4.86\n 9         479    6.02     4.62               12                  7.23\n10         480    3.64     2.32               94.9                3.94\n# ℹ 5 more variables: avg_uv_index &lt;dbl&gt;, avg_summer_temp &lt;dbl&gt;,\n#   n_content &lt;dbl&gt;, hairgrass_density_m2 &lt;dbl&gt;, penguin_density_5m2 &lt;dbl&gt;\n\n\n\n\nExercise 2: Linear Regression Function\nObjective: Develop a function that performs linear regression between two columns of a data frame and returns the model summary.\nSome helpful hints:\n\nHave 2 arguments, one for each column\nIn the lm() function, you’ll want to run it by specifying the dataframe and column (like dataframe$column) rather than just using the name of the column and setting the data = dataframe argument. It will look similar to how we run the code to get a correlation coefficient.\n\n\n# Write your function here!\ndo_regression &lt;- function(independent, dependent) {\n  \n  # Linear model\n  lm_model &lt;- lm(dependent ~ independent)\n  \n  # Print result\n  print(summary(lm_model))\n  \n}\n\nTest the function with two columns from the hairgrass data set.\n\n# Test function on hairgrass data\ndo_regression(hairgrass$soil_pH, hairgrass$hairgrass_density_m2)\n\n\nCall:\nlm(formula = dependent ~ independent)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.95914 -0.66321  0.02364  0.65938  2.64477 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.65039    0.20531  37.263   &lt;2e-16 ***\nindependent  0.04972    0.03605   1.379    0.169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.026 on 478 degrees of freedom\nMultiple R-squared:  0.003963,  Adjusted R-squared:  0.001879 \nF-statistic: 1.902 on 1 and 478 DF,  p-value: 0.1685\n\n\n\n\nExercise 3: Plotting Function\nObjective: Create a function to plot a scatter plot between two variables and add a regression line.\nFor complicated reasons we won’t get into, we need to do a few important things in this function to make it work:\n\nWhen you write the ggplot code, you’ll need to use aes_string() in place of our usual aes() function.\nWhen you test your function with the hairgrass data, you’ll need to put the names of the two columns in quotation marks (e.g., “penguin_density_m2” instead of penguin_density_m2).\nDon’t worry about adding labels to your plot, but do add a theme.\n\n\n# Write your function here!\nplot_data &lt;- function(dataframe, x, y) {\n  \n  # Make plot \n  plot &lt;- ggplot(dataframe, aes_string(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") +\n    theme_classic()\n  \n  # Return plot\n  return(plot)\n  \n}\n\nTest the function with two columns from the hairgrass data set.\n\n# Test function on harigrass data\nplot_data(hairgrass, \"soil_pH\", \"p_content\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nBonus:\nObjective: Create a function, plot_data_with_r2, that modifies the scatter plot to include the R^2 value from the linear regression.\nFirst, outside of the function, calculate the R^2 value for your chosen variables. Then, modify your plotting function to add an r_squared argument.\nUse the annotate() to add the R^2 value to the plot. You can learn more about annotate() here!\nYou might also want to learn about the paste() function here to specify what the value you are putting on the plot represents. It might look something like R squared = 0.4.\n**Remember to use aes_string() instead of aes()!\n\n# Write your function here!\n\n# Calculate R-squared outside the function\nr &lt;- cor(x = hairgrass$soil_pH, y = hairgrass$p_content)\n\n# Round the value because otherwise it is extremely long!\nr_squared &lt;- round(r^2, 6) \n\n# Make our function\nplot_with_r2 &lt;- function(dataframe, x, y, r_squared) {\n  \n  # Create the plot with points and regression line\n  plot &lt;- ggplot(dataframe, aes_string(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\") +\n    annotate(\"text\", x = 8, y = 20, label = paste(\"R^2 =\", r_squared)) +\n    theme_classic()\n\n  # Return plot\n  return(plot)\n  \n}\n\nTest your function here:\n\n# Test function on hairgrass data\nplot_with_r2(hairgrass, \"soil_pH\", \"p_content\", r_squared)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nGreat Work!\nRemember, we will only ask you to make a function like the example at the beginning :)"
  }
]